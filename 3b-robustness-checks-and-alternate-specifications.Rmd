---
title: "3b-robustness-checks-and-alternative-specifications"
author: "Seth Green"
date: "9/7/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=F, echo=F}
rm(list = ls())
library(dplyr, warn.conflicts = F)
library(lubridate)
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(ggrepel, warn.conflicts = F)
library(purrr)
library(stringr)
library(tidyr, warn.conflicts = F)

source('./functions/dplot.R')
source('./functions/sum_lm.R')
# NOTE ON SUM_LM: 
# `sum_lm(dat, d, se_d, coef_only = T)` = `summary(lm(formula = d ~ se_d, 
#                                          data = dat))$coefficients`
```

```{r load_data, include=F, echo=F}
# read in data, add in two useful variables
# labels that paste together author and year for plots;  
# reading later results);
# add attitudes_behaviors (for studies that have both), one or the other;
# new var for random assignment (probably should remove 'contains randomization')

dat <- readRDS(file = './data/sa_meta_data_for_analysis.rds') %>%
  select(unique_study_id, scale_type, everything()) %>%
  mutate(labels = paste(tools::toTitleCase(word(author)), year),
         ra = ifelse(study_design == 'rct' | study_design == 'pragmatic rct', 
                     TRUE,
                     FALSE)) %>%
  group_by(unique_study_id) %>%
  mutate(
    attitudes_behaviors = case_when(
      all(c('attitudes', 'behavior') %in% scale_type) ~ 0,
      scale_type == 'attitudes' ~ 1,
      scale_type == 'behavior' ~ 2)) %>%
  ungroup() 

behavioral_data <- dat %>% filter(scale_type == 'behavior')

behavioral_rcts <- dat %>%
  filter(scale_type == 'behavior', 
         study_design == 'rct')

has_both <- dat %>% 
  filter(attitudes_behaviors == 0) %>%
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d), 
         mean_var_d = mean(var_d), 
         mean_se_d = mean(se_d)) %>%
  select(author, year, paper_title, unique_study_id, intervention_name,
         study_design, mean_d, mean_var_d, mean_se_d,
         ra, labels, scale_type) %>%  
  slice(1) %>% 
  pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))


```

## Pre-registered analyses not in the main text

* Compare the effect size of behavioral outcomes for experimental and quasi-experimental studies vs. non-experimental observational studies, both in the short- and in the long-term.

```{r experimental_quasi_vs_non_experimental_delay_and_not}
dat %>% 
    filter(scale_type == 'behavior') %>%
  select(author, year, paper_title, behavior_type, scale_name, 
         intervention_name,d, se_d, n_t_post, n_c_post, everything()) %>%
  mutate(study_group = if_else
         (study_design %in% c('quasi-experimental', 'pragmatic cluster rct', 'rct'),
           'randomized and quasis',  'observational')) %>%
  split(list(.$study_group, .$yes_delay)) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

# TODO: what is driving the large effect sizes 
# for no delay quasi and randomized studies?
# what even does no delay measurement mean in this context? Presumably there's no delay
# from onset of treatment rather than conclusion otherwise you couldn't measure 
# anything behavioral, unless it's entirely driven by 'intent' stuff
```

* Compare the effect size of behavioral outcomes for studies with random assignment vs. quasi-experimental studies, both in the short- and in the long-term. 

```{r experimental_stricti_vs_non_experimental_delay_and_not}
dat %>% 
    filter(scale_type == 'behavior',
           study_design != 'pre-post') %>%
    mutate(study_group = if_else
         (study_design %in% c('pragmatic cluster rct', 'rct'),
           'randomized','quasis')) %>%
  split(list(.$study_group, .$yes_delay)) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
# TODO: This and the above chunk are syntactically elegant but maybe there's 
# an easier way to present results; really just need the model Results. 
# this is complex; I'm just moving everything into latex tables by hand
```


* Evaluate at the effect size of perpetration behavioral outcomes for only studies with random assignment, both in the short- and in the long-term. 

```{r perpetration outcomes}

rct_perp <- dat %>%
  filter(scale_type == 'behavior',
                   behavior_type == 'perpetration', 
                   study_design %in% c('rct', 'pragmatic cluster rct'))

# now an overall meta. behavioral data need time to accumulate 
# so instead of delay, yes/no, how about delay > 1 month (30 days)

rct_perp %>%
  split(.$delay > 30) %>%
  map(.f = ~robust(rma(yi = .x$d, vi = .x$var_d), 
                   cluster = .x$unique_study_id))

```

* Compare the effect size of behavioral outcomes and attitudinal outcomes, both in the short- and long-term, among the quasi-experimental and randomly-assigned studies. (The covariation of attitudes and behaviors will be assessed in the full sample of studies.)

```{r attitudes_behaviors_quasi_random_delay_yes_no}

 dat %>% 
  filter(study_design %in% c('rct', 'pragmatic cluster rct', 'quasi-experimental')) %>%
  split(list(.$scale_type, .$yes_delay)) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

```

## robustness checks, alternate specifications, and alternate visualizations

Some initial visualizations
```{r viz_attempts}
# visualize this?
dat %>% 
  split(.$study_design) %>% 
  map(.x = ., .f = ~dplot(sa_data = ., dot_size = 2, condense = F,
                          dot_informative = T))
# TODO: modify this so that text is lined up properly and so that color = 
# attitude/behavior
# 
# so basically there's little relationship in the quasi and pragmatic rcts -- 
# there is actually a negative relationship in the quasi experimental studies? -- 
# but a meaningful relationship in the RCTs and pre-post studies. 
# What if we separate into randomized and not-randomized



```

```{r grouping rbustness checks}

# robustness check: group by study AND scale_type and average the results
dat_grouped <- dat %>%
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) %>%
  slice(1)

robust(x = rma(yi = d, vi = var_d, data = dat_grouped),
       cluster = dat_grouped$unique_study_id)
# alternative specification 
rma(yi = d, vi = var_d, data = dat_grouped)

# once we've grouped by study_id, clustering changes the SE by just 0.0001

# How about not grouping by scale_type?s
dat_grouped_two <- dat %>%
  group_by(unique_study_id) %>%
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) %>%
  slice(1)

robust(x = rma(yi = d, vi = var_d, data = dat_grouped_two),
  cluster = dat_grouped_two$unique_study_id)

# again, very similar.
# NOTE FOR TEAM: should we average within studies?
# in the prejudice meta, y'all averaged all the Ds and var_ds
# within each (to create a single point estimate representing each
# study). But here, we are looking for a lot more within-study variance (e.g. to
# attitudes predict # behavior? What about perp/victimization? So I personally
# vote that we not average but take everything. As a robustness check above, I found
# that averaging doesn't change the overall results (it didn't
# in the prejudice meta that much either if I recall correctly). Another
# approach would be, JH, to create a function that has "average within study" as
# an input each time to replace the functions I am using. My intuition is that
# we should not average, because it's more trouble than it's worth to then parse
# out within-study correlations; but I defer to the group.
```

Robustness checks for different ways to break down main results:
```{r}

# ra vs not:
dat %>% 
  split(list(.$scale_type, .$ra)) %>%   
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))


dat %>% 
  split(.$behavior_type) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id)) 
# this suggests that moving the needle on perpetration is effectively unsolved problem

# in general, the effects on victimization and  perpetration are larger in the rcts, 
# which is encouraging (and surprising to me).


## ROBUSTNESS CHECKS (probably remove)

# First, behavior vs attitudes:
dat %>% 
  split(.$scale_type) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
# behavioral effects are *much* smaller than attitude results

table(dat$behavior_type)
# TODO: triple-check that thesse are right.

# ra vs not:
dat %>% 
  split(list(.$scale_type, .$ra)) %>%   
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

# scale type and delay
# and further by scale type
dat %>% split(list(.$scale_type,.$yes_delay)) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))


```

Sum lm() checks that don't make it into the final paper
```{r sum_lm checks}
dat %>% 
  split(.$ra) %>% 
  map(.f = ~sum_lm(y = d, x = se_d, dat = .))


# how about the overall relationship between lab/field and d? 
sum_lm(dat, d, lab_field)

# study type?
sum_lm(dat, d, study_design)
# yep, meaningful relationship

# scale type?
sum_lm(dat, d, scale_type)
# that is a HUGE difference

# publication status?
sum_lm(dat, d, publication_type)

sum_lm(dat, d, behavior_type)dat

sum_lm(dat, d, scale_type)

behavioral_rcts %>% 
  filter(behavior_type == 'perpetration') %>% sum_lm(d, se_d)

robust(rma(yi = d, vi = var_d, mods = behavior_type, data = behavioral_rcts),
       cluster = behavioral_rcts$unique_study_id)




# what's the standard error? 
# thank you https://stackoverflow.com/questions/16097453/how-to-compute-p-value-and-standard-error-from-correlation-analysis-of-rs-cor
cor.test.plus <- function(x) {
  list(x, 
       Standard.Error = unname(sqrt((1 - x$estimate^2)/x$parameter)))
}
cor.test.plus(
  cor.test(
    x = has_both$mean_d_attitudes, 
    y = has_both$mean_d_behavior))

```


```{r incomplete behavior_type_atitude_cor_tests, eval = F}
# THIS DOES NOT WORK 
# pivot wider is hard
# by outcome type
has_both_behavior_types <- dat %>% 
  filter(attitudes_behaviors == 0) %>%
  group_by(unique_study_id, scale_type, behavior_type) %>%
  mutate(mean_d = mean(d), 
         mean_var_d = mean(var_d), 
         mean_se_d = mean(se_d)) %>%
  select(author, year, paper_title, unique_study_id, intervention_name,
         study_design, mean_d, mean_var_d, mean_se_d,
         ra, labels, scale_type, behavior_type) %>%  
  slice(1) %>% 
  pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels),
    names_from = c(scale_type, , behavior_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d)) # %>%
#  select(-behavior_type_attitudes) %>% 
 # rename(behavior_type = behavior_type_behavior)

has_both %>% 
  split(.$behavior_type) %>%
  map(~cor.test(.$mean_d_attitudes, .$mean_d_behavior))


# What about just the RCTs?
  
behavioral_rcts %>%
  split(behavioral_rcts$behavior_type) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id)) 

```

### are effect sizes changing over time?

```{r}
dat %>% ggplot( (aes(x = year, y = d))) + geom_point() +   geom_smooth(method='lm')
# basically no slope

dat %>% sum_lm(d, year)
dat %>% split(list(.$ra, .$scale_type)) %>%
  map(~ ggplot(data = ., mapping = aes(x = year, y = d)) + 
        geom_point() + 
        geom_smooth(method = 'lm'))

# these are more or less the same results as  Anderson, L. A., & Whiston, S.C. (2005)
```

### appendix analysis

We did not anticipate, however, the quantity of cluster-assigned studies in our sample (X of Y studies, and X of Y featuring random assignment)

```{r}

dat %>% group_by(unique_study_id) %>% slice(1) %>% ungroup() %>% count(is.na(.$cluster_type))

 dat %>% filter(ra) %>% group_by(unique_study_id) %>% slice(1) %>% ungroup() %>% count(is.na(.$cluster_type))

```


*Robustness check of reclassifying pragmatic cluster RCTs as quasi-experimental studies*

```{r}

dat_robust <- dat %>%
  mutate(study_design = if_else(study_design == 'pragmatic cluster rct',
                                'quasi-experimental', 
                                as.character(study_design)),
         ra = ifelse(study_design == 'rct', TRUE,FALSE))

dat_robust %>% split(.$ra) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

dat_robust %>% split(.$study_design) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

```

### illustration of difference between d_i_p estimator and odds_ratio estimator
```{r}
dip_calc <- function(p1, p2){
  diff <- p1 - p2
  sd <- sqrt(p2 * (1 - p2))
  d <- round(diff/sd, digits = 3)
  d
}

odds_ratio_to_d <- function(p1, p2){
  odds_ratio <- p1 / p2
  d <- log(odds_ratio) * sqrt(3) / pi
  d
}

odds_ratio_to_d(.6, .3)
dip_calc(.6, .3)
odds_ratio_to_d(.6, .3) == odds_ratio_to_d(.06, .03)
dip_calc(.6, .3) - dip_calc(.06, .03)

### distribution of SD of Bernoulli SD

proportions <- seq(from = 0, to = 1, by = 0.01)
SDs <- as.numeric(lapply(X = proportions, FUN = function(p){sqrt(p * (1 - p))}))

ggplot() + geom_point(aes(proportions, SDs)) + theme_minimal()

# what's a DIP that corresponds to d = 0.28
dip_calc(0.05, 0.015)
```

visualizing how little attitudes and behavioral outcomes have changed over time 
```{r change_over_time}
# visualize average within  year and outcome type
dat %>% 
  group_by(year, scale_type) %>% 
  summarise(mean_d = mean(d), 
            mean_se_d = mean(se_d)) %>%
  ggplot(aes(x = year, y = mean_d, 
             group = scale_type)) +
  geom_point(aes(color = scale_type)) +
  geom_smooth(method = 'lm', aes(color = scale_type), se = F) + 
  theme_minimal() +
  scale_x_continuous(breaks = seq(1986, 2018, 5)) +
  labs(color = "Attitudes or behaviors")# +  ylab(expression(paste("Glass's", " ", Delta)))
  
```

additional ways to look at rct perp outcomes

```{r rct_perp}
# not a big difference

rct_perp %>% ggplot(mapping = aes(x = delay, y = d)) + 
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()

rct_perp %>% filter(delay >30) %>%
ggplot(mapping = aes(x = se_d, y = d)) + 
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()
# a stronger relationship