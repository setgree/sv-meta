---
title: "3-SA-meta-analyis.Rmd"
author: "Seth Green"
date: "8/19/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=F, echo=F}
rm(list = ls())
library(dplyr, warn.conflicts = F)
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(ggrepel, warn.conflicts = F)
library(purrr)
library(tidyr, warn.conflicts = F)

source('./functions/dplot.R')
source('./functions/sum_lm.R')
# NOTE ON SUM_LM: 
# `sum_lm(dat, d, se_d)` = `summary(lm(formula = d ~ se_d, data =  dat))`


# read in data, add in two useful variables
# labels that paste together author and year for plots;  
# reading later results);
# and attitudes_behaviors (for studies that have both), onem or the other.

dat <- readRDS(file = './data/sa_meta_data_for_analysis.rds') %>%
  select(unique_study_id, scale_type, everything()) %>%
  mutate(labels = paste(tools::toTitleCase(word(author)), year)) %>%
  group_by(unique_study_id) %>%
  mutate(
    attitudes_behaviors = case_when(
      all(c('attitudes', 'behavior') %in% scale_type) ~ 0,
      scale_type == 'attitudes' ~ 1,
      scale_type == 'behavior' ~ 2)) %>%
  ungroup() 

```

# General Methods 


**Initial data exploration**
```{r initial_exploration}

# first, is there a relationship between effect size and se?

sum_lm(dat, d, se_d)

# This is less of a relationship than I (Seth) expected;
# Roni is not very surprised because public health is 
# in general less p-hacked than psych
# JH agrees and there's an interesting point about
# not going for flashy journals

# how about the overall relationship between lab/field and d? 
sum_lm(dat, d, lab_field)
# this is equivalent to `
# essentially no difference at all

# study type?
sum_lm(dat, d, study_design)
# yep, meaningful relationship

# scale type?
sum_lm(dat, d, scale_type)
# that is a substantial difference

# now, an overall meta-analysis

robust(x = rma(yi = d, vi = var_d, data = dat),
       cluster = dat$unique_study_id)

# robustness check: group by study AND scale_type and average the results
dat_grouped <- dat %>%
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) %>%
  slice(1)

robust(x = rma(yi = d, vi = var_d, data = dat_grouped),
       cluster = dat_grouped$unique_study_id)
rma(yi = d, vi = var_d, data = dat_grouped)

# once we've already grouped by study_id, clustering changes the SE by just 0.0001

# How about not grouping by scale_type?s
dat_grouped_two <- dat %>%
  group_by(unique_study_id) %>%
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) %>%
  slice(1)

robust(x = rma(yi = d, vi = var_d, data = dat_grouped_two),
  cluster = dat_grouped_two$unique_study_id)
# again, very similar.
# NOTE FOR TEAM: should we average within studies?
# in the prejudice meta, y'all averaged all the Ds and var_ds
# within each (to create a single point estimate representing each
# study). But here, we are looking for a lot more within-study variance (e.g. to
# attitudes predidct # behavior? What about perp/victimization? So I personally
# vote that we not average but take everything. As a robustness check above, I found
# that averaging doesn't change the overall results (it didn't
# in the prejudice meta that much either if I recall correctly). Another
# approach would be, JH, to create a function that has "average within study" as
# an input each time to replace the functions I am using. My intuition is that
# we should not average, because it's more trouble than it's worth to then parse
# out within-study correlations; but I defer to the group.
```

### By study type:

"Analysis of subgroups or subsets: We will compare subsets of the data according to classifications of evaluation methodology (i.e., observational vs. quasi-experimental vs. experimental methods)..."

```{r study_type}
dat %>% 
  split(dat$study_design) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
```

### By outcome type:

"... and of outcome measurement (i.e., general behavior indices vs. perpetration behavior, and attitudinal measurement vs. behavioral measurement)."

```{r outcome_type}
# First, just behavior vs attitudes:
dat %>% 
  split(dat$scale_type) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
# behavioral effects are 60% smaller than attitudinal effects

### CATEGORIZE BEHAVIOR:
# perpetration, victimization, intervention are the big three intervention measures.
# Also, there's involvement, and then things that are harder to classify.
behavioral_data <- dat %>% 
  filter(scale_type == 'behavior') %>%
  mutate(
  behavior_type = as.factor(case_when(
    str_detect(scale_name, 
               'perp|perpetration|agression|comitted') ~ 'perpetration',
    str_detect(scale_name, 'vict|victimization|completed rape|underwent|ses-past year|survivor') ~ 'victimization',
    str_detect(scale_name, 'bystander|Observing') ~ 'bystander',
    str_detect(scale_name, 'volunteer|support|involvement') ~ 'involvement',
    TRUE ~ 'other')))

# TODO: this was crude, because the scale_names weren't written with this analysis in mind. 
#I think we should rewrite each of the scale names to clearly state which of the
# four categories it falls into, and I will discuss this with JH & Roni.

# how do effect sizes compare across behavioral types?
sum_lm(behavioral_data, d, behavior_type)
behavioral_data %>% 
  split(behavioral_data$behavior_type) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id)) 

# on average, interventions seem to be more effective at reducing victimization than perpetration, but both effects are qualitatively 'small'. 
# What about just the RCTs?

behavioral_rcts <- behavioral_data %>%
  filter(study_design == 'rct')

behavioral_rcts %>%
  split(behavioral_rcts$behavior_type) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id)) 

robust(rma(yi = d, vi = var_d, mods = behavior_type, data = behavioral_data), cluster = behavioral_data$unique_study_id)
# in general, the effects on victimization and  perpetration are larger in the rcts, which is encouraging (and surprising to me).
```

* Evaluate the overall effect size of behavioral outcomes for all of the studies in the dataset, both in the short- and in the long-term. 

```{r behavioral_over_time}

sum_lm(dat, d, scale_type)

robust(x = rma(yi = d, vi = var_d, data = behavioral_data), 
       cluster = behavioral_data$unique_study_id)

# first, meta-analysis with a split for delay, yes, no
behavioral_data %>% 
  split(as.factor(behavioral_data$delay > 0)) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

# a delay leads to a decline of effect size of about 1/3.

# in general, delayed results are smaller.
```

* Compare the effect size of behavioral outcomes for experimental and quasi-experimental studies vs. non-experimental observational studies, both in the short- and in the long-term.

```{r experimental_quasi_vs_non_experimental_delay_and_not}
behavioral_data %>% 
  mutate(study_group = if_else
         (study_design %in% c('quasi-experimental', 'pragmatic rct', 'rct'),
           'randomized and quasis',  'observational'),
         yes_delay = if_else(delay == 0, '& no delay', '& delay')) %>%
  split(list(.$study_group, .$yes_delay)) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
```

* Compare the effect size of behavioral outcomes for studies with random assignment vs. quasi-experimental studies, both in the short- and in the long-term. 

```{r experimental_stricti_vs_non_experimental_delay_and_not}

behavioral_data %>% 
  mutate(study_group = if_else(
    study_design %in% c('pragmatic rct', 'rct'), 'randomized', 'observational'),) %>%
    mutate(yes_delay = if_else(delay == 0, '& no delay', '& delay')) %>%
  split(list(.$study_group, .$yes_delay)) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
# TODO: This and the above chunk are syntactically elegant but maybe there's an easier way to present results
```

* Evaluate at the effect size of perpetration behavioral outcomes for only studies with random assignment, both in the short- and in the long-term. 
```{r perpetration outcomes}

rct_perp <- behavioral_rcts %>% filter(behavior_type == 'perpetration')
sum_lm(rct_perp, d, se_d)

# now an overall meta. behavioral data need time to accumulate 
# so instead of delay, yes/no, how about delay > 1 month (30 days)

rct_perp %>%
  split(.$delay > 30) %>%
  map(.f = ~robust(rma(yi = .x$d, vi = .x$var_d), 
                   cluster = .x$unique_study_id))

rct_perp %>% ggplot(mapping = aes(x = delay, y = d)) + 
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()

# one story here is the range of Ds -- what's this -0.5 increase in perpetration?
# TODO: double-check that we coded this in the right direction

rct_perp %>% filter(delay >30) %>%
ggplot(mapping = aes(x = se_d, y = d)) + 
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()
# a strong relationship

```

* Compare the effect size of behavioral outcomes and attitudinal outcomes, both in the short- and long-term, among the quasi-experimental and randomly-assigned studies. (The covariation of attitudes and behaviors will be assessed in the full sample of studies.)

```{r}

rcts_and_quasi <- dat %>% 
  filter(study_design %in% c('rct', 'pragmatic_rct', 'quasi-experimental')) 

# overall effect size difference between attitudes and behaviors?
rcts_and_quasi %>%
  sum_lm(d, scale_type)

# meta-analyze attitudes and behaviors separately, short and long term
rcts_and_quasi %>% 
    mutate(yes_delay = if_else(delay == 0, '& no delay', '& delay')) %>%
  split(list(.$scale_type, .$yes_delay)) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

```

What is the association between attitudes and behaviors (temporal match) – this answers our question of ‘should we care about attitudes?’

```{r}
has_both <- dat %>% 
  filter(attitudes_behaviors == 0) %>%
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d), 
         mean_var_d = mean(var_d), 
         mean_se_d = mean(se_d)) %>%
  select(author, year, paper_title, unique_study_id, 
         mean_d, mean_var_d, mean_se_d, everything()) %>%  
  slice(1)  %>% 
  pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id, study_design, labels),
    names_from = scale_type, 
    values_from = c(mean_d, mean_var_d, mean_se_d))


cor.test(x = has_both$mean_d_attitudes, 
         y = has_both$mean_d_behavior)

# by study type?
has_both %>% 
  split(.$study_design) %>%
  map(~cor.test(.$mean_d_attitudes, .$mean_d_behavior))


# what's the slope of these lines
has_both %>% sum_lm(mean_d_behavior, mean_d_attitudes)
has_both %>% filter(study_design == 'rct') %>% sum_lm(mean_d_behavior, mean_d_attitudes)
```


### Plotting
```{r plotting}

# overall linear plot
ggplot(data = dat, mapping = aes(x = se_d, y = d)) +
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()

#### Visualizing this: plot D and SE, with color corresponding to
#### study type and shape corresponding to lab or field
dplot(condense = T)
# I had really thought there was more of a relationship
dplot(condense = F)
# not especially informative

## Relationship between attitudes and behaviors 

has_both %>%
  select(author, year, mean_d_attitudes, mean_d_behavior) %>%
  ggplot(mapping = aes(x = mean_d_attitudes, y = mean_d_behavior)) +
  geom_point(size = 3) + 
  geom_smooth(method = 'lm', se = F, lty = 'dashed') + 
  theme_minimal()

# now same with RCTs 
has_both %>% filter(study_design == 'rct') %>% 
  select(author, year, mean_d_attitudes, mean_d_behavior, labels) %>%
  ggplot(mapping = aes(x = mean_d_attitudes, y = mean_d_behavior)) +
  geom_point(size = 3) + 
  geom_smooth(aes(fill = NULL), lty = "dashed", fullrange = TRUE, 
              method = "lm",
              show.legend = FALSE, alpha = .1, se = F) + 
  geom_label_repel(mapping = aes(label = labels)) +
  theme_minimal()
```
