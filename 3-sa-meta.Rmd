---
title: "3-SA-meta-analyis.Rmd"
author: "Seth Green"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
editor_options: 
  chunk_output_type: console
---

### Overview

This document will work through our results, and its aim is to both reflect the pre-analyis plan as it was written in 2018 and the most up-to-date version of the text. Everything in our pre-analysis plan will be included in either the main results section or an appendix, but not necessarily in the order it was drafted up.

This document will precede chunks with quotes prepended either a "*paper*" or a "*pap*" to denote whether the text is from the paper or the pre-analysis plan.

## Setup

### load libraries

```{r setup, include=F, echo=F}
rm(list = ls())
library(dplyr, warn.conflicts = F)
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(ggrepel, warn.conflicts = F)
library(purrr)
library(stringr)
library(tidyr, warn.conflicts = F)
library(tibble)

source('./functions/sum_lm.R')
# NOTE ON SUM_LM: 
# `sum_lm(dat, d, se_d, coef_only = T)` = `summary(lm(formula = d ~ se_d, 
#                                          data = dat))$coefficients`
```

### load data

```{r load_data, include=F, echo=F}
# read in data, add in two useful variables
# labels that paste together author and year for plots;  

raw_dat <- readRDS(file = './data/sa_meta_data_for_analysis.rds')

dat <- raw_dat %>%
  select(unique_study_id, scale_type, everything()) %>%
  mutate(labels = paste(tools::toTitleCase(word(author)), year),
         ra = ifelse(study_design == 'rct', TRUE, FALSE),
         yes_delay = as.factor(delay > 0))
```

## Results 

### Overall results and publication bias 

*paper*: "We estimate an overall effect size of..."
```{r overall_meta}

overall_results <- robust(x = rma(yi = d, vi = var_d, data = dat),
       cluster = dat$unique_study_id); overall_results

```

*paper*: "Across the sample as a whole, we do not observe a statistically significant relationship between effect sizes and standard errors"
```{r publication_bias_overall}
sum_lm(dat, d, se_d, coefs_only = T)
```

*paper*: "The relationship between SE and $\Delta$ remains insignificant within randomized and quasi-experimental designs, but highly significant within pre-post designs"

```{r publication_bias_by_study_type}
# by study type?
dat %>% 
  split(.$study_design) %>% 
  map(.f = ~sum_lm(y = d, x = se_d, dat = .))
```

## Heterogeneity across designs and outcomes

### By study type:

*pap*: "Analysis of subgroups or subsets: We will compare subsets of the data according to classifications of evaluation methodology (i.e., observational vs. quasi-experimental vs. experimental methods) and of outcome measurement

*pap*: Compare the effect size of behavioral outcomes for experimental and quasi-experimental studies vs. non-experimental observational studies, both in the short- and in the long-term."

...(i.e., general behavior indices vs. perpetration behavior, and attitudinal measurement vs. behavioral measurement).

*paper*: "The picture becomes murkier, however, when we break effect size estimates down by study design and attitudes vs. behaviors...Further, the largest behavioral effect sizes are found, on average, within randomized designs"

```{r study_type}

# main body of table
dat %>%   
  split(list(.$scale_type, .$study_design)) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

# behavior vs attitudes for the 'overall' row
dat %>% 
  split(.$scale_type) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

# study designs for the "overall" column
dat %>% 
      split(.$study_design) %>% 
      map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
# overall/overall text comes from overall_meta chunk

# TODO (maybe): extract the needed elements from the list and 
# format it into a nice table rather than filling in table by hand?
# EDIT: I tried this with chatGPT and got very complicated code in response
# is on the backburner

# double-check Ns;
dat %>% group_by(scale_type, study_design) %>% tally()
```

*paper* (footnote): "...In keeping with the consistency between available meta-analyses' estimates, we also observe a small, statistically insignificant relationship between effect size and year of publication"

```{r effect_size_and_pub_year}
dat %>% sum_lm(y = d, x = year)

# this graph isn't in the paper but making sure there's no obvious relationship:
dat %>%
  ggplot(aes(x = year, y = d, color = study_design)) +
  geom_point(aes(shape = scale_type), size = 3) +
  geom_smooth(aes(group = study_design), method = "lm", se = FALSE) +
  scale_x_continuous(breaks = seq(1986, 2018, by = 2)) +
  theme_minimal() +
  labs(color = "Study Design", shape = "Scale Type") +
  guides(color = guide_legend(order = 1), shape = guide_legend(order = 2)) +
  ggtitle('Effect sizes over time') +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_shape_manual(values = c("attitudes" = 16, "behavior" = 17))
```


## Do effects attenuate over time?

*pap* Evaluate the overall effect size of behavioral outcomes for all of the studies in the dataset, both in the short- and in the long-term. 

*paper*: Overall, the relationship between days of delay and effect size is diminutive and insignificant ($\beta$ = -0.00023). 
```{r d_and_delays}
dat %>% sum_lm(d,delay)

### by scale_type?
dat %>%
  split(.$scale_type) %>% 
  map(.f = ~sum_lm(y = d, x = delay, dat = .))

## What's the longest attitude/behavior delay measurement?
dat %>%
  group_by(scale_type) %>%
  summarize(max_delay = max(delay))
```

*paper*: "An alternative way to assess the overall relationship is to split the sample into outcomes with a delay of at least one day and those without and meta-analyze each separately. Estimates and standard errors for delayed and non-delayed outcomes are presented in table XXX, with separate estimates presented for attitudes and for involvement, bystander, victimization and perpetration behavioral outcomes

```{r do_effect_sizes_decay}

# split for delay and behaviors/attitudes 
dat %>% split(list(.$scale_type, as.factor(.$delay > 0))) %>%
                map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

# note: 'overall' behavioral/attitude measurements taken from `overall_results`

# split meta-analysis by delay/not for 'overall' estimates:
dat %>% split(.$yes_delay) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

# split for delay and behavioral categories
dat %>% 
  split(list(.$behavior_type, .$yes_delay)) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

## TODO: revisit these
dat %>% filter(behavior_type == 'victimization') %>% filter(yes_delay == 'FALSE') %>% select(author, year, paper_title, d, se_d) # really two negative #s? 
## TODO: Maybe work into paper breakdown of behavioral types? 
table(dat$behavior_type)

# just within RCTs
dat %>% filter(scale_type == 'behavior') %>% group_by(behavior_type) %>% tally()
```

## A closer look at behavioral outcomes

### Behavioral effect sizes by delay and study design

### Do attitudinal changes predict behavioral changes?
*paper*: A clear majority of interventions in our sample are aimed directly at changing attitudes towards sexual violence, and attitudinal outcomes outnumber behavioral outcomes by about five to two (355 to 145)

```{r attitudes_behaviors_count}
table(dat$scale_type)
# by study design?
dat %>% group_by(scale_type, study_design) %>% tally()

# by categories of behavior/attitudes?
dat %>% group_by(study_design, behavior_type) %>% tally()
```

*pap*: "What is the association between attitudes and behaviors (temporal match) – this answers our question of ‘should we care about attitudes?’"
...The covariation of attitudes and behaviors will be assessed in the full sample of studies

*paper*: To assess this, we look at studies that measure both behavioral and attitudinal changes and estimate the average correlation. In the entire universe of studies reporting both types of outcomes, we see an average Pearson's product-moment correlation of 0.14; this is a modest, positive relationship, but with a p-value of  0.278, it is not statistically distinguishable from zero. 

```{r}
has_both <- dat %>% 
  filter(all(c('attitudes', 'behavior') %in% scale_type)) %>%
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d), 
         mean_var_d = mean(var_d), 
         mean_se_d = mean(se_d)) %>%
  select(author, year, paper_title, unique_study_id, intervention_name,
         study_design, mean_d, mean_var_d, mean_se_d,
         ra, labels, scale_type) %>%  
  slice(1) %>% 
  pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))

cor.test(x = has_both$mean_d_attitudes, 
         y = has_both$mean_d_behavior)

```

*paper*:  Breaking down these results by study type, we see a _negative_, statistically significant correlation between attitudes and behaviors in observational research (-0.615057, p = 0.03329); a stronger, positive relationship within quasi-experimental research (0.3464049, p = 0.1054)); and a middling relationship within RCTs (0.1594935, p = 0.4364).

```{r ttitude_behavior_correlation_for_rcts_vs_observational}

# by study type?
has_both %>% 
  split(.$study_design) %>%
  map(~cor.test(.$mean_d_attitudes, .$mean_d_behavior))

# not in paper but what's the slope of these lines
has_both %>%
  split(.$study_design) %>%
  map(~sum_lm(y = mean_d_behavior,
              x = mean_d_attitudes, 
              dat = .))
```

### Forest Plot (Figure)

*paper*: Figure XXX highlights the randomized controlled trials that measure both behavioral and attitudinal outcomes, with horizontal lines for the two categories' average meta-analytic effects.

```{r forest_plot}
dat_for_forest_plot <- raw_dat %>% 
  select(unique_study_id, scale_type, everything()) %>%
  filter(study_design == 'rct',
         all(c('attitudes', 'behavior') %in% scale_type)) %>%
  mutate(study_names = paste0(tools::toTitleCase(word(author)), " ",
                              year, " ", "(", scale_type, ")")) %>%
  group_by(unique_paper_id, scale_type) %>%
  mutate(d = mean(d), var_d = mean(var_d), se_d = mean(se_d)) %>%
  slice(1) %>%
  ungroup(scale_type) %>% 
  mutate(mean_se = mean(se_d)) %>%
  arrange(desc(mean_se)) %>%
  ungroup() %>%
  select(study_names, d, se_d, 
         scale_type, unique_paper_id)

# overall effect size
overall_forest <- dat_for_forest_plot %>%
  split(.$scale_type) %>%
  map(~robust(x = rma(yi = .$d,  sei = .$se_d), 
              cluster = .$unique_paper_id))


dat_for_forest_plot <- dat_for_forest_plot %>% 
  add_row(study_names = "Overall (Attitudes)",
          d = overall_forest[[1]]$beta,
          se_d = overall_forest[[1]]$se,
          scale_type = "attitudes",
          unique_paper_id = NA) %>%
  add_row(study_names = "Overall (Behavior)",
          d = overall_forest[[2]]$beta,
          se_d = overall_forest[[2]]$se,
          scale_type = "behavior",
          unique_paper_id = NA) %>%
  mutate(index = row_number()) %>%
  relocate(index, .before = 1)
  
# plot 

p <- dat_for_forest_plot %>%
  ggplot(aes(y = index, x = d, xmin = d - (1.96 * se_d),
             xmax = d + (1.96 * se_d))) +  
  geom_point(size = 1) +
  geom_errorbarh(height = .1, aes(color = scale_type)) +
  geom_vline(xintercept = 0, color = "black", alpha = .5) +
  scale_x_continuous(name = expression(paste("Glass's", " ", Delta))) +
  scale_y_continuous(name = "", breaks = 1:length(dat_for_forest_plot$study_names),
                     trans = "reverse", labels = dat_for_forest_plot$study_names) + 
  ylab("Study") +
  geom_vline(xintercept = overall$attitudes$beta, color = '#F8766D', lty = 'dashed') +
  geom_vline(xintercept = overall$behavior$beta, color = '#00BFC4', lty = 'dashed') +
  labs(color = "Attitudes or behaviors") +
  theme_minimal() + 
  theme(axis.text.y = element_text(face = ifelse(
    dat_for_forest_plot$study_names %in% 
      c("Overall (Attitudes)", "Overall (Behavior)"), 
    "bold", "plain"))) +
  ggtitle("Attitudinal and Behavioral Changes Compared")

# getting a warning message about vectorized input is not officially supported
# and results may change in future versions of ggplot2. 
# Guess we'll hope they don't? :)

p
png(filename = './results/4-ggplot-forest-plot.png', width = 1920/2, height = 1080/2)
p
dev.off()
# useful for figuring out colors & such: `ggplot_build(p)$data`
```


### Assessing differences between categories of behavioral outcomes
*pap*: Compare the effect size of behavioral outcomes for studies with random assignment vs. quasi-experimental studies, both in the short- and in the long-term. 

*paper*: Table XXX presents the effect sizes associated with each category of behavior, separated by random assignment, with attitudinal effect sizes presented as a reference category.

```{r heterogeneity_study_design_behavior_type}
# overall estimates
# first, how many are in each category?
dat %>% group_by(study_design, behavior_type) %>% tally()
# so there are zero pre-post designs that ask about involvement
dat %>% filter(study_design == 'pre-post', behavior_type == 'involvement') %>% count()

# Two possible solutions to this. 1) group studies into RCT vs non-randomized,
# and because we specify this in the pre-analysis plan 
# Or we can combine involvement and bystander behaviors into one category which 
# we might call 'anti-rape culture' or just 'involvement'. As of this writing,
# March 2022, that's probably what I (Seth) would do; but our PAP says random 
# vs not. So here we'll do both analyses, and put random vs not in the text

## I (Seth) am totally forgetting what I was talking about in the 
## above comment as of May 31 2023
dat %>% 
  split(list(.$ra, .$behavior_type)) %>% 
   map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
# for 'behavior overall' category (to keep consistent with other tables)
dat %>% split(list(.$ra, .$scale_type)) %>% 
   map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

# Alternative grouping:

dat %>% 
  mutate(behavior_type = as.character(behavior_type)) %>%
  mutate(behavior_type = if_else(
  behavior_type == 'bystander', 'involvement', behavior_type)) %>% 
  split(list(.$study_design, .$behavior_type)) %>%
     map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
# This was trickier than it should have been 
# https://github.com/ReeceGoding/Frustration-One-Year-With-R#410-factor-variables
```

### Zooming in on Perpetration effects within randomized research}
just sanity checking the above results via a filter method
```{r}
rct_perp <- dat %>% 
  filter(study_design == 'rct' & behavior_type == 'perpetration') 

robust(x = rma(yi = d, vi = var_d, data = rct_perp),
       cluster = rct_perp$unique_study_id)

```
