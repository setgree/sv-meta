---
title: "3-SA-meta-analyis.Rmd"
author: "Seth Green"
date: "8/19/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=F, echo=F}
rm(list = ls())
library(broom)
library(dplyr, warn.conflicts = F)
library(metafor, quietly = T)
library(rmeta)
library(ggplot2, warn.conflicts = F)
library(ggrepel, warn.conflicts = F)
library(purrr)
library(stringr, warn.conflicts = F)
library(tidyr, warn.conflicts = F)

source('./functions/dplot.R')
source('./functions/sum_lm.R')
# read in data, add in two useful variables: 
# labels (for behavior) and attitudes_behaviors


dat <- readRDS(file = './data/sa_meta_data_for_analysis.rds') %>%
  select(unique_study_id, scale_type, everything()) %>%
  mutate(labels = paste(tools::toTitleCase(word(author)), year)) %>%
  group_by(unique_study_id) %>%
  mutate(
    attitudes_behaviors = case_when(
      all(c('attitudes', 'behavior') %in% scale_type) ~ 0,
      scale_type == 'attitudes' ~ 1,
      scale_type == 'behavior' ~ 2
    )
  ) %>%
  ungroup()


```

### First, an overall meta-analysis. 

```{r initial_exploration}

# first, is there a relationship between effect size and se?
# `sum_lm(dat, d, se_d)` = `summary(lm(formula = d ~ se_d, data =  dat))`
sum_lm(dat, d, se_d)

# This is less of a relationship than I (Seth) expected;
# Roni is not very surprised because public health is 
# in general less p-hacked than psych
# JH agrees and there's an interesting point about
# not going for flashy journals


# how about the overall relationship between lab/field and d? 
sum_lm(dat, d, lab_field)
# this is equivalent to `
# essentially no difference at all

# study type?
sum_lm(dat, d, study_design)
# yep, meaningful relationship

# scale type?
sum_lm(dat, d, scale_type)
# that is a substantial decline

```


### By study type:
"Analysis of subgroups or subsets: We will compare subsets of the data according to classifications of evaluation methodology (i.e., observational vs. quasi-experimental vs. experimental methods)..."
```{r study_type}
# first crack at meta-analysis

robust(x = rma(yi = d, vi = var_d, data = dat),
       cluster = dat$unique_study_id)

# group by study AND scale_type
dat_grouped <- dat %>%
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) %>%
  slice(1)
# if we're doing this all throughout, it'll be a lot easier

robust(x = rma(yi = d, vi = var_d, data = dat_grouped),
       cluster = dat_grouped$unique_study_id)
rma(yi = d, vi = var_d, data = dat_grouped)
# once we've already grouped by study_id, clustering changes the SE by just 0.0001

# How about not grouping by scale_type?s
dat_grouped_two <- dat %>%
  group_by(unique_study_id) %>%
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) %>%
  slice(1)

robust(x = rma(yi = d, vi = var_d, data = dat_grouped_two),
  cluster = dat_grouped_two$unique_study_id)
# again, very similar.Consult with group. What's our one true way of doing this -- 
# cluster on unique_study_id, have one outcome per study as default?
# Aren't those two redundant...this is hard

# meta-analyze each of the study design types separately:
dat %>% 
  split(dat$study_design) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
```

### By outcome type:

"... and of outcome measurement (i.e., general behavior indices vs. perpetration behavior, and attitudinal measurement vs. behavioral measurement)."

```{r outcome_type}
# let's talk about this -- what's the right way to filter by  behavior type? 
# unique(dat$scale_name)
dat %>% 
  split(dat$scale_type) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

### CATEGORIZE BEHAVIOR:
# perpetration, victimization, intervention are the big three intervention measures.
# Also, there's involvement, I now see 
# unique(behavioral_data$scale_name)
behavioral_data <- dat %>% 
  filter(scale_type == 'behavior') %>%
  mutate(
  behavior_type = as.factor(case_when(
    str_detect(scale_name, 
               'perp|perpetration|agression|comitted') ~ 'perpetration',
    str_detect(scale_name, 'vict|victimization|completed rape|underwent|ses-past year|survivor') ~ 'victimization',
    str_detect(scale_name, 'bystander|Observing') ~ 'bystander',
    str_detect(scale_name, 'volunteer|support|involvement') ~ 'involvement',
    TRUE ~ 'other')))
# TODO: this was crude, because the scale_names weren't written with this analysis in mind. 
#I think we should rewrite each of the scale names to clearly state which of the
# four categories it falls into, and I will discuss this with JH & Roni 
# when we next speak. Anyway:

sum_lm(behavioral_data, d, behavior_type)
# on average, interventions seem to be more effective at reducing victimization than perpetration, 
# but the differences aren't huge. The effects are pretty small across the board.

behavioral_data %>% 
  split(behavioral_data$behavior_type) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id)) 

robust(rma(yi = d, vi = var_d, mods = behavior_type, data = behavioral_data), cluster = behavioral_data$unique_study_id)

# what about just the RCTs?
rct_behavioral <- behavioral_data %>% 
  filter(study_design == 'rct')

rct_behavioral %>% sum_lm(d, behavior_type)

robust(rma(yi = d, vi = var_d, data = rct_behavioral), 
       cluster = rct_behavioral$unique_study_id)

```

* Evaluate the overall effect size of behavioral outcomes for all of the studies in the dataset, both in the short- and in the long-term. 
```{r behavioral_over_time}

sum_lm(dat, d, scale_type)

robust(x = rma(yi = d, vi = var_d, data = behavioral_data), 
       cluster = behavioral_data$unique_study_id)

# first, meta-analysis with a split for delay, yes, no
behavioral_data %>% 
  split(as.factor(behavioral_data$delay > 0)) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

# declines by about half. now as a regression on the same split
behavioral_data %>% 
  split(as.factor(dat$delay > 0)) %>%
  map(.f = sum_lm,
      d, scale_type)
# in general, delayed results are smaller.

# TODO: rewrite robust(rma) as robust_rma to have this work w/othe ~ and .x
# low priority
```

* Compare the effect size of behavioral outcomes for experimental and quasi-experimental studies vs. non-experimental observational studies, both in the short- and in the long-term. 
* Compare the effect size of behavioral outcomes for studies with random assignment vs. quasi-experimental studies, both in the short- and in the long-term. 

```{r behavioral_outcomes_ranodmization_short_term_long_term}
behavioral_data %>% 
  mutate(rct_and_quasi_design = if_else(
    study_design %in% c('pragmatic rct', 'quasi-experimental', 'rct'), TRUE,
    FALSE)) %>%
  split(.$rct_and_quasi_design == TRUE) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
  
# TODO: this is a single one of the things we want.
# I think what we're looking for is a double-map. That or 
# something a lot more inelegant.
  
```

* Evaluate at the effect size of perpetration behavioral outcomes for only studies with random assignment, both in the short- and in the long-term. 
```{r perpetration outcomes}

rct_behavioral %>%
  split(.$behavior_type) %>%
  map(.f = ~robust(rma(yi = .x$d, vi = .x$var_d), 
                   cluster = .x$unique_study_id))
# still about 0.1
# TODO: these _all_ have delays. maybe something like delay > 1 month?
rct_perp <- rct_behavioral %>% filter(behavior_type == 'perpetration')
summary(lm(formula = d ~ delay, data = rct_perp))
#...kind of not really any clear relationship?
rct_perp %>% ggplot(mapping = aes(x = delay, y = d)) + 
  geom_point() +
  geom_smooth(method = 'lm')
# the story here is the range of Ds -- what's this -0.5 increase in perpetration?
# TODO: double-check that we coded this in the right direction
range(rct_perp$d); mean(rct_perp$d)
# TODO: all this delay stuff could be addresesd with a delay yes/no variable.
# TBD: sensible thing to call that?
```
* Compare the effect size of behavioral outcomes and attitudinal outcomes, both in the short- and long-term, among the quasi-experimental and randomly-assigned studies. (The covariation of attitudes and behaviors will be assessed in the full sample of studies.)
```{r}

rcts_and_quasi <- dat %>% 
  filter(study_design == 'rct' | study_design == 'quasi-experimental')  %>%
  mutate(delayed_outcome = as.factor(delay > 0))

# overall effect size difference between attitudes and behaviors?
rcts_and_quasi %>%
  sum_lm(d, scale_type)

rcts_and_quasi %>% 
  group_split(delayed_outcome) %>% 
  # split(rcts_and_quasi$delayed_outcome) %>% 
  map(.f = ~sum_lm(dataset = .x, y = d, x = scale_type))
# TODO BLARGH why is it so hard to rename the dataframes in a list
```

What is the association between attitudes and behaviors (temporal match) – this answers our question of ‘should we care about attitudes?’

```{r}
# TODO: reformat this whole thing
has_both <- dat %>% 
  filter(attitudes_behaviors == 0) %>%
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d), mean_var_d = mean(var_d), mean_se_d = mean(se_d)) %>%
  select(author, year, paper_title, unique_study_id, 
         mean_d, mean_var_d, mean_se_d, everything()) %>%  
  slice(1) 

cor.test(x = has_both$mean_d[has_both$scale_type == 'attitudes'], 
         y = has_both$mean_d[has_both$scale_type == 'behavior'])

# alternative is pivot_wider
has_both_two <- has_both %>% 
  pivot_wider(id_cols = c(author, year, paper_title, unique_study_id, study_design, labels),
              names_from = scale_type, 
              values_from = c(mean_d, mean_var_d, mean_se_d))


cor.test(x = has_both_two$mean_d_attitudes, 
         y = has_both_two$mean_d_behavior)

# RCTs?
has_both_rcts <- has_both_two %>% filter(study_design == 'rct')
cor.test(x = has_both_rcts$mean_d_attitudes, 
         y = has_both_rcts$mean_d_behavior)

# double-check that
has_both_rcts_alternative <- has_both %>% filter(study_design == 'rct')

cor.test(x = has_both_rcts_alternative$mean_d[has_both$scale_type == 'attitudes'], 
         y = has_both_rcts_alternative$mean_d[has_both$scale_type == 'behavior'])

# what's the slope of these lines
summary(lm(mean_d_behavior ~ mean_d_attitudes, data = has_both_two))
summary(lm(mean_d_behavior ~ mean_d_attitudes, data = has_both_rcts))

```


### Plotting
```{r plotting}

# overall linear plot
ggplot(data = dat, mapping = aes(x = se_d, y = d)) +
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()

#### Visualizing this: plot D and SE, with color corresponding to
#### study type and shape corresponding to lab or field
dplot(condense = T)
# I had really thought there was more of a relationship
dplot(condense = F)
# if we look just at rtcs and quasi experiments?
# dplot(sa_data = rcts_and_quasi)
# What's going on here...are the outlier studies getting a lot of weight
dat %>% filter(d <= 2 & d >= -2) %>% dplot(condense = T)


## Relationship between attitudes and behaviors 

has_both_two %>%
  select(author, year, mean_d_attitudes, mean_d_behavior) %>%
  ggplot(mapping = aes(x = mean_d_attitudes, y = mean_d_behavior)) +
  geom_point(size = 3) + 
  geom_smooth(method = 'lm', se = F, lty = 'dashed') + 
  theme_minimal()

# now same with RCTs 
has_both_rcts %>% 
  select(author, year, mean_d_attitudes, mean_d_behavior, labels) %>%
  ggplot(mapping = aes(x = mean_d_attitudes, y = mean_d_behavior)) +
  geom_point(size = 3) + 
  geom_smooth(aes(fill = NULL), lty = "dashed", fullrange = TRUE, 
              method = "lm",
              show.legend = FALSE, alpha = .1, se = F) + 
  geom_label_repel(mapping = aes(label = labels)) +
  theme_minimal()

# forest plot
rcts_grouped <- dat %>%
  filter(study_design == 'rct') %>%
  group_by(unique_paper_id) %>%
  mutate(mean_d = mean(d), mean_var_d = mean(var_d), mean_se_d = mean(se_d)) %>%
  select(author, year, paper_title, unique_study_id, 
         mean_d, mean_var_d, mean_se_d, everything()) %>%  
  slice(1) 

meta_rcts_grouped <- rma(yi = mean_d, vi = mean_var_d, data = rcts_grouped, slab = labels); meta_rcts_grouped
#forest plot attempt one
pdf('./results/forest-plot.pdf',width = 20, height = 28)
sort.by.error <- rcts_grouped[order(rcts_grouped$mean_se_d), ]
yi <- sort.by.error$mean_d
sei <- sort.by.error$mean_se_d
var_names <- sort.by.error$labels

metaplot(mn = yi, se = sei, 
         labels = var_names,
         xlab = "Effect Size", ylab = "Study", 
         colors = meta.colors(box = "blue",
                              lines = 44,
                              zero = "white",
                              summary = "orange",
                              text = "black"),
         title(main = "SA  Forest Plot"))
abline(v = meta_rcts_grouped$beta, col = "blue", lty = "dashed")
dev.off()
dev.off()
```
