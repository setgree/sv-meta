---
title: "3-SA-meta-analyis.Rmd"
author: "Seth Green"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
editor_options: 
  chunk_output_type: console
---

### Overview

This document will work through our results, and its aim is to both reflect the pre-analyis plan as it was written in 2018 and the most up-to-date version of the text. Everything in our pre-analysis plan will be included in either the main results section or an appendix, but not necessarily in the order it was drafted up.

This document will precede chunks with quotes prepended either a "*paper*" or a "*pap*" to denote whether the text is from the paper or the pre-analysis plan.

## Setup

### load libraries

```{r setup, include=F, echo=F}
rm(list = ls())
library(dplyr, warn.conflicts = F)
library(knitr)
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(ggrepel, warn.conflicts = F)
library(purrr)
library(stringr)
library(tidyr, warn.conflicts = F)
library(tibble)

source('./functions/sum_lm.R')
source('./functions/map_robust.R')
source('./functions/study_count.R')
# NOTE ON SUM_LM: 
# `sum_lm(dat, d, se_d, coef_only = T)` = `summary(lm(formula = d ~ se_d, 
#                                          data = dat))$coefficients`

options(scipen = 99)
```

### load data

```{r load_data, include=F, echo=F}
# read in data

dat <- readRDS(file = './data/sa_meta_data_for_analysis.rds') %>%
  select(unique_study_id, scale_type, everything())

# add in useful variables
dat <- dat %>%
  mutate(decade = as.factor(case_when(
    year <= 1989 ~ "1980s",
    year >= 1990 & year <= 1999 ~ "1990s",
    year >= 2000 & year <= 2009 ~ "2000s",
    year >= 2010 ~ "2010s",
    TRUE ~ "Other")),
    total_n = n_t_post + n_c_post,
    quantile_var = case_when(
    total_n < quantile(total_n, 0.2) ~ 1,
    total_n < quantile(total_n, 0.4) ~ 2,
    total_n < quantile(total_n, 0.6) ~ 3,
    total_n < quantile(total_n, 0.8) ~ 4,
    TRUE ~ 5),
    labels = paste(tools::toTitleCase(word(author)), year),
         ra = ifelse(study_design == 'rct', TRUE, FALSE),
         yes_delay = as.factor(delay > 0),
    published_unpublished = 
           case_when(publication_type == 'published' ~ 'published',
                     NA ~ 'unpublished', 
                     TRUE ~ 'unpublished')) %>% 
  group_by(unique_study_id) %>% 
  mutate(has_both = case_when(all(c('attitudes', 'behavior') %in% scale_type) ~ 'both',
                              scale_type == 'attitudes' ~ 'attitudes',
                              scale_type == 'behavior' ~ 'behavior')) %>%
  ungroup()
```

## Results 

### Overall results and publication bias 

*paper*: "Our random effects meta-analysis of all primary prevention strategies from 1985 to 2018 reveals an overall effect size, across all interventions on all kinds of outcomes, of $\Delta$ = 0.281 (se = 0.024). This effect is statistically significant at p $<$ 0.0001."
```{r overall_meta}

robust(x = rma(yi = d, vi = var_d, data = dat),
       cluster = dat$unique_study_id)
# more verbose version of
dat %>% map_robust()
```


# Figure 1
```{r fig_1}

dat %>% ggplot(mapping = aes(y = d, 
                             x = se_d)) + 
  geom_point(aes(color = behavior_type,
             shape = study_design),
             size = 3) + 
  stat_smooth(method = 'lm', se = F) + 
  ggtitle("Relatoinship between effect sizes and standard errors") +
  labs(x = "Standard errors",
       y = "Effect size",
       color = "Outcome type",
       shape = "Study") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))

```

"First, we do not observe a significant relationship between effect sizes and standard errors: $\beta$ = 0.145 (se = 0.176),  p = 0.411"

```{r publication_bias_overall}
sum_lm(dat, d, se_d, coefs_only = T)
```

"Second, the relationship between SE and $\Delta$ remains insignificant within randomized (N = XXX) and quasi-experimental (N = XXX) designs, but highly significant within observational designs (N = XXX)"

```{r publication_bias_by_study_design}
# Ns
dat %>%
  group_by(study_design) %>%
  study_count()
# effect sizes
dat %>% 
  split(.$study_design) %>% 
  map(.f = ~sum_lm(y = d, x = se_d, dat = .)) # can just do `map(.f = ~sum_lm())
```

"Our third check for publication bias is to separate the literature into studies published in peer-reviewed journals (N = 186) and those not (N = 118)."

```{r published_vs_unpubished}

dat %>% 
  group_by(published_unpublished) %>% 
    study_count()

# sanity check
length(unique(dat$unique_study_id)) == 186 + 118
```

 In our sample, being unpublished is associated with a small reduction in effect size ($\beta$  = -0.0467), but not at a statistically significant level (p = 0.28).
 
```{r publish_relationship_or_not}
dat %>% sum_lm(d, published_unpublished)
```

"Our results are also robust to variation in methodological rigor. The pooled effect size in the largest quintile of studies,  with at least 405 participants combined in treatment and control groups, is $\Delta$ = 0.275 (se = 0.073), while those in our smallest quintile, which have between 20 and 83 participants, is $\Delta$ = 0.302 (se = 0.043). In other words, neither is more 0.02 away from our overall effect size of $\Delta$ = 0.281.

```{r quintile_check}
# what are the quantiles
dat %>% group_by(quantile_var) %>%
  summarise(min_n = min(n_t_post + n_c_post),
            max_n = max(n_t_post + n_c_post))

dat %>% 
  split(.$quantile_var) %>% map(.f = map_robust)
  
## well it's not *nothing*...but it's pretty small
dat %>% sum_lm(y = d, x = quantile_var) # it's basically nothing
```

Neither do we observe a statistically significant relationship between days of delay and effect size: $\beta$ = 0.00023, p = 0.121. (We discuss this relationship, and the difficulties of interpreting it, in an appendix).

```{r d_and_delays}

dat %>% sum_lm(y = d, x = delay)

# TODO: remove or move to appendix
# how many delayed outcomes
table(dat$yes_delay)

dat %>% sum_lm(d, yes_delay)
## What's the longest attitude/behavior delay measurement?
dat %>%
  group_by(scale_type) %>%
  summarize(max_delay = max(delay))
```

However,  when pooling all effects, we observe that results vary both significantly and substantively by study design. 

```{r differences_by_study_design}
# count Ns (again, just for convenience of writing)
dat %>% group_by(study_design) %>% 
  study_count()

# check overall effect sizes
dat %>%
  split(.$study_design) %>%
  map(.f = map_robust)

#TODO: can we reshape these two objects to be one table and then make them a latex table? 
# and then eventually a function which just takes dat as its input?
```

# Attitudinal Change exceeds behavioral change in every category of study

In our sample, attitudinal outcomes outnumber behavioral outcomes by 355 to 145. 207 studies measure attitudes only, 36 measure only behaviors, while 61 measure both. 

```{r att_beh_breakdown}
table(dat$scale_type)

dat %>% 
  group_by(has_both) %>%
  study_count()

# sanity check
length(unique(dat$unique_study_id)) == 207 + 36 + 61 # true
```

Behavioral measurement has become more common over time, particularly from 2010 onward. 

```{r behavioral_outcomes_by_decade}
range(dat$year)

# how many studies in each decade
dat %>% group_by(decade) %>% study_count()

tab <- merge(x = dat %>% group_by(decade) %>% study_count(),
             y = dat %>% group_by(decade, scale_type) %>% summarise(n = n())) 
# print tab
tab

#reshape and make latex table
tab %>%
  pivot_wider(names_from = scale_type, values_from = n) %>%
  mutate(ratio = attitudes/behavior) %>%
  knitr::kable(format = 'latex')

# what was the earliest study to measure behavior as we conceive it?
dat %>% filter(scale_type == 'behavior') %>% 
  arrange(year) %>% 
  select(author, year, paper_title) %>% head()

# optional histogram
ggplot(dat, aes(x = decade, fill = scale_type)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = 
                      c("attitudes" = "#F8766D", "behavior" = "#00BFC4")) +
  labs(x = "Decade", y = "Count", fill = "Scale type") +
  theme_minimal()
```

Overall, behaviors are much more resistant to change than attitudes.  The overall effect size for attitudes is $\Delta$ =  0.367 (se = .03), p < 0.0001; while the overall effect size for behaviors is $\Delta$ = 0.072 (se = 0.022) p = 0.0013. While this is still a statistically significant result, it does not meet the conventional threshold for a 'small' effect size.

```{r behaviors_attitudes_overall}
dat %>% 
  split(.$scale_type) %>%
  map(.f = map_robust)
```

This discrepancy is particularly pronounced in observational research, where attitudinal outcomes are about 18 times larger than behavioral responses; but the discrepancy is substantial across every category of study design. However, it is notable that the largest behavioral effects are found within randomized studies, i.e. the most rigorous designs.

```{r behavior_attitudes_discrepancy_within_study_designs}

# first, Ns

dat %>% 
  group_by(scale_type, study_design) %>% 
  study_count() %>%
  kable('latex')

# then effect sizes
dat %>% 
  split(list(.$scale_type, .$study_design)) %>% 
  map(.f = map_robust)

```

In our dataset, behavioral outcomes can be classified as victimization, perpetration, bystander (intervening in a situation where perpetration is perceived to be a risk), and involvement (for example, signing up to be part of a rape awareness group on campus).

Table XXX shows the number of studies which measure each behavioral category and their pooled effect sizes, as well as attitudes for reference.

```{r categories_of_behavior_and_attitudes}
#Ns
dat %>% 
  group_by(behavior_type) %>% 
  summarize('N (studies)' = n_distinct(unique_study_id)) %>%
  kable('latex')

dat %>% 
  split(.$behavior_type) %>% 
  map(.f = map_robust)


```

Here we see that primary prevention interventions do not, on average, meaningfully reduce victimization or perpetration. This, we think, is the key finding of this meta-analysis. % if I were reading this right now I'd want more zoom in on how different _categories_ of intervention reduce victimization & perpetration. 

### RCT perpetration 

```{r rct_perpetration}
dat %>% 
  filter(study_design == 'rct' & behavior_type == 'perpetration') %>% 
  map_robust()
```
# correlation between attitudes and behavior

Another way to assess the discrepancy between attitudes and behaviors is to focus on the 61 studies that measure both and look at the association between the two categories.

```{r dat_has_both_count}
dat_has_both <- dat %>% filter(has_both == 'both')
dat_has_both %>% study_count()
```

In this subset of studies, the pooled attitudinal effect is $\Delta$ = 0.278 (se = .036), p < .0001, while the pooled behavioral outcome is $\Delta$ = 0.081 (se = .03), p = .009. In other words, the attitudinal effects are slightly lower than the overall attitude effect of 0.367, while the behavioral effects are slightly larger than the overall behavioral effect of 0.072.
```{r has_both_behav_att_meta_results}
dat_has_both %>% split(.$scale_type) %>% map(.f = map_robust)
```

A key question for this literature is: when attitudes change, do behaviors change with them? Unfortunately, in the studies that measure change across both categories, we find a small, statistically insignificant correlation of $\rho$ (59) = 0.141, p = 0.279.

```{r attitude_behavior_correlation_overall}
## TODO John-Henry: is there a way to get the behavior_type that corresponds 
# to the behavior (rather than 'attitude') into this reshaped dataset, 
# such that we could:
# A) subsequently filter it down just to look at each kind behavior
# B) color code the scatterplot 
has_both_means <- dat_has_both %>%
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d), 
         mean_var_d = mean(var_d), 
         mean_se_d = mean(se_d)) %>%
  select(author, year, paper_title, unique_study_id, intervention_name,
         study_design, mean_d, mean_var_d, mean_se_d,
         ra, labels, scale_type, decade, behavior_type) %>%  
  slice(1) 

has_both_reshaped <- has_both_means %>% 
  pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels, decade),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))

cor.test(x = has_both_reshaped$mean_d_attitudes, 
         y = has_both_reshaped$mean_d_behavior)
```

We find broadly similar results when looking at just at RCTs; ($\rho$ (25) = 0.16, p = 0.468); just at studies that measure both attitudes and either victimization or perpetration outcomes ($\rho$ (42) = 0.153, p = 0.322); and, finally, within RCTs that measure attitudes as well as either victimization or perpetration outcomes ($\rho$ (59) = 0.141, p = 0.279).
```{r attitude_behavior_correlation_subsets}
# RCTs only?
rcts_both <- has_both_reshaped %>% filter(study_design == 'rct')
cor.test(x = rcts_both$mean_d_attitudes, 
         y = rcts_both$mean_d_behavior)

#perp and vict only?
perp_and_vict_both <- has_both_means %>%
  group_by(unique_study_id) %>%
  filter(if_else(any(behavior_type %in% c('involvement', 'bystander')), F, T)) %>%
   pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels, decade),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))

cor.test(perp_and_vict_both$mean_d_attitudes,
         perp_and_vict_both$mean_d_behavior)


# finally, perp and vict only in RCTs only

perp_and_vict_rcts <- has_both_means %>%
  group_by(unique_study_id) %>%
  filter(if_else(any(behavior_type %in% c('involvement', 'bystander')), F, T)) %>%
  filter(study_design == 'rct') %>%
   pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels, decade),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))

cor.test(perp_and_vict_rcts$mean_d_attitudes,
         perp_and_vict_rcts$mean_d_behavior)
```

Figure XXX highlights this lack of correlation. Each point in the scatterplot is a study that measures both attitudes and behaviors, with the attitudinal effect size on the X axis and the behavioral effect size on the Y axis. The dotted black line shows what a perfect correlation would look like, and the dotted gray line shows the correlation that we actually find. Studies are color-coded by design.

```{r correlation_scatterplot}
# ok now scatterplot

has_both_reshaped %>%
  ggplot(mapping = aes(x = mean_d_attitudes,
                       y = mean_d_behavior)) + 
  geom_point(aes(color = study_design),
             size = 3) + 
  geom_abline(slope = 1, lty = 'dashed') +
  stat_smooth(lty = "dashed", 
              method = "lm", 
              se = FALSE,
              color = 'grey') +  
  labs(title = "Correlation between attitudinal and behavioral change",
       x = "Effect size (attitudes)",
       y = "Effect size (behaviors)",
       color = "Study Design") +
#  scale_color_discrete(labels = c("pre-post" = "observational")) +
  guides(color = guide_legend(title = "Study Design")) + 
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

In sum, while attitudes towards rape myths appear to malleable, the problem of rape itself is much more resistant to change. 
