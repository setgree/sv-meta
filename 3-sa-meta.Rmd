---
title: "3-SA-meta-analyis.Rmd"
author: "Seth Green"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
editor_options: 
  chunk_output_type: console
---

### Overview

This document will work through our results, and its aim is to both reflect the pre-analyis plan as it was written in 2018 and the most up-to-date version of the text. Everything in our pre-analysis plan will be included in either the main results section or an appendix, but not necessarily in the order it was drafted up.

This document will precede chunks with quotes prepended either a "*paper*" or a "*pap*" to denote whether the text is from the paper or the pre-analysis plan.

## Setup

### load libraries

```{r setup, include=F, echo=F}
rm(list = ls())
library(dplyr, warn.conflicts = F)
library(knitr)
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(ggrepel, warn.conflicts = F)
library(purrr)
library(stringr)
library(tidyr, warn.conflicts = F)
library(tibble)

source('./functions/sum_lm.R')
source('./functions/map_robust.R')
# NOTE ON SUM_LM: 
# `sum_lm(dat, d, se_d, coef_only = T)` = `summary(lm(formula = d ~ se_d, 
#                                          data = dat))$coefficients`

options(scipen = 99)
```

### load data

```{r load_data, include=F, echo=F}
# read in data, add in two useful variables
# labels that paste together author and year for plots;  

raw_dat <- readRDS(file = './data/sa_meta_data_for_analysis.rds')

dat <- raw_dat %>%
  select(unique_study_id, scale_type, everything()) %>%
  mutate(labels = paste(tools::toTitleCase(word(author)), year),
         ra = ifelse(study_design == 'rct', TRUE, FALSE),
         yes_delay = as.factor(delay > 0))
```

## Results 

### Overall results and publication bias 

*paper*: "Our random effects meta-analysis of all primary prevention strategies from 1985 to 2018 reveals an overall effect size, across all interventions on all kinds of outcomes, of $\Delta$ = 0.281 (se = 0.024). This effect is statistically significant at p $<$ 0.0001."
```{r overall_meta}

overall_results <- robust(x = rma(yi = d, vi = var_d, data = dat),
       cluster = dat$unique_study_id); overall_results
overall_results
```

"First, we do not observe a significant relationship between effect sizes and standard errors: $\beta$ = 0.145 (se = 0.176),  p = 0.411"

```{r publication_bias_overall}
sum_lm(dat, d, se_d, coefs_only = T)
```

"Second, the relationship between SE and $\Delta$ remains insignificant within randomized (N = XXX) and quasi-experimental (N = XXX) designs, but highly significant within observational designs (N = XXX)"

```{r publication_bias_by_study_design}
# Ns
dat %>%
  group_by(study_design) %>% 
    summarize(count_unique_study_id = n_distinct(unique_study_id))
# effect sizes
dat %>% 
  split(.$study_design) %>% 
  map(.f = ~sum_lm(y = d, x = se_d, dat = .))
```

"Our third check for publication bias is to separate the literature into studies published in peer-reviewed journals (N = 186) and those not (N = 118)."

```{r published_vs_unpubished}

dat <- dat %>% 
  mutate(published_unpublished = 
           case_when(publication_type == 'published' ~ 'published',
                     NA ~ 'unpublished', 
                     TRUE ~ 'unpublished'))
dat %>%
  group_by(published_unpublished) %>% 
    summarize(count_unique_study_id = n_distinct(unique_study_id))

# sanity check
length(unique(dat$unique_study_id)) == 186 + 118
```

 In our sample, being unpublished is associated with a small reduction in effect size ($\beta$  = -0.0467), but not at a statistically significant level (p = 0.28).
 
```{r publish_relationship_or_not}
dat %>% sum_lm(d, published_unpublished)

```

"Our results are also robust to variation in methodological rigor. The pooled effect size in the largest quintile of studies,  with at least 405 participants combined in treatment and control groups, is $\Delta$ = 0.275 (se = 0.073), while those in our smallest quintile, which have between 20 and 83 participants, is $\Delta$ = 0.302 (se = 0.043). In other words, neither is more 0.02 away from our overall effect size of $\Delta$ = 0.281.

```{r quintile_check}
# what are the quantiles
round(quantile(dat$n_t_post + dat$n_c_post))

# overall results largest and smallest
dat <- dat %>%   
  mutate(sum_var = n_t_post + n_c_post) %>%
   mutate(quantile_var = case_when(
    sum_var < quantile(sum_var, 0.2) ~ 1,
    sum_var < quantile(sum_var, 0.4) ~ 2,
    sum_var < quantile(sum_var, 0.6) ~ 3,
    sum_var < quantile(sum_var, 0.8) ~ 4,
    TRUE ~ 5))

dat %>% 
  split(.$quantile_var) %>% map(.f = map_robust)
  

## well it's not *nothing*...but it's pretty small
dat %>% sum_lm(y = d, x = quantile_var) # it's basically nothing
```

Neither do we observe a statistically significant relationship between days of delay and effect size: $\beta$ = 0.00023, p = 0.121. (We discuss this relationship, and the difficulties of interpreting it, in an appendix).

```{r d_and_delays}

dat %>% sum_lm(y = d, x = delay)

# TODO: remove or move to appendix
# how many delayed outcomes
table(dat$yes_delay)

dat %>% sum_lm(d, yes_delay)
## What's the longest attitude/behavior delay measurement?
dat %>%
  group_by(scale_type) %>%
  summarize(max_delay = max(delay))

```

However,  when pooling all effects, we observe that results vary both significantly and substantively by study design. 

```{r differences_by_study_design}
# count Ns (again, just for convenience of writing)
dat %>% group_by(study_design) %>% 
  summarise(count_unique_study_id = n_distinct(unique_study_id))

# check overall effect sizes
dat %>%
  split(.$study_design) %>%
  map(.f = map_robust)

#TODO: can we reshape these two objects to be one table and then make them a latex table? 
# and then eventually a function which just takes dat as its input?

# sanity check
dat %>% split(.$study_design) %>% 
   map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

```

# Attitudinal Change exceeds behavioral change in every category of study

In our sample, attitudinal outcomes outnumber behavioral outcomes by 355 to 145. 207 studies measure attitudes only, 36 measure only behaviors, while 61 measure both. 

```{r att_beh_breakdown}
table(dat$scale_type)

dat %>% 
  group_by(unique_study_id) %>% 
  mutate(has_both = case_when(all(c('attitudes', 'behavior') %in% scale_type) ~ 'both',
                              scale_type == 'attitudes' ~ 'attitudes',
                              scale_type == 'behavior' ~ 'behavior')) %>%
  ungroup() %>% 
  group_by(has_both) %>%
  summarize(count_unique_study_id = n_distinct(unique_study_id))
# sanity check

length(unique(dat$unique_study_id)) == 207 + 36 + 61 # true
```

Behavioral measurement has become more common over time, particularly from 2010 onward. 

```{r behavioral_outcomes_by_decade}
range(dat$year)

dat <- dat %>%
  mutate(decade = as.factor(case_when(
    year <= 1989 ~ "1980s",
    year >= 1990 & year <= 1999 ~ "1990s",
    year >= 2000 & year <= 2009 ~ "2000s",
    year >= 2010 ~ "2010s",
    TRUE ~ "Other"
  )))

# how many studies in each decade
merge(x = dat %>% group_by(decade) %>% summarise(count_unique_study_id = n_distinct(unique_study_id)), y = dat %>%  group_by(decade, scale_type) %>%
  summarize(count = n()) %>% 
    pivot_wider(names_from = scale_type, values_from = count)) %>%
  mutate(ratio = attitudes/behavior) %>% 
  rename('N studies' = count_unique_study_id) %>% 
  knitr::kable(format = 'latex')

# what was the earliest study to measure behavior as we conceive it?
dat %>% filter(scale_type == 'behavior') %>% 
  arrange(year) %>% 
  select(author, year, paper_title) %>% head()

# optional histogram
ggplot(dat, aes(x = decade, fill = scale_type)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = 
                      c("attitudes" = "#F8766D", "behavior" = "#00BFC4")) +
  labs(x = "Decade", y = "Count", fill = "Scale type") +
  theme_minimal()
```

Overall, behaviors are much more resistant to change than attitudes.  The overall effect size for attitudes is $\Delta$ =  0.367 (se = .03), p < 0.0001; while the overall effect size for behaviors is $\Delta$ = 0.072 (se = 0.022) p = 0.0013. While this is still a statistically significant result, it does not meet the conventional threshold for a 'small' effect size.

```{r behaviors_attitudes_overall}
dat %>% 
  split(.$scale_type) %>%
  map(.f = map_robust)
```

This discrepancy is particularly pronounced in observational research, where attitudinal outcomes are about 18 times larger than behavioral responses; but the discrepancy is substantial across every category of study design. However, it is notable that the largest behavioral effects are found within randomized studies, i.e. the most rigorous designs.

```{r behavior_attitudes_discrepancy_within_study_designs}

# first, Ns

dat %>% 
  group_by(scale_type, study_design) %>% 
  summarize(count_unique_study_id = n_distinct(unique_study_id)) %>%
  kable('latex')

# then effect sizes
dat %>% 
  split(list(.$scale_type, .$study_design)) %>% 
  map(.f = map_robust)

```

In our dataset, behavioral outcomes can be classified as victimization, perpetration, bystander (intervening in a situation where perpetration is perceived to be a risk), and involvement (for example, signing up to be part of a rape awareness group on campus).

Table XXX shows the number of studies which measure each behavioral category and their pooled effect sizes, as well as attitudes for reference.

```{r categories_of_behavior_and_attitudes}
#Ns
dat %>% 
  group_by(behavior_type) %>% 
  summarize('N (studies)' = n_distinct(unique_study_id)) %>%
  kable('latex')

dat %>% 
  split(.$behavior_type) %>% 
  map(.f = map_robust)


```

Here we see that primary prevention interventions do not, on average, meaningfully reduce victimization or perpetration. This, we think, is the key finding of this meta-analysis. % if I were reading this right now I'd want more zoom in on how different _categories_ of intervention reduce victimization & perpetration. 

### RCT perpetration 

```{r rct_perpetration}
dat %>% 
  filter(study_design == 'rct' & behavior_type == 'perpetration') %>% 
  map_robust()
```
# correlation 
Another way to assess the discrepancy between attitudes and behaviors is to focus on studies that measure both and look at the association between the two categories.

Overall, X studies. attitudes Y effect size, Behavior X effect size. Correlation: Z. correlation within RCTs: Q. 
```{r}
has_both <- dat %>% 
  group_by(unique_study_id) %>%
  filter(all(c('attitudes', 'behavior') %in% scale_type)) %>%
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d), 
         mean_var_d = mean(var_d), 
         mean_se_d = mean(se_d)) %>%
  select(author, year, paper_title, unique_study_id, intervention_name,
         study_design, mean_d, mean_var_d, mean_se_d,
         ra, labels, scale_type, decade, behavior_type) %>%  
  slice(1) %>% 
  pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels, decade),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))

cor.test(x = has_both$mean_d_attitudes, 
         y = has_both$mean_d_behavior)

# RCTs only?
rcts_both <- has_both %>% filter(study_design == 'rct')
cor.test(x = rcts_both$mean_d_attitudes, 
         y = rcts_both$mean_d_behavior)
# rly about the same

# ok now scatterplot

has_both %>% ungroup() %>%
  ggplot(mapping = aes(x = mean_d_attitudes,
                       y = mean_d_behavior,
                       )) + 
  geom_point(aes(color = study_design),
             size = 3) + 
  geom_abline(slope = 1, lty = 'dashed') +
 stat_smooth(aes(x = mean_d_attitudes, 
                 y = mean_d_behavior),
             lty = "dashed", 
              method = "lm", 
             se = F) +  
  theme_minimal()
```

scatterplot demonstrating this

Overall we conclude that








