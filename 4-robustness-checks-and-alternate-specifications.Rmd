---
title: "4-robustness-checks-and-alternative-specifications"
author: "Seth Green"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output: html_document
editor_options: 
  chunk_output_type: console
---


## Libraries and data 

```{r setup, include=F, echo=F}
rm(list = ls())
library(dplyr, warn.conflicts = F)
library(lubridate)
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(ggrepel, warn.conflicts = F)
library(purrr)
library(stringr)
library(tidyr, warn.conflicts = F)

source('./functions/dplot.R')
source('./functions/sum_lm.R')
source('./functions/dip_calc.R')
source('./functions/odds_ratio_to_d.R')
# NOTE ON SUM_LM: 
# `sum_lm(dat, d, se_d, coef_only = T)` = `summary(lm(formula = d ~ se_d, 
#                                          data = dat))$coefficients`
```

```{r load_data, include=F, echo=F}
# read in data, add in two useful variables
# labels that paste together author and year for plots;  
# reading later results);
# add attitudes_behaviors (for studies that have both), one or the other;
# new var for random assignment (probably should remove 'contains randomization')

raw_dat <- readRDS(file = './data/sa_meta_data_for_analysis.rds')

dat <- raw_dat %>%
  select(unique_study_id, scale_type, everything()) %>%
  mutate(labels = paste(tools::toTitleCase(word(author)), year),
         ra = ifelse(study_design == 'rct', TRUE, FALSE),
         yes_delay = as.factor(delay > 0))

has_both <- dat %>% 
filter(all(c('attitudes', 'behavior') %in% scale_type)) %>% 
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d), 
         mean_var_d = mean(var_d), 
         mean_se_d = mean(se_d)) %>%
  slice(1) %>% 
  pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))

```

## Pre-registered analyses not in the main text

* Compare the effect size of behavioral outcomes for experimental and quasi-experimental studies vs. non-experimental observational studies, both in the short- and in the long-term. 

```{r behavioral_attenuation_by_study_design}
dat %>%
  filter(scale_type == 'behavior') %>%
split(list(.$study_design, .$yes_delay)) %>% 
  map(~robust(x = rma(yi = .$d,  sei = .$se_d), 
              cluster = .$unique_paper_id))
# 'overall' estimates taken from tables in text
```

* Compare the effect size of behavioral outcomes for studies with random assignment vs. quasi-experimental studies, both in the short- and in the long-term. 

```{r experimental_stricti_vs_non_experimental_delay_and_not}

# how many in each category? 
dat %>% 
  filter(scale_type == 'behavior', study_design != 'pre-post') %>%
  group_by(study_design, yes_delay) %>% 
  tally()
# The following will fail for reasons that  don't make sense to me
# might be a metafor bug
# dat %>% 
#   filter(scale_type == 'behavior', study_design != 'pre-post') %>%
#   split(list(.$study_design, .$yes_delay)) %>%
#   map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
#        cluster = .x$unique_study_id))

# so instead...
dat %>% filter(study_design == 'quasi-experimental',
               scale_type == 'behavior') %>%
  split(.$yes_delay) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
                   cluster = .x$unique_study_id))

dat %>% filter(study_design == 'rct', scale_type == 'behavior') %>%
  split(.$yes_delay) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
                   cluster = .x$unique_study_id))

```

* Evaluate at the effect size of perpetration behavioral outcomes for only studies with random assignment, both in the short- and in the long-term. 

*paper*: For reasons discussed in the main text, perpetration outcomes with no delay are not necessarily capturing the full effect of treatment. We instead split perpetration outcomes within randomized designs by the presence of a delay of at least 30 days or not.

```{r perpetration outcomes}

rct_perp <- dat %>%
  filter(scale_type == 'behavior',
                   behavior_type == 'perpetration')

# behavioral data need time to accumulate 
# so instead of delay, yes/no, how about delay > 1 month (30 days)

rct_perp %>%
  split(.$delay > 30) %>%
  map(.f = ~robust(rma(yi = .x$d, vi = .x$var_d), 
                   cluster = .x$unique_study_id))

```

* Compare the effect size of behavioral outcomes and attitudinal outcomes, both in the short- and long-term, among the quasi-experimental and randomly-assigned studies.

```{r attitudes_behaviors_quasi_random_delay_yes_no}

 dat %>% 
  filter(study_design %in% c('rct', 'quasi-experimental')) %>%
  split(list(.$scale_type, .$yes_delay)) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

```


### A note on clustered study designs

*paper*: We did not anticipate, however, the quantity of cluster-assigned studies in our sample..

```{r cluster_study_redo}

dat %>% group_by(unique_study_id) %>% slice(1) %>% ungroup() %>%
  count(is.na(.$cluster_type))

 dat %>% 
   filter(ra) %>% 
   group_by(unique_study_id) %>% 
   slice(1) %>% ungroup() %>% 
   count(is.na(.$cluster_type))

```

*paper*: In practice, there were 10 studies with fewer than 10 clusters, drawn from 8 papers and comprising 22 effect size estimates in total. If we reclassify all of these studies as RCTs, our overall meta-analytic estimate for RCTs moves from
```{r reclassify_low_cluster_rcts}
dat %>%
  mutate(study_design = if_else(robust_quasi_check == 1,
                                'rct', 
                                as.character(study_design))) %>%
  split(.$study_design) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

```

### Synthesizing dichotomous and continuous dependent variables

*paper*: Using the conventional conversion formula from odds ratio to Cohen's d, ATEs that correspond in reductions of incidence from...

```{r dip_vs_odds_ratio}
odds_ratio_to_d(.6, .3)
dip_calc(.6, .3)
odds_ratio_to_d(.6, .3) == odds_ratio_to_d(.06, .03)
dip_calc(.6, .3) - dip_calc(.06, .03)

# A visualization of the intuition behind this estimator: 
# distribution of SDs drawn from Bernoulli distribution

proportions <- seq(from = 0, to = 1, by = 0.01)
SDs <- as.numeric(lapply(X = proportions, FUN = function(p){sqrt(p * (1 - p))}))

ggplot() + geom_point(aes(proportions, SDs)) + theme_minimal()

# test: what's a DIP that corresponds to approximately d = 0.28,
# our overall average
dip_calc(0.05, 0.015)
```

## robustness checks, alternate specifications, and alternate visualizations
Everything that follows is exploratory and not included in the paper or appendix

### Plotting

```{r viz_attempts}
dat %>% 
  split(.$study_design) %>% 
  map(.x = ., .f = ~dplot(sa_data = ., dot_size = 2, condense = F,
                          dot_informative = T))
 
# overall linear plot
ggplot(data = dat, mapping = aes(x = se_d, y = d)) +
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()

dplot(condense = F)
## Relationship between attitudes and behaviors 

has_both %>%
  ggplot(mapping = aes(x = mean_d_attitudes, y = mean_d_behavior,
                       color = study_design)) +
  geom_point(size = 3) + 
  geom_smooth(method = 'lm', se = F, lty = 'dashed') + 
  theme_minimal()

# now same with RCTs, and with study labels
has_both %>% filter(study_design == 'rct') %>% 
  ggplot(mapping = aes(x = mean_d_attitudes, y = mean_d_behavior)) +
  geom_point(size = 3, color = 'blue') + 
  geom_smooth(aes(fill = NULL), lty = "dashed", fullrange = TRUE,
              method = "lm",
              show.legend = FALSE, alpha = .1, se = F) +
  geom_label_repel(mapping = aes(label = labels)) +
  theme_minimal() +
  labs(x = "mean d (attitudes)",
       y = "mean d (behavior)")
# actually kind of a cool plot when you zoom in

```

### Robustness checks

Group results within study

```{r grouping rbustness checks}

# robustness check: group by study AND scale_type and average the results
dat_grouped <- dat %>%
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) %>%
  slice(1)

robust(x = rma(yi = d, vi = var_d, data = dat_grouped),
       cluster = dat_grouped$unique_study_id)

# alternative specification 
rma(yi = d, vi = var_d, data = dat_grouped)

# once we've grouped by study_id, clustering changes the SE by just 0.0001

# How about not grouping by scale_type/
dat_grouped_two <- dat %>%
  group_by(unique_study_id) %>%
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) %>%
  slice(1)

robust(x = rma(yi = d, vi = var_d, data = dat_grouped_two),
  cluster = dat_grouped_two$unique_study_id)

# again, very similar.
# NOTE FOR TEAM: should we average within studies?
# in the prejudice meta, y'all averaged all the Ds and var_ds
# within each (to create a single point estimate representing each
# study). But here, we are looking for a lot more within-study variance (e.g. to
# attitudes predict # behavior? What about perp/victimization? So I personally
# vote that we not average but take everything. As a robustness check above, I found
# that averaging doesn't change the overall results (it didn't
# in the prejudice meta that much either if I recall correctly). Another
# approach would be, JH, to create a function that has "average within study" as
# an input each time to replace the functions I am using. My intuition is that
# we should not average, because it's more trouble than it's worth to then parse
# out within-study correlations; but I defer to the group.
```

different ways to break down main results:
```{r alternate_breakdowns}

# ra vs not (in paper we break down by study)
dat %>% 
  split(list(.$scale_type, .$ra)) %>%   
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

# We *would* include this next outcome if our behavioral category table 
# included an 'overall' row

dat %>% 
  split(.$behavior_type) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id)) 
# this suggests that moving the needle on perpetration is effectively unsolved problem

# in general, the effects on victimization and  perpetration are larger in the rcts, 
# which is encouraging (and surprising to me).

table(dat$behavior_type)
# TODO: triple-check that thesse are right.

# scale type and delay
dat %>% split(list(.$scale_type,.$yes_delay)) %>% 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
```

sum_lm() checks that don't make it into the final paper

```{r sum_lm checks}
dat %>% 
  split(.$ra) %>% 
  map(.f = ~sum_lm(y = d, x = se_d, dat = .))


# how about the overall relationship between lab/field and d? 
sum_lm(dat, d, lab_field)

# study type?
sum_lm(dat, d, study_design)
# yep, meaningful relationship

# scale type?
sum_lm(dat, d, scale_type)
# that is a HUGE difference

# publication status?
sum_lm(dat, d, publication_type)

dat %>%
  filter(behavior_type == 'perpetration', 
         study_design == 'rct') %>% 
  sum_lm(d, se_d)
```

Correlation between attitudes and behaviors -- standard errors of estimates

```{r attitude_behavior_correlation_standard_error}

# thank you https://stackoverflow.com/questions/16097453/how-to-compute-p-value-and-standard-error-from-correlation-analysis-of-rs-cor
cor.test.plus <- function(x) {
  list(x, 
       Standard.Error = unname(sqrt((1 - x$estimate^2)/x$parameter)))
}
cor.test.plus(
  cor.test(
    x = has_both$mean_d_attitudes, 
    y = has_both$mean_d_behavior))

```

Different ways of assessing effect size change over time

```{r effect_size_over_time}

dat %>% ggplot((aes(x = year, y = d))) + 
  geom_point() +   
  geom_smooth(method = 'lm') + 
  theme_minimal()
# basically no slope

dat %>% sum_lm(d, year)
dat %>% split(list(.$ra, .$scale_type)) %>%
  map(~ ggplot(data = ., mapping = aes(x = year, y = d)) + 
        geom_point() + 
        geom_smooth(method = 'lm') +
        theme_minimal())

# these are more or less the same results as  Anderson, L. A., & Whiston, S.C. (2005)

# visualizing how little attitudes and behavioral outcomes have changed over time 
dat %>% 
  group_by(year, scale_type) %>% 
  summarise(mean_d = mean(d), 
            mean_se_d = mean(se_d)) %>%
  ggplot(aes(x = year, y = mean_d, 
             group = scale_type)) +
  geom_point(aes(color = scale_type)) +
  geom_smooth(method = 'lm', aes(color = scale_type), se = F) + 
  theme_minimal() +
  scale_x_continuous(breaks = seq(1986, 2018, 5)) +
    labs(x = "year",
       y = "mean d") +
  labs(color = "Attitudes or behaviors")
```

additional ways to look at rct perp outcomes

```{r rct_perp}
# not a big difference

rct_perp %>% ggplot(mapping = aes(x = delay, y = d)) + 
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()

rct_perp %>% filter(delay > 30) %>%
ggplot(mapping = aes(x = se_d, y = d)) + 
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()
# a stronger relationship
```

### Quintile check
```{r quintile}
dat %>% ungroup %>% arrange(n_t_post) %>%
  mutate(quintile = ntile(n_t_post, 5)) %>% 
  group_by(unique_paper_id, intervention_name) %>% 
  split(.$quintile) %>%
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
                   cluster = .x$unique_study_id))

# just reinforces that there's no meaningful relationship between d and se; if anything the highestlargest decile seems to have the largest effect sizes
```

what about things that get tested more than once?

```{r repeat_tests}

dat %>% 
  filter(intervention_name == "bringing in the bystander") %>% 
  split(.$scale_type) %>% 
  map(~robust(x = rma(yi = .$d,  sei = .$se_d), 
              cluster = .$unique_paper_id))

```
