---
title: "1-download-and-clean-data.Rmd"
author: "Seth Green and John-Henry Pezzuto"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
editor_options: 
  chunk_output_type: console
---

### setup
```{r setup}
rm(list = ls())
library(dplyr, warn.conflicts = FALSE)
library(googledrive)
library(readr)
library(stringr)
library(tidyr)
options(scipen = 99)
```

### Download most up-to-date dataset
```{r download_data, eval=F}
# download sheet from google -----------------------------------
drive_auth(email = T)
drive_download("Primary Prevention Meta 2020",
               path = "./data/sa_meta_data",
               overwrite = T,
               type = 'csv') # dl to csv gets just first sheet

```


```{r clean_data}
raw_dat <- read.csv('./data/sa_meta_data.csv', na.strings = c("NA")) %>% as_tibble()
readr::problems(raw_dat) # none as of March 2022

# drop 'eliminate' studies & add unique paper ID
raw_dat <- raw_dat %>%
  fill(paper_title) %>%
  replace_na(list(eliminiate = 0)) %>% 
  filter(!eliminiate == 1) %>%
  group_by(paper_title) %>%
  mutate(first_line = row_number() == 1) %>%
  ungroup() %>%
  mutate(unique_paper_id = cumsum(first_line))

#' drop team-internal stuff 
colnames(raw_dat)
raw_dat <- raw_dat %>% 
  select(-c(coder, page_number, comments, d, var_d, se_d, checked, checker_comment, first_line))
```

#' prepare Cohen's D calcs

making sure that inputs are all what the functions expect: eff_type, cluster_type, and checking for NAs

```{r standardizing_inputs}
unique(raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'd.i.d', replacement = 'd_i_d', x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'f.test', replacement = 'f_test', x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'd.i.m', replacement = 'd_i_m', x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'difference_in_proportion', replacement = 'd_i_p', x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'reg.coef', replacement = 'reg_coef', x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 't.test', replacement = 't_test', x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'beta', replacement = 'reg_coef', x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'log.odds.ratio', replacement = 'log_odds_ratio', x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'odds.ratio', replacement = 'odds_ratio', x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'NA', replacement = NA, x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = '-', replacement = NA, x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'did', replacement = "d_i_d", x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'dim', replacement = "d_i_m", x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = "^$|^ $", replacement =  NA, x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'd.i.p', replacement = "d_i_p", x = raw_dat$eff_type)
raw_dat$eff_type <- gsub(pattern = 'd.i.p.', replacement = "d_i_p", x = raw_dat$eff_type)
unique(raw_dat$eff_type)


# fix cluster type data
raw_dat$cluster_type <- gsub('n_groups',
                             'n groups', raw_dat$cluster_type)
raw_dat$cluster_type <- gsub('-', NA, raw_dat$cluster_type)
raw_dat$cluster_type <- gsub("^$|^ $", NA, raw_dat$cluster_type)

## how many of these are going to require non-standard procedures
## or potential re-doing of the recording
table(raw_dat$eff_type, useNA = 'ifany') 
# As of March 2022 there were none, we've either pinklined/redlined or filled them in
```

#'ctrl_SDs

```{r ctrl_sds }
class(raw_dat$ctrl_sd)
table(raw_dat$ctrl_sd)

which(raw_dat$ctrl_sd == "")
# as of 06/18/2021, this is no rows

#' convert "no.sd" and "-" to NA
raw_dat$ctrl_sd <- gsub(pattern = 'no.sd', replacement = NA, x = raw_dat$ctrl_sd)
raw_dat$ctrl_sd <- gsub(pattern = '-', replacement = NA, x = raw_dat$ctrl_sd)
table(raw_dat$ctrl_sd)

#' Before convert to numeric,check which  will be converted to NA "by coercion" 
sum(which(is.na(as.numeric(raw_dat$ctrl_sd))) != which(is.na(raw_dat$ctrl_sd)))
# as of 6/18/2021. there are no studies converted by coercion
# note: if there are some, re-download the data
raw_dat$ctrl_sd <- as.numeric(raw_dat$ctrl_sd)

#' Convert all ctrl_SDs to 1 when Cohen's D is eff_type
#' probably not strictly necessary but good practice to note this
raw_dat <- raw_dat %>% 
  mutate(ctrl_sd = if_else(condition = (eff_type == 'd'),
                           true = 1, false = ctrl_sd))

## are there any ctrl_sds that are NA when that's going to break the calculator?
na_dat <- raw_dat %>% filter(is.na(ctrl_sd)) %>%
  filter(eff_type == 'd_i_d' | eff_type == 'd_i_m') %>%
    select(author, year, paper_title, eff_type, u_s_d, ctrl_sd,
         n_t_post, n_c_post, n_t_group, n_c_group,
         unique_paper_id, intervention_name,
         study_design, scale_name, is_red)
## Fixed all these! Thank goodness (08-31-2021)

table(raw_dat[is.na(raw_dat$ctrl_sd),]$eff_type, useNA = 'ifany')  
which(!is.na(raw_dat$ctrl_sd) & is.na(raw_dat$eff_type)) # 0 as of 06/18/21

if(nrow(na_dat) == 0) {
  rm(na_dat) 
  } else {
  print("Check the NAs")
  }

# Noting for posterity that I used chatGPT to write the above if/else   
```

# Effect Types & Cluster Types
```{r eff_types}
table(raw_dat$eff_type)
range(raw_dat$u_s_d, na.rm = T) 

## change the cluster randomized var to be factors with an NA rather than a -
raw_dat$cluster_type <- gsub(pattern = '-', replacement = NA, x = raw_dat$cluster_type)
```

# Ns
Through inspection we see that there are blanks, spaces, dashes, and 'not given'
where we need all numeric values. So here we replace each of these with NA

```{r Ns_characters_become_NAs}
raw_dat[, c('n_t_pre', 'n_t_post', 'n_c_pre', 
            'n_c_post', 'n_t_group', 'n_c_group')] <-
  lapply(raw_dat[, c('n_t_pre', 'n_t_post', 'n_c_pre', 'n_c_post', 'n_t_group',
                     'n_c_group')], function(x) {
                       x <- gsub(pattern = '-', replacement = NA, x = x)
                       x <- gsub(pattern = '^\\s*$', replacement = NA, x = x)
                       x <- gsub(pattern = 'not given', replacement = NA, x = x)
                       return(x)
                     })
```

I asked chatGPT to explain the regular expression part of this and got
> In regular expressions, ^ matches the start of the string and $ matches the end of the string. \\s represents any whitespace character (space, tab, newline, etc.). So ^\\s*$ means "match any string that contains zero or more whitespace characters from the start to the end of the string".

> Therefore, gsub(pattern = '^\\s*$', replacement = NA, x = x) will replace any strings that contain only whitespace characters (and nothing else) with NA. This can be useful to catch cases where a field appears to be empty, but in reality contains some whitespace characters.

Now to convert everything to numeric
```{r further_N_clearning}
class(raw_dat$n_c_post)
table(raw_dat$n_c_post)
table(raw_dat$n_t_pre)

cols_to_convert <- c('n_t_pre', 'n_t_post', 'n_c_pre', 
                     'n_c_post', 'n_t_group', 'n_c_group',
                     'u_s_d', 'anticipated_direction')
raw_dat[cols_to_convert] <- lapply(raw_dat[cols_to_convert], as.numeric)

## Now checking for outstanding NAs where you really need one, which are n_t_post & n_c_post

na_vals <- subset(raw_dat, is.na(anticipated_direction) | 
                      apply(select(raw_dat, c('n_t_post', 'n_c_post')), 1, anyNA)) 

## beautiful, rm
rm(na_vals)
```

### check lab/field variable
```{r lab_field}
unique(raw_dat$lab_field)
sum(is.na(raw_dat$lab_field)) # 0 as of 09/19
raw_dat$lab_field <- as.factor(raw_dat$lab_field)

```

# fix study_design var and scale type
```{r study_design}
raw_dat$study_design <- gsub(pattern = 'cross sectional',
                             replacement = 'cross-sectional',  
                             x = raw_dat$study_design)
raw_dat$study_design <- gsub(pattern = 'prepost', replacement = 'pre-post',  x = raw_dat$study_design)
raw_dat$study_design <- as.factor(raw_dat$study_design)

raw_dat <- raw_dat %>% 
  select(-lab_or_field)

# scale type
raw_dat$scale_type <- as.factor(raw_dat$scale_type)
```


### unique study id
```{r}
raw_dat <- raw_dat %>%
  group_by(unique_paper_id, intervention_name) %>% 
  mutate(unique_study_id = cur_group_id())
# based this on https://stackoverflow.com/questions/62562499/obtain-a-unique-id-by-group-in-mutate-pipeline
# hope it's right

```

### Provisional: make everything under 10 clusters quasi-experimental
As Paluck et al. note in 'progress and challenges'
> in cluster-randomized experiments, groups of people, rather than individuals, are randomly assigned to experimental conditions. Reliably estimating standard errors requires at least 10 clusters, but many intervention studies do not approach even this minimal number.
So the cutoff is ten

```{r too_few_clusters}
raw_dat$study_design <- as.character(raw_dat$study_design)

table(raw_dat$study_design)

raw_dat <- raw_dat %>%
  mutate(robust_quasi_check = if_else(!is.na(n_t_group) &
      study_design == 'rct' &
      n_t_group + n_c_group < 10, TRUE, FALSE)) %>%
  mutate(study_design = if_else(
    !is.na(n_t_group) &
      study_design == 'rct' &
      n_t_group + n_c_group < 10,  'quasi-experimental',
    study_design))

# how many did that affect
redesignated <- raw_dat %>% filter(robust_quasi_check) %>%
  select(author, year, intervention_name, scale_name, delay, 
         unique_study_id, unique_paper_id)
# TODO: check that these are really all RCTs, it's not that many
raw_dat$study_design <- as.factor(raw_dat$study_design)
# how many studies, how many papers?
n_distinct(redesignated$unique_study_id) # 10
n_distinct(redesignated$unique_paper_id) # 8

```

### fix delay

```{r delay}
# TODO: double-check dat$delay. it's a string vector and we need to convert everything to days. 
# I'm doing a lubridate thing here but obviously this is not perfect
# I am assuming that NA = 0 days because if we had that info, we'd record it. 
# I know there's a better way to do this and I will consult with JH

# raw_dat$delay <- gsub(NA, '0', raw_dat$delay)
# raw_dat$delay <- gsub('NA', '0', raw_dat$delay

raw_dat$delay <- gsub('4 month', '122', raw_dat$delay)
raw_dat$delay <- gsub('2.5 years', '912.5', raw_dat$delay)
raw_dat$delay <- gsub('2 weeks', '14', raw_dat$delay)
raw_dat$delay <- gsub('3 weeks', '21', raw_dat$delay)
raw_dat$delay <- gsub('1 month', '30.5', raw_dat$delay)
raw_dat$delay <- gsub('5 weeks', '35', raw_dat$delay)
raw_dat$delay <- gsub('14 days', '14', raw_dat$delay)

raw_dat$delay[is.na(raw_dat$delay)] = "0"

table(raw_dat$delay)
raw_dat$delay <- as.numeric(raw_dat$delay)

```

### categorize behavior

```{r categorize_behavior}
### CATEGORIZE BEHAVIOR:
# perpetration, victimization, intervention are the big three intervention measures.
# Also, there's involvement, and then things that are harder to classify.
#behavioral_data <- dat %>% 
  
raw_dat <- raw_dat %>%
#  filter(scale_type == 'behavior') %>%
  mutate(scale_name = str_to_lower(scale_name),
  behavior_type = as.factor(case_when(
    scale_type == 'behavior' & 
    str_detect(scale_name, 
               'perp|perpetration|agression|comitted|aggression|aggressive') ~ 'perpetration',
    scale_type == 'behavior' & 
    str_detect(scale_name, 'vict|victimization|completed rape|underwent
               |ses-past year|survivor') ~ 'victimization',
    scale_type == 'behavior' & 
    str_detect(scale_name, 'bystander|observing|intervention|intervene') ~ 'bystander',
    scale_type == 'behavior' & 
    str_detect(scale_name, 'volunteer|support|involvement') ~ 'involvement',
    scale_type == 'attitudes' ~ 'attitude',
    TRUE ~ NA_character_)))


```

### is_red: loading?
```{r is_red, eval=F}
raw_dat %>% filter(is_red == "Loading...") %>% select(author, year, unique_study_id, is_red)
# GENERAL CHECK: make sure there are no rows that meet this condition
# As of 02/07/22, there are none
raw_dat <- raw_dat %>% select(-is_red)
```

### add useful levels
```{r levels}
levels(raw_dat$publication_type) <- c("published", "dissertation", "government report", "unpublished", "formal evaluation")
levels(raw_dat$origin) <- c("DeGue", "online search", "sent to us by authors")

# TODO: cleanup the following variables
# participant sex
raw_dat %>% group_by(participant_sex) %>% select(author, year, participant_sex, paper_title) %>% slice(1) 
# stage in life
raw_dat %>% group_by(stage_in_life) %>% select(author, year, participant_sex, paper_title) %>% slice(1) 
# there are probably others
## TOOD: CHECK EVERYTHING AGAINST coding instructions.docx
```

### save as clean data and start the Cohen's D script
```{r save_clean_data}

saveRDS(object = raw_dat, file = './data/sa_meta_data_cleaned.rds')

## remove raw data
# file.remove('./data/sa_meta_data.csv')
```

**Session info**
```{r sesion_info}
sessioninfo::session_info()

```