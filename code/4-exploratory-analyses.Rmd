---
title: "4-exploratory-analyses"
author: "Seth Green"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
editor_options: 
  chunk_output_type: console
---

### Setup 

```{r setup, echo=F}
# libraries
library(dplyr, warn.conflicts = F)
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(ggrepel, warn.conflicts = F)
library(purrr)
library(stringr)
library(tidyr, warn.conflicts = F)
library(tibble)

# data
dat <- readRDS(file = '../data/sv_meta_data.rds') |>
  select(unique_study_id, scale_type, everything())

# functions
source('./functions/sum_lm.R')
source('./functions/map_robust.R')
source('./functions/study_count.R')

# personal preference for how digits get rendered
options(scipen = 99)
```


## Robustness checks

**Group results within study**

```{r grouping rbustness checks}
# robustness check: group by study AND scale_type and average the results
dat_grouped <- dat |>
  group_by(unique_study_id, scale_type) |>
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) |> 
  slice(1) 

dat_grouped |> map_robust()

# alternative specification -- don't cluster on unique_study_id

rma(yi = d, vi = var_d, data = dat_grouped)

# How about not grouping by scale_type
dat |>
  group_by(unique_study_id) |>
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) |>
  slice(1) %>% map_robust()

# slightly raises the SE

rm(dat_grouped)
```


**Different ways to subset and then analyze the data:**

```{r alternate_breakdowns}
# ra vs not (in paper we break down by study)
dat |> 
  split(~interaction(scale_type, ra)) |>   
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

# by behavior type overall
table(dat$behavior_type)

dat |> 
  split(~behavior_type) |> 
  map(map_robust)

# scale type and delay
dat |> 
  split(~interaction(scale_type,yes_delay)) |> 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
```

**sum_lm() checks that don't make it into the final paper**

```{r sum_lm checks}
dat |>  
  split(~ra) |>  # ra = random assignment (so RCTs vs. everything else)
  map(.f = ~sum_lm(y = d, x = se_d, dat = .))


# how about the overall relationship between lab/field and d? 
sum_lm(dat, d, lab_field)

# study type?
sum_lm(dat, d, study_design)
# yep, meaningful relationship

# scale type?
sum_lm(dat, d, scale_type)

# publication status?
sum_lm(dat, d, publication_type)

dat |>
  filter(behavior_type == 'Perpetration', 
         study_design == 'Randomized Control Trial') |> 
  sum_lm(d, se_d)
```


**Quintile check**
```{r quintile}
dat |> arrange(n_t_post) |>
  mutate(quintile = ntile(n_t_post, 5)) |> 
  group_by(unique_paper_id, intervention_name) |> 
  split(~quintile) |>
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
                   cluster = .x$unique_study_id))

```

Just reinforces that there's no meaningful relationship between d and se; if anything the largest decile seems to have the largest effect sizes.

**What is the 'bringing in the bystander' average effect on ideas/behaviors**
```{r repeat_tests}
dat |> 
  filter(intervention_name == "bringing in the bystander") |> 
  split(~scale_type) |> 
  map(map_robust)
# a nice-to-have would be a modified map_robust that, if there weren't enough 
# rows in a set it would just tell you that and continue on with the meta-analysis
# instead of stopping but, for now;

dat |>
  filter(intervention_name == "bringing in the bystander") |>
  group_by(behavior_type) |>
  summarise(count = n())

# so it's pretty much all bystander, with one victmization outcome
dat |> 
  filter(intervention_name == "bringing in the bystander",
         behavior_type == 'Victimization') |> 
  select(paper_title, author, year, d, se_d)
# d = 0.02
```

**Behavioral measurement has become more common over time**
Particularly from 2010 onwards
```{r behavioral_outcomes_by_decade}
range(dat$year)

# how many studies in each decade
dat |> group_by(decade) |> study_count()

merge(x = dat |> group_by(decade) |> study_count(),
      y = dat |> group_by(decade, scale_type) |> summarise(n = n())) 


# what was the earliest study to measure behavior as we conceive it?
dat |> filter(scale_type == 'behavior') |> 
  arrange(year) |> 
  select(author, year, paper_title) |> head()

```


## Supplementary figures

**bystander interventions are a product of their time and place**
```{r bystander_interventions_over_time}

bystander_dat <- dat |>
  filter(
    str_detect(paper_title, "bystander") |
      str_detect(brief_description_of_the_intervention, "bystander") |
      str_detect(intervention_name, "bystander") |
      str_detect(program_name, "bystander") |
      str_detect(stated_purpose, "4")) |>
    select(author, year, paper_title, unique_study_id, study_design,
           d, var_d, se_d, scale_type,
           behavior_type, has_both, country) |>
  mutate(perp_vict_grouped = case_when(
    behavior_type == 'Victimization' | behavior_type == 'Perpetration' ~ 'perp_vict',
    behavior_type == 'Involvement' ~ 'involvement',
    behavior_type == 'ideas' ~ 'ideas',
    behavior_type == 'Bystander' ~ 'Bystander'))

bystander_dat |> group_by(unique_study_id) |> slice(1) |> ungroup() |>
  ggplot(aes(x = year)) +
  geom_line(stat = 'count', color = 'light blue') + 
  labs(x = "Year", 
       y = "Count", 
       title = "Bystander interventions over time") +
  # scale_x_date(date_breaks = "2 years") +
  scale_x_continuous(n.breaks = 20) +
  scale_y_continuous(n.breaks = 15) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

## BTW an alternate way to do this
dat |>
  group_by(unique_study_id) |>
  summarise(
    bystander_count = ifelse(
      any(
        str_detect(c(
          brief_description_of_the_intervention, 
          intervention_name, program_name, paper_title), "bystander")) |
        any(str_detect(stated_purpose, "4")),
      TRUE, FALSE)
  ) |> 
  summarise(
    bystander = sum(ifelse(is.na(bystander_count), FALSE, bystander_count)),
    no_bystander = sum(ifelse(is.na(bystander_count), TRUE, !bystander_count))
  )

# where do bystander interventions occur
bystander_dat |> group_by(country) |> study_count()
rm(bystander_dat)
```

**Different ways of assessing effect size change over time**

```{r effect_size_over_time}
dat |> ggplot((aes(x = year, y = d))) + 
  geom_point() +   
  geom_smooth(method = 'lm') + 
  theme_minimal()
# basically no slope

dat |> sum_lm(d, year)
dat |> split(~interaction(ra, scale_type)) |>
  map(~ ggplot(data = ., mapping = aes(x = year, y = d)) + 
        geom_point() + 
        geom_smooth(method = 'lm') +
        theme_minimal())
# more or less the same results as  Anderson, L. A., & Whiston, S.C. (2005)
```


**Categories of outcome over time** 
```{r outcome_categories_over_time}
dat |>
  ggplot(aes(x = year, color = behavior_type)) + 
  geom_line(stat = 'count') + 
  labs(x = "Year", 
       y = "Count", 
       color = "Behavior type",
       title = "Categories of behavioral outcomes over time") +
  scale_x_continuous(n.breaks = 10) +
  scale_y_continuous(n.breaks = 10) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```


**ideas and behavior effect sizes over time**
```{r att_beh_over_time}
dat |> 
  group_by(year, scale_type) |> 
  summarise(mean_d = mean(d), 
            mean_se_d = mean(se_d)) |>
  ggplot(aes(x = year, y = mean_d, 
             group = scale_type)) +
  geom_point(aes(color = scale_type)) +
  geom_smooth(method = 'lm', aes(color = scale_type), se = F) + 
  theme_minimal() +
  scale_x_continuous(breaks = seq(1986, 2018, 5)) +
    labs(x = "year",
       y = "mean d") +
  labs(color = "ideas or behaviors")
# both seem to be trending up, more behaviors than ideas
# how much is this b/c of bystander and involvement outcomes?

dat |> 
  filter(behavior_type != "Bystander", behavior_type != 'Involvement') |>
  group_by(year, scale_type) |> 
  summarise(mean_d = mean(d), 
            mean_se_d = mean(se_d)) |>
  ggplot(aes(x = year, y = mean_d, 
             group = scale_type)) +
  geom_point(aes(color = scale_type)) +
  geom_smooth(method = 'lm', aes(color = scale_type), se = F) + 
  theme_minimal() +
  scale_x_continuous(breaks = seq(1986, 2018, 5)) +
    labs(x = "year",
       y = "mean d") +
  labs(color = "ideas or behaviors")
# indeed slope goes down, but still up. interesting
```


**additional ways to look at rct perp outcomes**
```{r rct_perp}
# not a big difference
rct_perp <- dat |> filter(study_design == 'Randomized Control Trial',
                          behavior_type == 'Perpetration')
rct_perp |> ggplot(mapping = aes(x = delay, y = d)) + 
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()

rct_perp |> filter(delay > 30) |>
ggplot(mapping = aes(x = se_d, y = d)) + 
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()
# a stronger relationship
```

**A rough idea for a  D/SE plot**

analogous to the D/SE plot in the contact hypothesis paper. It turned out to not be very informative. 

```{r d_se_plot}
dat |>
  drop_na(study_design) |> 
  ggplot(aes(x = se_d, y = d)) +
  geom_point(aes(color = behavior_type,
                 shape = study_design),
             size = 3) +
  geom_smooth(method = "lm",
              se = F) +
  #ggtitle("Relationship between effect sizes and standard errors") +
  labs(x = "Standard errors",
       y = "Effect size",
       color = "Outcome type",
       shape = "Study Design") +
  theme_bw() +
  theme(axis.text = element_text(size = 12),
        legend.text = element_text(size = 12),
        legend.position = "bottom",
        legend.box = "vertical")

# alternative where we separate by outcome type
dat |> ggplot(mapping = aes(y = d, 
                             x = se_d)) + 
  geom_point(aes(color = behavior_type,
             shape = study_design),
             size = 3) + 
  stat_smooth(method = 'lm', se = F) + 
  ggtitle("Relationship between effect sizes and standard errors") +
  labs(x = "Standard errors",
       y = "Effect size",
       color = "Outcome type",
       shape = "Study Design") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))

```


**Forest plot alternative to correlation scatterplot**

Here's a messier way of slicing the data to get at the relationship between attitudinal and behavioral change:

```{r att_beh_change}

## Relationship between ideas and behaviors 
has_both <- dat |> filter(has_both == 'both') |> 
  group_by(unique_study_id, scale_type) |>
  mutate(mean_d = mean(d), 
         mean_var_d = mean(var_d), 
         mean_se_d = mean(se_d)) |>
  select(author, year, paper_title, unique_study_id, intervention_name,
         study_design, mean_d, mean_var_d, mean_se_d,
         ra, labels, scale_type, decade, behavior_type) |>  
  slice(1) |> 
  pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels, decade),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))
```

Then, looking at the correlation between ideas and behaviors just within the RCTs, and with study labels:

```{r labeled_correlation_plot}
has_both |> 
  filter(study_design == 'Randomized Control Trial') |> 
  ggplot(mapping = aes(x = mean_d_ideas, y = mean_d_behavior)) +
  geom_point(size = 3, color = 'blue') + 
  geom_smooth(aes(fill = NULL), lty = "dashed", fullrange = TRUE,
              method = "lm",
              show.legend = FALSE, alpha = .1, se = F) +
  geom_label_repel(mapping = aes(label = labels)) +
  theme_minimal() +
  labs(x = "mean d (ideas)",
       y = "mean d (behavior)")
```

hard to read in this in a PDF, but if you're curious about studies that produce change in ideas but not behaviors, this is a good place to start -- look at some of the outliers.

**Forest plot**

This was a different way to visualize the (lack of) correlation between ideas-based and behavioral outcomes

dat for forest plot and sign tests:

```{r forest_plot_dat_and_sign_tests}
dat_for_forest_plot <- dat |> 
  select(unique_study_id, scale_type, everything()) |>
  filter(study_design == 'Randomized Control Trial') |>
  group_by(unique_study_id) |>
         filter(all(c('ideas', 'behavior') %in% scale_type)) |>
  mutate(study_names = paste0(tools::toTitleCase(word(author)), " ",
                              year, " ", "(", scale_type, ")")) |>
  group_by(unique_paper_id, scale_type) |>
  mutate(d = mean(d), var_d = mean(var_d), se_d = mean(se_d)) |>
  slice(1) |>
  ungroup(scale_type) |> 
  mutate(mean_se = mean(se_d)) |>
  arrange(mean_se) |>
  ungroup() |>
  select(study_names, d, se_d, var_d,
         scale_type, unique_study_id, unique_paper_id)

# sign test: how many studies have an ideas d that's > behavior d
dat_for_forest_plot |> group_by(unique_paper_id) |>
  summarise(mean_d_ideas = mean(d[scale_type == "ideas"]),
            mean_d_behavior = mean(d[scale_type == "behavior"])) |>
  mutate(comparison = case_when(
    mean_d_ideas > mean_d_behavior ~ "Higher",
    mean_d_ideas == mean_d_behavior ~ "Same",
    mean_d_ideas < mean_d_behavior ~ "Lower"
  )) |>
  count(comparison) |>
  rename(Comparison = comparison, Count = n)
binom.test(15, 21, 0.5)
```

next, regenerating the forest plot from the file itself, but not filtering on RCTs:

```{r dat_for_forest_plot_not_rcts}
readRDS(file = '../data/sv_meta_data.rds') |> 
  select(unique_study_id, scale_type, everything()) |>
  group_by(unique_study_id) |>
         filter(all(c('ideas', 'behavior') %in% scale_type)) |>
  mutate(study_names = paste0(tools::toTitleCase(word(author)), " ",
                              year, " ", "(", scale_type, ")")) |>
  group_by(unique_paper_id, scale_type) |>
  mutate(d = mean(d), var_d = mean(var_d), se_d = mean(se_d)) |>
  slice(1) |>
  ungroup(scale_type) |> 
  mutate(mean_se = mean(se_d)) |>
  arrange(mean_se) |>
  ungroup() |>
    group_by(unique_paper_id) |>
  summarise(mean_d_ideas = mean(d[scale_type == "ideas"]),
            mean_d_behavior = mean(d[scale_type == "behavior"])) |>
  mutate(comparison = case_when(
    mean_d_ideas > mean_d_behavior ~ "Higher",
    mean_d_ideas == mean_d_behavior ~ "Same",
    mean_d_ideas < mean_d_behavior ~ "Lower"
  )) |>
  count(comparison) |>
  rename(Comparison = comparison, Count = n)
binom.test(36, (36 + 12 + 1), 0.5)
# This code could stand to be cleaned up a lot
```

**Forest Plot (Figure):**

```{r forest_plot}
head(dat_for_forest_plot) # checking we've already created subset (chunk above)
# overall effect size
overall_forest <- dat_for_forest_plot |>
  split(~scale_type) |>
  map(map_robust)

dat_for_forest_plot |> 
  add_row(study_names = "Overall (ideas)",
          d = overall_forest[[1]]$beta,
          se_d = overall_forest[[1]]$se,
          scale_type = "ideas",
          unique_paper_id = NA) |>
  add_row(study_names = "Overall (Behavior)",
          d = overall_forest[[2]]$beta,
          se_d = overall_forest[[2]]$se,
          scale_type = "behavior",
          unique_paper_id = NA) |>
  mutate(index = row_number()) |>
  relocate(index, .before = 1) |>
  ggplot(aes(y = index, x = d, xmin = d - (1.96 * se_d),
             xmax = d + (1.96 * se_d))) +  
  geom_point(size = 1) +
  geom_errorbarh(height = .1, aes(color = scale_type)) +
  geom_vline(xintercept = 0, color = "black", alpha = .5) +
  scale_x_continuous(name = expression(paste("Glass's", " ", Delta))) +
  scale_y_continuous(name = "", breaks = 1:length(dat_for_forest_plot$study_names),
                     trans = "reverse", labels = dat_for_forest_plot$study_names) + 
  ylab("Study") +
  geom_vline(xintercept = overall_forest$ideas$beta, color = '#F8766D', lty = 'dashed') +
  geom_vline(xintercept = overall_forest$behavior$beta, color = '#00BFC4', lty = 'dashed') +
  labs(color = "ideas or behaviors") +
  theme_minimal() + 
  theme(axis.text.y = element_text(face = ifelse(
    dat_for_forest_plot$study_names %in% 
      c("Overall (ideas)", "Overall (Behavior)"), 
    "bold", "plain"), 
    margin = margin(t = 0.5, b = 0.5, unit = "cm"))) +
  ggtitle("Ideas and Behavioral Changes Compared")
```

### Reclassifying the robust quasi check studies as RCTs
They had random assignment,  but fewer than ten clusters, so we called them RCTs. reclassify them, then run the main meta, and by study design as well


### What if we ignore all clustering information when calculating standard errors?