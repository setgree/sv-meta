---
title: "4-exploratory-analyses"
author: "Seth Ariel Green"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
editor_options: 
  chunk_output_type: console
---

**Libraries, functions and data**

```{r setup, message=F}
# libraries
library(dplyr, warn.conflicts = F)
library(knitr)
library(ggplot2, warn.conflicts = F)
library(ggrepel, warn.conflicts = F)
library(metafor, quietly = T)
library(purrr)
library(stringr)
library(tidyr, warn.conflicts = F)
library(tibble)

# data
dat <- readRDS(file = '../data/sv_meta_data.rds') |>
  select(unique_study_id, scale_type, everything())

source('./functions/sum_lm.R')
source('./functions/map_robust.R')
source('./functions/study_count.R')
source('./functions/d_calc.R')
source('./functions/var_d_calc.R')

# personal preference for how digits get rendered
options(scipen = 99)
```


## additional Robustness checks

**Group results within study**

```{r grouping rbustness checks}
# robustness check: group by study AND scale_type and average the results
dat_grouped <- dat |>
  group_by(unique_study_id, scale_type) |>
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) |> 
  slice(1) 

dat_grouped |> map_robust()

# alternative specification -- don't cluster on unique_study_id

rma(yi = d, vi = var_d, data = dat_grouped)

# How about not grouping by scale_type
dat |>
  group_by(unique_study_id) |>
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) |>
  slice(1) |> map_robust()

# slightly raises the SE

rm(dat_grouped)
```

**Different ways to subset and then analyze the data:**

```{r alternate_breakdowns}
# ra vs not (in paper we break down by study)
dat |> 
  split(~interaction(scale_type, ra)) |>   
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))

# by behavior type overall
table(dat$behavior_type)

dat |> 
  split(~behavior_type) |> 
  map(map_robust)

# scale type and delay
dat |> 
  split(~interaction(scale_type,yes_delay)) |> 
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
       cluster = .x$unique_study_id))
```

**sum_lm() checks that don't make it into the final paper**

```{r sum_lm checks}
dat |>  
  split(~ra) |>  # ra = random assignment (so RCTs vs. everything else)
  map(.f = ~sum_lm(y = d, x = se_d, dat = .))


# how about the overall relationship between lab/field and d? 
sum_lm(dat, d, lab_field)

# study type?
sum_lm(dat, d, study_design)
# yep, meaningful relationship

# scale type?
sum_lm(dat, d, scale_type)

# publication status?
sum_lm(dat, d, publication_type)

dat |>
  filter(behavior_type == 'Perpetration', 
         study_design == 'Randomized Controlled Trial') |> 
  sum_lm(d, se_d)
```


**Quintile check**
```{r quintile}
dat |> arrange(n_t_post) |>
  mutate(quintile = ntile(n_t_post, 5)) |> 
  group_by(unique_paper_id, intervention_name) |> 
  split(~quintile) |>
  map(.f = ~robust(x = rma(yi = .x$d, vi = .x$var_d),
                   cluster = .x$unique_study_id))

```

Just reinforces that there's no meaningful relationship between d and se; if anything the largest decile seems to have the largest effect sizes.

## Additional analyses

**Effect size heterogeneity by country/region?**

```{r eff_size_hetereogeneity}
# can't do meta-analysis when there's 1 country or 1 unique_study_id_per country
dat |>
  mutate(country = as.character(country)) |> # drop the variable's levels
  group_by(country) |> 
  filter(n() > 1 & n_distinct(unique_study_id) >= 2) |> 
  ungroup() |> split(~country) |>   
  imap(~bind_cols(study_count(.x), map_robust(.x)) |> kable('markdown')) 

# just look at the ones where n = 1 per country

dat |> group_by(country) |> filter(n() <= 3) |> 
  select(author, year, paper_title, study_design, n_t_post, n_c_post, d, se_d, behavior_type)

# this is suggestive that a US vs everywhere else analysis might be fruitful

dat |> mutate(us_dummy = if_else(country == 'USA', TRUE, FALSE)) |> 
  split(~us_dummy) |>  
  imap(~bind_cols(study_count(.x), map_robust(.x)) |> kable('markdown')) 

# what about by Continent?

dat <- dat |>
  mutate(continent = case_when(
    country %in% c("Canada", "USA", "Mexico") ~ "North America",
    country %in% c("Germany", "Netherlands", "Spain") ~ "Europe",
    country %in% c("Ghana", "Kenya", "Uganda") ~ "Africa",
    country %in% c("Haiti", "St. Lucia") ~ "Caribbean",
    country == "Israel" ~ "Asia",
    TRUE ~ "Other"))
 
  dat |>  filter(!(continent %in% c('Asia', 'Caribbean', 'Other'))) |>
  split(~continent) |>
  imap(~bind_cols(study_count(.x), map_robust(.x)) |> kable('markdown'))

  # look at the core results of the studies we excluded here
  
dat |>  filter(continent %in% c('Asia', 'Caribbean', 'Other')) |> 
  select(author, year, paper_title, continent, 
         study_design, d, se_d)


```


**heterogeneity by lab_field or setting?**

```{r pop_lab_field_hetereogeneity}

dat |> split(~lab_field) |> 
  imap(~bind_cols(study_count(.x), map_robust(.x)) |> kable('markdown'))

dat |> mutate(setting = as.character(setting)) |> split(~setting) |>  # fix a 'level' problem
  imap(~bind_cols(study_count(.x), map_robust(.x)) |> kable('markdown'))
```


**How many studies say that they are testing behavioral theories, and what are their reported effects?**

```{r behavior_theories_of_change}
dat |> 
  filter(str_detect(paper_title, regex("behavior", ignore_case = TRUE))) |> sum_lm()
# slightly higher

```

**What is the 'bringing in the bystander' average effect on ideas/behaviors**
```{r repeat_tests}
dat |> 
  filter(intervention_name == "bringing in the bystander") |> 
  split(~scale_type) |> 
  map(map_robust)
# a nice-to-have would be a modified map_robust that, if there weren't enough 
# rows in a set it would just tell you that and continue on with the meta-analysis
# instead of stopping but, for now;

dat |>
  filter(intervention_name == "bringing in the bystander") |>
  group_by(behavior_type) |>
  summarise(count = n())

# so it's pretty much all bystander, with one victmization outcome
dat |> 
  filter(intervention_name == "bringing in the bystander",
         behavior_type == 'Victimization') |> 
  select(paper_title, author, year, d, se_d)
# d = 0.02
```

**Behavioral measurement has become more common over time**
Particularly from 2010 onwards
```{r behavioral_outcomes_by_decade}
range(dat$year)

# how many studies in each decade
dat |> group_by(decade) |> study_count()

merge(x = dat |> group_by(decade) |> study_count(),
      y = dat |> group_by(decade, scale_type) |> summarise(n = n())) 


# what was the earliest study to measure behavior as we conceive it?
dat |> filter(scale_type == 'behavior') |> 
  arrange(year) |> 
  select(author, year, paper_title) |> head()

```

**What if we ignore all clustering information when calculating standard errors?**

Ignoring all information about clustering \textemdash treating all sample sizes as if treatment was assigned individually every time \textemdash creates a strong relationship between standard errors and $\Delta$: $\Beta$ = 0.677 (se = 0.3), p = 0.0246. 

```{r redoing Ds without any cluster info}
no_clusters <- dat |> 
  select(author, year, paper_title,  unique_paper_id, unique_study_id, n_t_post,
         n_c_post, eff_type, u_s_d, ctrl_sd, anticipated_direction,
         behavior_type, study_design, scale_type) |> 
  mutate(d = 
           mapply(
             FUN = d_calc,
             stat_type = eff_type,
             stat =  u_s_d,
             sample_sd = ctrl_sd,
             n_t = n_t_post,
             n_c = n_c_post
             ),
         var_d =
           mapply(
               FUN = var_d_calc,
               d = d,
               n_t = n_t_post,
               n_c = n_c_post),
           se_d = sqrt(var_d)
         ) |> 
  mutate(d = d * anticipated_direction)
no_clusters |> sum_lm()
no_clusters |> map_robust()

# alternative where we separate by outcome type
no_clusters |> ggplot(mapping = aes(y = d, 
                             x = se_d)) + 
  geom_point(aes(color = behavior_type,
             shape = study_design),
             size = 3) + 
  stat_smooth(method = 'lm', se = F) + 
  ggtitle("Relationship between effect sizes and standard errors if we don't do any cluster adjustment") +
  labs(x = "Standard errors",
       y = "Effect size",
       color = "Outcome type",
       shape = "Study Design") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))

# without ideas outcomes? 

no_clusters |> 
  filter(scale_type == 'behavior') |>
    ggplot(mapping = aes(y = d, 
                             x = se_d)) + 
  geom_point(aes(color = behavior_type,
             shape = study_design),
             size = 3) + 
  stat_smooth(method = 'lm', se = F) + 
  ggtitle("Relationship between effect sizes and standard errors if we don't do any cluster adjustment") +
  labs(x = "Standard errors",
       y = "Effect size",
       color = "Outcome type",
       shape = "Study Design") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```

**The one bystander study that measured involvement found much bigger effects than the other categories on average:**

```{r bystander_involvement}
 dat |>
  filter(
    str_detect(paper_title, "[Bb]ystander") |
      str_detect(brief_description_of_the_intervention, "[Bb]ystander") |
      str_detect(intervention_name, "[Bb]ystander") |
      str_detect(program_name, "[Bb]ystander") |
      str_detect(stated_purpose, "4")) |>
  filter(behavior_type == 'Involvement') |>
  select(author, year, paper_title, n_t_post, n_c_post, d, se_d)

```

## Supplementary figures

**bystander interventions are a product of their time and place**
```{r bystander_interventions_over_time}

bystander_dat <- dat |>
  filter(
    str_detect(paper_title, "bystander") |
      str_detect(brief_description_of_the_intervention, "bystander") |
      str_detect(intervention_name, "bystander") |
      str_detect(program_name, "bystander") |
      str_detect(stated_purpose, "4")) |>
    select(author, year, paper_title, unique_study_id, study_design,
           d, var_d, se_d, scale_type,
           behavior_type, has_both, country) |>
  mutate(perp_vict_grouped = case_when(
    behavior_type == 'Victimization' | behavior_type == 'Perpetration' ~ 'perp_vict',
    behavior_type == 'Involvement' ~ 'involvement',
    behavior_type == 'ideas' ~ 'ideas',
    behavior_type == 'Bystander' ~ 'Bystander'))

bystander_dat |> group_by(unique_study_id) |> slice(1) |> ungroup() |>
  ggplot(aes(x = year)) +
  geom_line(stat = 'count', color = 'light blue') + 
  labs(x = "Year", 
       y = "Count", 
       title = "Bystander interventions over time") +
  # scale_x_date(date_breaks = "2 years") +
  scale_x_continuous(n.breaks = 20) +
  scale_y_continuous(n.breaks = 15) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

## BTW an alternate way to do this
dat |>
  group_by(unique_study_id) |>
  summarise(
    bystander_count = ifelse(
      any(
        str_detect(c(
          brief_description_of_the_intervention, 
          intervention_name, program_name, paper_title), "bystander")) |
        any(str_detect(stated_purpose, "4")),
      TRUE, FALSE)
  ) |> 
  summarise(
    bystander = sum(ifelse(is.na(bystander_count), FALSE, bystander_count)),
    no_bystander = sum(ifelse(is.na(bystander_count), TRUE, !bystander_count))
  )

# where do bystander interventions occur
bystander_dat |> group_by(country) |> study_count()
rm(bystander_dat)
```

**Different ways of assessing effect size change over time**

```{r effect_size_over_time}
dat |> ggplot((aes(x = year, y = d))) + 
  geom_point() +   
  geom_smooth(method = 'lm') + 
  theme_minimal()
# basically no slope

dat |> sum_lm(d, year)
dat |> split(~interaction(ra, scale_type)) |>
  map(~ ggplot(data = ., mapping = aes(x = year, y = d)) + 
        geom_point() + 
        geom_smooth(method = 'lm') +
        theme_minimal())
# more or less the same results as  Anderson, L. A., & Whiston, S.C. (2005)
```


**Categories of outcome over time** 
```{r outcome_categories_over_time}
dat |>
  ggplot(aes(x = year, color = behavior_type)) + 
  geom_line(stat = 'count') + 
  labs(x = "Year", 
       y = "Count", 
       color = "Behavior type",
       title = "Categories of behavioral outcomes over time") +
  scale_x_continuous(n.breaks = 10) +
  scale_y_continuous(n.breaks = 10) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```


**ideas and behavior effect sizes over time**
```{r att_beh_over_time}
dat |> 
  group_by(year, scale_type) |> 
  summarise(mean_d = mean(d), 
            mean_se_d = mean(se_d)) |>
  ggplot(aes(x = year, y = mean_d, 
             group = scale_type)) +
  geom_point(aes(color = scale_type)) +
  geom_smooth(method = 'lm', aes(color = scale_type), se = F) + 
  theme_minimal() +
  scale_x_continuous(breaks = seq(1986, 2018, 5)) +
    labs(x = "year",
       y = "mean d") +
  labs(color = "ideas or behaviors")
# both seem to be trending up, more behaviors than ideas
# how much is this b/c of bystander and involvement outcomes?

dat |> 
  filter(behavior_type != "Bystander", behavior_type != 'Involvement') |>
  group_by(year, scale_type) |> 
  summarise(mean_d = mean(d), 
            mean_se_d = mean(se_d)) |>
  ggplot(aes(x = year, y = mean_d, 
             group = scale_type)) +
  geom_point(aes(color = scale_type)) +
  geom_smooth(method = 'lm', aes(color = scale_type), se = F) + 
  theme_minimal() +
  scale_x_continuous(breaks = seq(1986, 2018, 5)) +
    labs(x = "year",
       y = "mean d") +
  labs(color = "ideas or behaviors")
# indeed slope goes down, but still up. interesting
```


**additional ways to look at rct perp outcomes**
```{r rct_perp}
# not a big difference
rct_perp <- dat |> filter(study_design == 'Randomized Controlled Trial',
                          behavior_type == 'Perpetration')
rct_perp |> ggplot(mapping = aes(x = delay, y = d)) + 
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()

rct_perp |> filter(delay > 30) |>
ggplot(mapping = aes(x = se_d, y = d)) + 
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()
# a stronger relationship
```

**A rough idea for a  D/SE plot**

analogous to the D/SE plot in the contact hypothesis paper. It turned out to not be very informative. 

```{r d_se_plot}
dat |>
  drop_na(study_design) |> 
  ggplot(aes(x = se_d, y = d)) +
  geom_point(aes(color = behavior_type,
                 shape = study_design),
             size = 3) +
  geom_smooth(method = "lm",
              se = F) +
  #ggtitle("Relationship between effect sizes and standard errors") +
  labs(x = "Standard errors",
       y = "Effect size",
       color = "Outcome type",
       shape = "Study Design") +
  theme_bw() +
  theme(axis.text = element_text(size = 12),
        legend.text = element_text(size = 12),
        legend.position = "bottom",
        legend.box = "vertical")

# alternative where we separate by outcome type
dat |> ggplot(mapping = aes(y = d, 
                             x = se_d)) + 
  geom_point(aes(color = behavior_type,
             shape = study_design),
             size = 3) + 
  stat_smooth(method = 'lm', se = F) + 
  ggtitle("Relationship between effect sizes and standard errors") +
  labs(x = "Standard errors",
       y = "Effect size",
       color = "Outcome type",
       shape = "Study Design") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))

```


**Forest plot alternative to correlation scatterplot**

Here's a messier way of slicing the data to get at the relationship between attitudinal and behavioral change:

```{r att_beh_change}

## Relationship between ideas and behaviors 
has_both <- dat |> filter(has_both == 'both') |> 
  group_by(unique_study_id, scale_type) |>
  mutate(mean_d = mean(d), 
         mean_var_d = mean(var_d), 
         mean_se_d = mean(se_d)) |>
  select(author, year, paper_title, unique_study_id, intervention_name,
         study_design, mean_d, mean_var_d, mean_se_d,
         ra, labels, scale_type, decade, behavior_type) |>  
  slice(1) |> 
  pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels, decade),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))
```

Then, looking at the correlation between ideas and behaviors just within the RCTs, and with study labels:

```{r labeled_correlation_plot}
has_both |> 
  filter(study_design == 'Randomized Controlled Trial') |> 
  ggplot(mapping = aes(x = mean_d_ideas, y = mean_d_behavior)) +
  geom_point(size = 3, color = 'blue') + 
  geom_smooth(aes(fill = NULL), lty = "dashed", fullrange = TRUE,
              method = "lm",
              show.legend = FALSE, alpha = .1, se = F) +
  geom_label_repel(mapping = aes(label = labels)) +
  theme_minimal() +
  labs(x = "mean d (ideas)",
       y = "mean d (behavior)")
```

hard to read in this in a PDF, but if you're curious about studies that produce change in ideas but not behaviors, this is a good place to start -- look at some of the outliers.

**Forest plot**

This was a different way to visualize the (lack of) correlation between ideas-based and behavioral outcomes

dat for forest plot and sign tests:

```{r forest_plot_dat_and_sign_tests}
dat_for_forest_plot <- dat |> 
  select(unique_study_id, scale_type, everything()) |>
  filter(study_design == 'Randomized Controlled Trial') |>
  group_by(unique_study_id) |>
         filter(all(c('ideas', 'behavior') %in% scale_type)) |>
  mutate(study_names = paste0(tools::toTitleCase(word(author)), " ",
                              year, " ", "(", scale_type, ")")) |>
  group_by(unique_paper_id, scale_type) |>
  mutate(d = mean(d), var_d = mean(var_d), se_d = mean(se_d)) |>
  slice(1) |>
  ungroup(scale_type) |> 
  mutate(mean_se = mean(se_d)) |>
  arrange(mean_se) |>
  ungroup() |>
  select(study_names, d, se_d, var_d,
         scale_type, unique_study_id, unique_paper_id)

# sign test: how many studies have an ideas d that's > behavior d
dat_for_forest_plot |> group_by(unique_paper_id) |>
  summarise(mean_d_ideas = mean(d[scale_type == "ideas"]),
            mean_d_behavior = mean(d[scale_type == "behavior"])) |>
  mutate(comparison = case_when(
    mean_d_ideas > mean_d_behavior ~ "Higher",
    mean_d_ideas == mean_d_behavior ~ "Same",
    mean_d_ideas < mean_d_behavior ~ "Lower"
  )) |>
  count(comparison) |>
  rename(Comparison = comparison, Count = n)
binom.test(15, 21, 0.5)
```

next, regenerating the forest plot from the file itself, but not filtering on RCTs:

```{r dat_for_forest_plot_not_rcts}
readRDS(file = '../data/sv_meta_data.rds') |> 
  select(unique_study_id, scale_type, everything()) |>
  group_by(unique_study_id) |>
         filter(all(c('ideas', 'behavior') %in% scale_type)) |>
  mutate(study_names = paste0(tools::toTitleCase(word(author)), " ",
                              year, " ", "(", scale_type, ")")) |>
  group_by(unique_paper_id, scale_type) |>
  mutate(d = mean(d), var_d = mean(var_d), se_d = mean(se_d)) |>
  slice(1) |>
  ungroup(scale_type) |> 
  mutate(mean_se = mean(se_d)) |>
  arrange(mean_se) |>
  ungroup() |>
    group_by(unique_paper_id) |>
  summarise(mean_d_ideas = mean(d[scale_type == "ideas"]),
            mean_d_behavior = mean(d[scale_type == "behavior"])) |>
  mutate(comparison = case_when(
    mean_d_ideas > mean_d_behavior ~ "Higher",
    mean_d_ideas == mean_d_behavior ~ "Same",
    mean_d_ideas < mean_d_behavior ~ "Lower"
  )) |>
  count(comparison) |>
  rename(Comparison = comparison, Count = n)
binom.test(36, (36 + 12 + 1), 0.5)
```

**Forest Plot (Figure):**

```{r forest_plot}
head(dat_for_forest_plot) # checking we've already created subset (chunk above)
# overall effect size
overall_forest <- dat_for_forest_plot |>
  split(~scale_type) |>
  map(map_robust)

dat_for_forest_plot |> 
  add_row(study_names = "Overall (ideas)",
          d = overall_forest[[1]]$beta,
          se_d = overall_forest[[1]]$se,
          scale_type = "ideas",
          unique_paper_id = NA) |>
  add_row(study_names = "Overall (Behavior)",
          d = overall_forest[[2]]$beta,
          se_d = overall_forest[[2]]$se,
          scale_type = "behavior",
          unique_paper_id = NA) |>
  mutate(index = row_number()) |>
  relocate(index, .before = 1) |>
  ggplot(aes(y = index, x = d, xmin = d - (1.96 * se_d),
             xmax = d + (1.96 * se_d))) +  
  geom_point(size = 1) +
  geom_errorbarh(height = .1, aes(color = scale_type)) +
  geom_vline(xintercept = 0, color = "black", alpha = .5) +
  scale_x_continuous(name = expression(paste("Glass's", " ", Delta))) +
  scale_y_continuous(name = "", breaks = 1:length(dat_for_forest_plot$study_names),
                     trans = "reverse", labels = dat_for_forest_plot$study_names) + 
  ylab("Study") +
  geom_vline(xintercept = overall_forest$ideas$beta, color = '#F8766D', lty = 'dashed') +
  geom_vline(xintercept = overall_forest$behavior$beta, color = '#00BFC4', lty = 'dashed') +
  labs(color = "ideas or behaviors") +
  theme_minimal() + 
  theme(axis.text.y = element_text(face = ifelse(
    dat_for_forest_plot$study_names %in% 
      c("Overall (ideas)", "Overall (Behavior)"), 
    "bold", "plain"), 
    margin = margin(t = 0.5, b = 0.5, unit = "cm"))) +
  ggtitle("Ideas and Behavioral Changes Compared")
```