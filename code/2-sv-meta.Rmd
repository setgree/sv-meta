---
title: "2-sv-meta"
author: "Seth Ariel Green"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
editor_options: 
  chunk_output_type: console
---

### Overview

This document works through our results section. Each chunk corresponds to, and is preceded by, a quantitative claim in the results section. 

**libraries, functions, options, and data **

```{r setup, include=F, echo=F}

#' libraries
library(dplyr, warn.conflicts = F)
library(knitr)
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(lubridate, warn.conflicts = F)
library(purrr)
library(stringr)
library(tidyr, warn.conflicts = F)
library(tibble)

#' functions
source('./functions/sum_lm.R')
source('./functions/map_robust.R')
source('./functions/study_count.R')
source('./functions/d_calc.R')
source('./functions/var_d_calc.R')

options(scipen = 99)

#' data
dat <- readRDS(file = '../data/sv_meta_data.rds') |>
  select(unique_study_id, scale_type, everything())
```

**Add Glass's Delta, Variance, and Standard Error**

This chunk checks if `d` is present, and if not, adds d, var_d, and se_d

```{r delta_var_se}
if (!"d" %in% names(dat)) {
  dat <- dat |>
    mutate(d = case_when(
      !is.na(n_t_group) & study_design %in% c('Randomized Control Trial') ~ 
        mapply(
          FUN = d_calc,
          stat_type = eff_type,
          stat =  u_s_d,
          sample_sd = ctrl_sd,
          n_t = n_t_group,
          n_c = n_c_group
        ),
      TRUE ~ mapply(
        FUN = d_calc,
        stat_type = eff_type,
        stat =  u_s_d,
        sample_sd = ctrl_sd,
        n_t = n_t_post,
        n_c = n_c_post)
    )) |>
    mutate(d = abs(d) * anticipated_direction) |>
    mutate(var_d = case_when(
      !is.na(n_t_group) & study_design %in% c('Randomized Control Trial') ~
        mapply(
          FUN = var_d_calc,
          d = d,
          n_c = n_c_group,
          n_t = n_t_group
        ),
      TRUE  ~
        mapply(
          FUN = var_d_calc,
          d = d,
          n_t = n_t_post,
          n_c = n_c_post
        )
    )) |>
    mutate(se_d = sqrt(var_d))
}

```

# Results 

## Overall results and (lack of) publication bias

> Our random effects meta-analysis of all primary prevention strategies from 1985 to 2018 reveals an overall effect size, across all interventions on all kinds of outcomes, of $\Delta$ = 0.28 (se = 0.024). This effect is statistically significant at p $<$ 0.0001.

```{r overall_meta}
dat |> map_robust()

# more verbose version of
robust(x = rma(yi = d, vi = var_d, data = dat),
       cluster = dat$unique_study_id)
```

> In our database we find this relationship between effect sizes and standard errors to be weak and non-significant: $\beta$ = 0.16 (SE = 0.176), p = 0.412.

```{r publication_bias_overall}
sum_lm(dat, d, se_d, coefs_only = T)
```

> Second, the relationship between SE and $\Delta$ remains non-significant within randomized (N = 105, $\beta$ = 0.208, SE = 0.256, p = .373) and quasi-experimental (N = 104, $\beta$ = -0.201, SE = 0.393, p = .597) designs, providing further evidence to the lack of publication bias within the literature. However, we do find a significant relationship within observational designs (N = 102, $\beta$ = 2.228, SE = 0.64, p $<$ .001), suggesting some publication bias within these studies. 

```{r publication_bias_by_study_design}
# Ns
dat |>
  group_by(study_design) |>
  study_count()

# effect sizes
dat |> 
  split(~study_design) |> 
  map(~sum_lm(.))
```


> Our third check for publication bias is to separate the literature into studies published in peer-reviewed journals (N = 186) and those that were not (N = 118). 

```{r published_vs_unpubished}
dat |> 
  group_by(published_unpublished) |> 
    study_count()

# sanity check
length(unique(dat$unique_study_id)) == 189 + 109
```

> In our sample, being unpublished is associated with a small reduction in effect size ($\beta$  = -0.047), but not at a statistically significant level (p = 0.28) 

```{r publish_relationship_or_not}
dat |> sum_lm(d, published_unpublished)
```

> Within published studies, the relationship between standard errors and $\Delta$ is tiny and insignificant 

```{r pub_unpub_se_d}

dat |> split(~published_unpublished) |> map(sum_lm)
```

>  while the association between the two variables is larger in unpublished studies (N = 118, $\beta$  = 0.5842 (SE = 0.360)), the relationship is still statistically non-significant (p = 0.106).

```{r pub_unpub_eff_size}
dat |> split(~published_unpublished) |> map(map_robust)
```

## The Meta-Analytic Effect of Primary Prevention on Behavior and on Ideas about Sexual Violence

> Overall we find that behaviors are much more resistant than ideas to change. The overall effect size for behaviors is ∆ = 0.071 (SE = 0.022), p = .0015; whereas the overall effect size for idea-based outcomes is ∆ = 0.367 (SE = .03), p < 0.0001.

```{r differences_by_ideas_behavior}

dat |> split(~scale_type) |> map(.f = map_robust)
```

> We further report on the meta-analytic effect of primary prevention interventions on behavior, and on ideas about sexual violence by study design (see Table 2).

```{r behavior_ideas_discrepancy_within_study_designs}
dat |>
  split(~interaction(scale_type, study_design)) |>
  imap(~bind_cols(study_count(.x), map_robust(.x)) |> kable('markdown')) 
```

> This analysis reveals that the discrepancy between changing behaviors and changing ideas is substantial across every design category, and in particular in observational research, where outcomes pertaining to an individual’s ideas about sexual violence are about 21 times larger than behavioral responses. 

```{r obs_ideas_beh_multiplier}
round(0.503 / 0.024)
```


\subsection{Do bystander interventions Change Behavior?}

> We tested this on the 92 bystander-related interventions in our database, the majority (74/92) of which measure ideas about sexual violence like beliefs, ideas, and knowledge, and are successful at changing them, $\Delta$ = .398 (SE = 0.069), p $<$ 0.0001.  

```{r bystander_interventions_overall}
# create bystander dataset
# filter based on mentioning bystander in either title, description, 
# intervention name, or program name, or have a stated purpose of "4" (bystander)
bystander_dat <- dat |>
  filter(
    str_detect(paper_title, "bystander") |
      str_detect(brief_description_of_the_intervention, "bystander") |
      str_detect(intervention_name, "bystander") |
      str_detect(program_name, "bystander") |
      str_detect(stated_purpose, "4")) |>
    select(author, year, paper_title, unique_study_id, study_design,
           d, var_d, se_d, scale_type,
           behavior_type, has_both, country) |>
  mutate(perp_vict_grouped = case_when(
    behavior_type == 'Victimization' | behavior_type == 'Perpetration' ~ 'perp_vict',
    behavior_type == 'Involvement' ~ 'involvement',
    behavior_type == 'ideas' ~ 'ideas',
    behavior_type == 'Bystander' ~ 'Bystander'))

# how many studies overall?
bystander_dat |>  study_count()

# how many studies look at ideas?
bystander_dat |> group_by(behavior_type) |> study_count()

bystander_dat |> filter(behavior_type == 'ideas') |> map_robust()

```

> 43 of the 96 bystander studies measure behaviors.

```{r bystander scale_type}
bystander_dat |> group_by(scale_type) |> study_count()
```

> Overall, 22 studies measured whether bystander interventions increase bystander behaviors, which they do to a modest extent $\Delta$ = 0.154 (SE = 0.056), p = 0.013. In our database, 20 bystander studies measure perpetration or victimization, the ultimate goal of these interventions. We did not find that bystander interventions meaningfully change rates of perpetration  ($\Delta$ = 0.019, SE = 0.019, p = 0.329) or victimization ($\Delta$ =-0.009, SE = 0.041,  p = 0.835).

```{r bystander_dv_count}
# filter out the one 'involvement' outcome because 
# having just one cluster breaks the meta-analysis function
bystander_dat |> 
  filter(behavior_type == 'Involvement') |>
  select(author, year, paper_title, d, se_d)

 bystander_dat |>
  filter(behavior_type != 'Involvement') |>
  split(~behavior_type) |>
  imap(~bind_cols(study_count(.x), map_robust(.x)) |> kable('markdown'))
 
 # how many measure perpetration or victimization?
bystander_dat |>
  filter(perp_vict_grouped == 'perp_vict') |> 
  study_count()
```


# A Closer Look at Behavioral Outcomes

> Table \ref{tab:meta_beh} shows the number of studies which measure each behavioral category and their pooled effect sizes. We also provide the overall effect size for idea-based outcomes for reference. We find that on an aggregate level, primary prevention programs haven't been effective in reducing perpetration and victimization behaviors at all. The corresponding effect sizes for these two critical outcomes are neither meaningful in magnitude nor statistically significant. It appears that primary prevention programming is currently effective in increasing only bystander or involvement behavior \textemdash and changing ideas about sexual violence. 

```{r eff_size_by_outcome_type}
dat |>
  split(~behavior_type) |>
  imap(~bind_cols(study_count(.x), map_robust(.x)) |>
      kable('markdown'))
```

> Our sample features 28 randomized designs that measure perpetration, which collectively produce a meta-analytic effect of 0.086 (SE = 0.061), with a p-value of 0.173.

```{r rct_perp}
dat |>
  filter(behavior_type == 'Perpetration', study_design == 'Randomized Control Trial') %>%
  # needs %>% because |> doesn't work with `.`
    summarise(
      count_result = study_count(.),
      robust_result = map_robust(.))
```

## Do Changes in Ideas Predict Changes in Behaviors?

> We focus on the 62 studies that measure both type of outcomes.

```{r dat_has_both_count}
dat_has_both <- dat |> filter(has_both == 'both')
dat_has_both |> study_count()
```

> In this subset of studies, the pooled behavior change effect is $\Delta$ = 0.083 (SE = 0.029), p = 0.006, while the pooled idea change effect is $\Delta$ = 0.3 (SE = 0.04), p $<$0.0001. In other words, the idea change effects are slightly lower than the overall effect we obtained when looking at the full database ($\Delta$ = 0.368), while the behavior change effects are slightly larger than the overall effect ($\Delta$ = 0.071).

```{r has_both_behav_att_meta_results}
dat_has_both |> split(~scale_type) |> 
  map(map_robust)
```

> In other words, the idea change effects are slightly lower than the overall effect we obtained when looking at the full database ($\Delta$ = 0.368), while the behavior change effects are slightly larger than the overall effect ($\Delta$ = 0.071).

```{r reitirating_main_effects}
dat |> split(~scale_type) |> 
   imap(~map_robust(.x) |> kable('markdown'))
```

> Unfortunately, in these studies, we find a small, statistically non-significant correlation of $\rho$ (58) = 0.135, p = 0.372 between changes in ideas and changes in behaviors.

```{r ideas_behavior_correlation_overall}

#' First, a kind of hacky solution to a hard problem, which is that some studies
#' offer some outcomes that can be evaluated quasi-experimentally (i.e. compare 
#' a treatment group to a non-randomly assigned control group) but other outcomes
#' that can only be evaluated pre-post. Peterson (2014) evaluates its behavioral 
#' outcomes -- bystander behaviors -- quasi-experimentally, but its ideas-based
#' outcomes pre-post. (Things in this category are also why we get an N of 
#' 301 when divide the studies into design and then count unique_study_ids, 
#' but we only have 298 unique study IDs). This means that when we pivot
#' the data to have one column for mean attitude outcomes and one for mean
#' behavioral outcomes, Peterson 2014 is getting treated as two studies b/c
#' the identifying information is inconsistent. This results in a few rows of
#' NAs. You can ignore this and still get basically the same results, but
#' just for completeness, here we're assigning 'Quasi-Experimental' to 
#' to the study design for all Peterson 2014 results so that every attitude
#' outcome has a corresponding behavior outcome.

dat_has_both <- dat_has_both |> mutate(
  study_design = if_else(author == 'peterson', 'Quasi-Experimental',
                         study_design))

has_both_reshaped  <- dat_has_both |> 
  group_by(unique_study_id, scale_type) |>
  mutate(mean_d = mean(d), 
         mean_var_d = mean(var_d), 
         mean_se_d = mean(se_d)) |>
    slice(1) |> 
  pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels, decade),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))

cor.test(x = has_both_reshaped$mean_d_ideas, 
         y = has_both_reshaped$mean_d_behavior)
```

> The relationship remains small and non-significant when looking solely at randomized evaluations (\textit{r} = 0.138, p = 0.48). 
 
```{r ideas_behavior_correlation_subsets}
rcts_both <- has_both_reshaped |> filter(study_design == 'Randomized Control Trial')
cor.test(x = rcts_both$mean_d_ideas, 
         y = rcts_both$mean_d_behavior)
rm(rcts_both)
```

> Figure \ref{fig:corr} highlights this lack of correlation. Specifically, each point in the figure represents a study that measures both ideas and behaviors, with the average effect size for ideas about sexual violence (within a given study) on the X axis and the average within-study behavioral effect size on the Y axis. The dotted black line shows what a correlation of 1.0 would look like, and the dotted gray line shows the correlation that we actually observe. Studies are color-coded by design.

```{r correlation_scatterplot}

dat_has_both |>
  group_by(author, year, study_design, unique_study_id, scale_type) |>
  filter(delay == min(delay)) |>
  summarise(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) |>
  pivot_wider(id_cols = c(author, year, unique_study_id, study_design),
    names_from = c(scale_type),
    values_from = c(mean_d, mean_var_d, mean_se_d)) |>
  ggplot(aes(x = mean_d_ideas,
             y = mean_d_behavior)) +
  geom_point(aes(color = study_design),
             size = 3) +
  geom_abline(slope = 1,
              lty = 'dashed') +
  geom_smooth(lty = "dashed",
              method = "lm",
              se = FALSE,
              color = 'grey') +
  labs(#title = "Correlation between ideas and behavioral change",
       x = "Effect size (ideas)",
       y = "Effect size (behaviors)",
       color = "Study Design") +
 guides(color = guide_legend(title = "")) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text = element_text(size = 14),
        axis.title = element_text(size = 14),
        legend.text = element_text(size = 14),
        legend.title = element_text(size = 14),
        legend.position = "bottom",
        legend.box = "vertical")
ggsave("../results/figures/att_beh_corr.pdf", width = 9.6, height = 9.6)
```


### record computational environment
```{r write_dockerfile}
source('./functions/write_dockerfile.R')

write_dockerfileR(dir = '../documentation/')
```