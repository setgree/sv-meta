---
title: "3-appendixes"
author: "Seth Green"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output: html_document
editor_options: 
  chunk_output_type: console
---


## Libraries and data 

```{r setup, include=F, echo=F}
rm(list = ls())
library(dplyr, warn.conflicts = F)
library(lubridate)
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(ggrepel, warn.conflicts = F)
library(purrr, warn.conflicts = F)
library(stringr)
library(tidyr, warn.conflicts = F)

source('./functions/sum_lm.R')
source('./functions/dip_calc.R')
source('./functions/odds_ratio_to_d.R')
source('./functions/map_robust.R')
source('./functions/study_count.R')
```

```{r load_data, include=F, echo=F}
# read in data, add in two useful variables
# labels that paste together author and year for plots;  
# reading later results);
# add attitudes_behaviors (for studies that have both), one or the other;
# new var for random assignment (probably should remove 'contains randomization')

# read in data

dat <- readRDS(file = '../data/sa_meta_data_final.rds') |>
  select(unique_study_id, scale_type, everything())

# data for 'has both' analyses

has_both <- dat |> 
filter(all(c('attitudes', 'behavior') %in% scale_type)) |> 
  group_by(unique_study_id, scale_type) |>
  mutate(mean_d = mean(d), 
         mean_var_d = mean(var_d), 
         mean_se_d = mean(se_d)) |>
  slice(1) |> 
  pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))

```

## Robustness checks

Our main meta-analytic result comes from a random effects meta-analysis clustering at the level of individual studies.

```{r main_result_for_comparison}

robust(x = rma(yi = d, vi = var_d, data = dat),
       cluster = dat$unique_study_id)

```

Clustering at the level of paper rather than study raises the standard error to 0.0280, while removing the cluster entirely lowers it to 0.0206; in both cases, the estimate of $\Delta$ does not change, and both are significant at the p < .0001 level.
```{r cluster_paper_level}
# cluster at paper
robust(x = rma(yi = d, vi = var_d, data = dat),
       cluster = dat$unique_paper_id)

#no cluster
rma(yi = d, vi = var_d, data = dat)

```

 Averaging $\Delta$ within each study and then meta-analyzing the means raises the result somewhat to 0.331 (se = 0.028), p < .0001,
```{r collapse_individual_ds}

dat |> group_by(unique_study_id) |>
  mutate(d = mean(d),
         var_d = mean(var_d)) |>
  slice(1) |>
  map_robust()
```

while averaging $\Delta$ within each study's behavioral and ideas outcomes separately leads to $\Delta$ = 0.307
```{r collapse_ds_within_study_and_scale_type}
dat |> group_by(unique_study_id, scale_type) |>
  mutate(d = mean(d),
         var_d = mean(var_d)) |>
  slice(1) |>
  map_robust()
```

## Additional checks for publication bias
 In our sample, the pooled effect size in the largest quintile of studies, with at least 405 participants combined in treatment and control groups, is $\Delta$ = 0.275 (se = 0.073), while those in our smallest quintile, which have between 20 and 83 participants, is $\Delta$ = 0.302 (se = 0.043). In other words, neither is more 0.02 away from our overall effect size of $\Delta$ = 0.281.

```{r quintile_check}
# what are the quantiles
dat |> group_by(quantile_var) |> 
  summarise(min_n = min(n_t_post + n_c_post),
            max_n = max(n_t_post + n_c_post))

dat |> 
  split(~quantile_var) |> map(.f = map_robust)
  
## well it's not *nothing*...but it's pretty small
dat |> sum_lm(y = d, x = quantile_var) # it's basically nothing
```

Neither do we observe a statistically significant relationship between days of delay and effect size: $\beta$ = 0.00023, p = 0.121.

```{r d_and_delays}

dat |> sum_lm(y = d, x = delay)

table(dat$yes_delay)

dat |> sum_lm(d, yes_delay)
## What's the longest attitude/behavior delay measurement?
dat |>
  group_by(scale_type) |>
  summarize(max_delay = max(delay))
```

This suggests that results are not being driven by short-term outcome measurement. However, we caution to note that our perpetration and victimization behavioral outcomes are \textit{counts of incidents}, which means they can only either flatline or increase in magnitude over time. Comparing the relationship between behavioral effect  and elapsed time to the relationship betewen  mentalizing effects and the passage of time is not comparing like to like. We also note that our measure of `delay' captures the number of days between a treatment's \textit{conclusion} and outcome measurement. If a treatment had multiple `doses' \tetxemdash for example, multiple classroom sessions over the course of a semester \textemdash then a behavioral survey administered on the last session would be recording incidents of sexual violence over a whole semester, but be marked in our study as having a delay of zero. Thus we caution readers to not read too much into the relationship between effect sizes and time elapsed. 

# Pre-registered analyses (from Prospero Doc)

> Analysis of subgroups or subsets: We will compare subsets of the data according to classifications of evaluation methodology (i.e., observational vs. quasi-experimental vs. experimental methods) and of outcome measurement (i.e., general behavior indices vs. perpetration behavior, and attitudinal measurement vs. behavioral measurement). 

This we did in the main text.

> Evaluate the overall effect size of behavioral outcomes for all of the studies in the dataset, both in the short- and in the long-term. 

```{r}
# number of studies that measure behavior
dat |> 
  filter(scale_type == 'behavior') |>
  study_count()

# study count with and without a delay?
dat |> 
  filter(scale_type == 'behavior') |>
  group_by(yes_delay) |> 
  study_count()
# note that this is larger than the number of studies that measure behavior
# because some studies have outcome measurements at 
# multiple points

# now effect sizes
dat |> filter(scale_type == 'behavior') |>
  split(~yes_delay) |>
  map(.f = map_robust)
# big diff, but is that just capturing 
# difference between bystander/involvement
# and victimization/perpetration?

dat |> 
  filter(behavior_type %in% c('Victimization', 'Perpetration')) |>
  split(~yes_delay) |>
  map(.f = map_robust)
# now the relationship is in the opposite direction
# how many studies measure vict/perp with no delay?

dat |> 
  filter(behavior_type %in% c('Victimization', 'Perpetration')) |> 
  group_by(yes_delay) |>
  study_count()
# very few

```


> Compare the effect size of behavioral outcomes for experimental and quasi-experimental studies vs. non-experimental observational studies, both in the short- and in the long-term. 

```{r behavioral_attenuation_by_study_design}
# N of studies
dat |> 
  filter(scale_type == 'behavior') |>
  group_by(study_design, yes_delay) |> 
  study_count()

dat |>
  filter(scale_type == 'behavior') |>
  split(~interaction(study_design, yes_delay)) |>
  map(map_robust)

```

> Compare the effect size of behavioral outcomes for studies with random assignment vs. quasi-experimental studies, both in the short- and in the long-term.

See above table.


> Evaluate at the effect size of perpetration behavioral outcomes for only studies with random assignment, both in the short- and in the long-term. 

```{r perp_rcts_short_long_term}

dat |> filter(behavior_type == 'Perpetration',
               study_design == 'Randomized Control Trial') |>
  group_by(yes_delay) |> study_count()

# what is the one RCt with short-term perpetration measurement
short_term_rct_perp <- dat |> filter(behavior_type == 'Perpetration',
               study_design == 'Randomized Control Trial', 
               yes_delay == F) |> 
  select(author, year, intervention_name, scale_name, delay, d, se_d)

# average effect size for the remaining studies
dat |> filter(behavior_type == 'Perpetration',
               study_design == 'Randomized Control Trial',
               yes_delay == T) |>
  map_robust()
```
> Compare the effect size of behavioral outcomes and attitudinal outcomes, both in the short- and long-term, among the quasi-experimental and randomly-assigned studies. (The covariation of attitudes and behaviors will be assessed in the full sample of studies.)

We have already reported on behavioral outcomes in the short- and long-term within quasi-experimental and randomized designs. 

Within randomized and quasi-experimental designs,  103 studies measure attitudinal outcomes immediately after treatment, with a pooled effect size of $\Delta$ = 0.313 (se = 0.043), p< 0.0001. By contrast,  114 randomized and quasi-experimental evaluations measure attitudes with a delay, and find a pooled effect size of  $\Delta$ = 0.286 (se = 0.032), p< 0.0001.

```{r attitudes_behaviors_quasi_random_delay_yes_no}

 dat |> 
  filter(study_design == 'Quasi-Experimental' | 
           study_design == 'Randomized Control Trial',
         scale_type == 'attitudes') |>
  group_by(yes_delay) |> study_count()
  
dat |> 
  filter(study_design == 'Quasi-Experimental' | 
           study_design == 'Randomized Control Trial',
         scale_type == 'attitudes') |>
  split(~yes_delay) |> map(map_robust)

``` 


# A note on clustered study designs

> In this study's pre-analysis plan, we detailed a search for studies that were either ``purely observational,\ldots quasi-experimental,\ldots or experimental,''defined as featuring random assignment. We did not anticipate, however, the quantity of cluster-assigned studies in our sample (173 of 304 studies, and 54 of 100 featuring random assignment.) 

```{r cluster_study_redo}

dat |> group_by(cluster_type) |> study_count()
```

> In practice, there were 10 studies with fewer than 10 clusters, drawn from 8 papers and comprising 22 effect size estimates in total. If we reclassify all of these studies as RCTs, our overall meta-analytic estimate for RCTs moves from $\Delta$ = 0.2364 to $\Delta$ = 0.2312, and for quasi-experimental studies, from $\Delta$ = 0.2157 to $\Delta$ = 0.2173.

```{r reclassify_low_cluster_rcts}
dat |>
  mutate(study_design = if_else(robust_quasi_check == 1,
                                'Randomized Control Trial', 
                                as.character(study_design))) |>
  split(~study_design) |>
  map(map_robust)
```

### Synthesizing dichotomous and continuous dependent variables

> Using the conventional conversion formula from odds ratio to Cohen's d, ATEs that correspond in reductions of incidence from...

```{r dip_vs_odds_ratio}
odds_ratio_to_d(.6, .3)
dip_calc(.6, .3)
odds_ratio_to_d(.6, .3) == odds_ratio_to_d(.06, .03)
dip_calc(.6, .3) - dip_calc(.06, .03)

# A visualization of the intuition behind this estimator: 
# distribution of SDs drawn from Bernoulli distribution

proportions <- seq(from = 0, to = 1, by = 0.01)
SDs <- as.numeric(lapply(X = proportions, FUN = function(p){sqrt(p * (1 - p))}))

ggplot() + geom_point(aes(proportions, SDs)) + theme_minimal()

# test: what's a DIP that corresponds to approximately d = 0.28,
# our overall average
dip_calc(0.05, 0.015)
```

