---
title: "3-appendixes"
author: "Seth Ariel Green"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output: html_document
editor_options: 
  chunk_output_type: console
---


**Libraries, functions and data**

```{r setup, message=F, echo=F}

#' libraries
library(dplyr, warn.conflicts = F)
library(knitr)
library(lubridate)
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(ggrepel, warn.conflicts = F)
library(purrr, warn.conflicts = F)
library(stringr)
library(tidyr, warn.conflicts = F)

#' functions
source('./functions/d_calc.R')
source('./functions/dip_calc.R')
source('./functions/map_robust.R')
source('./functions/odds_ratio_to_d.R')
source('./functions/study_count.R')
source('./functions/sum_lm.R')
source('./functions/var_d_calc.R')

#' data
dat <- readRDS(file = '../data/sv_meta_data.rds') |>
  select(unique_study_id, scale_type, everything())

```

## Robustness checks

> Our main meta-analytic result comes from a random effects meta-analysis clustering at the level of individual studies.

```{r main_result_for_comparison}

robust(x = rma(yi = d, vi = var_d, data = dat),
       cluster = dat$unique_study_id)

```

> Clustering at the level of paper rather than study raises the standard error to 0.0287, while removing the cluster entirely lowers it to 0.0206; in both cases, the estimate of $\Delta$ does not change, and both are significant at the p < .0001 level.

```{r cluster_paper_level}
# cluster at paper
robust(x = rma(yi = d, vi = var_d, data = dat),
       cluster = dat$unique_paper_id)
0.0287 - 0.0247
#no cluster
rma(yi = d, vi = var_d, data = dat)

```

> Averaging $\Delta$ within each study and then meta-analyzing the means raises the result somewhat to 0.326 (se = 0.028), p < .0001,
 
```{r collapse_individual_ds}

dat |> group_by(unique_study_id) |>
  mutate(d = mean(d),
         var_d = mean(var_d)) |>
  slice(1) |>
  map_robust()
```

> while averaging $\Delta$ within each study's behavioral and ideas outcomes separately leads to $\Delta$ = 0.302
 
```{r collapse_ds_within_study_and_scale_type}
dat |> group_by(unique_study_id, scale_type) |>
  mutate(d = mean(d),
         var_d = mean(var_d)) |>
  slice(1) |>
  map_robust()
```


## Additional checks for publication bias

> In our sample, the pooled effect size in the largest quintile of studies, with at least 496 participants combined in treatment and control groups, is $\Delta$ = 0.275 (se = 0.073), while those in our smallest quintile, which have between 20 and 83 participants, is $\Delta$ = 0.302 (se = 0.043). In other words, neither is more 0.02 away from our overall effect size of $\Delta$ = 0.281.

```{r quintile_check}
# what are the quantiles
dat |> group_by(quantile_var) |> 
  summarise(min_n = min(n_t_post + n_c_post),
            max_n = max(n_t_post + n_c_post))

dat |> 
  split(~quantile_var) |> map(.f = map_robust)
  
dat |> sum_lm(y = d, x = quantile_var) # it's basically nothing
```

> Neither do we observe a statistically significant relationship between days of delay and effect size: $\beta$ = 0.00023, p = 0.121.

```{r d_and_delays}

dat |> sum_lm(y = d, x = delay)

table(dat$yes_delay)

dat |> sum_lm(d, yes_delay)
## What's the longest ideas/behavior delay measurement?
dat |>
  group_by(scale_type) |>
  summarize(max_delay = max(delay))
```

## Pre-registered analyses not in the main text

**Evaluate the overall effect size of behavioral outcomes for all of the studies in the dataset, both in the short- and in the long-term**

> Seventeen studies measure behavioral outcomes within one day of the conclusion of treatment and produce a combined effect size of $\Delta$ = 0.125 (SE = 0.057), p = 0.0426, while 83 studies measure behavioral outcomes at least a day after treatment concludes, and produce a combined effect size of $\Delta$ = 0.063 (SE = 0.023), p = 0.0081.

```{r short_long_term_behavioral}
dat |> 
  filter(scale_type == 'behavior') |>
  split(~yes_delay) |> 
  imap(~bind_cols(study_count(.x), map_robust(.x)) |> kable('markdown')) 
#' note that this is larger than the number of studies that measure behavior
#' because some studies have outcome measurements at 
#' multiple points

#' big diff, but is that just capturing 
#' difference between bystander/involvement
#' and victimization/perpetration?

dat |> 
  split(~interaction(yes_delay, behavior_type)) |>
  imap(~bind_cols(study_count(.x), map_robust(.x)) |> kable('markdown')) 

#' yes, bystander behaviors tend to be much larger than other categories.
#' we cover this in the main text.
```

**Analysis of subgroups or subsets: We will compare subsets of the data according to classifications of evaluation methodology (i.e., observational vs. quasi-experimental vs. experimental methods) and of outcome measurement (i.e., general behavior indices vs. perpetration behavior, and attitudinal measurement vs. behavioral measurement).**

This we did in the main text.

**Compare the effect size of behavioral outcomes for experimental and quasi-experimental studies vs. non-experimental observational studies, both in the short- and in the long-term.** 
```{r behavioral_attenuation_by_study_design}
dat |> 
  filter(scale_type == 'behavior') |>
  split(~interaction(study_design, yes_delay)) |> 
  imap(~bind_cols(study_count(.x), map_robust(.x)) |> kable('markdown'))
```

**Compare the effect size of behavioral outcomes for studies with random assignment vs. quasi-experimental studies, both in the short- and in the long-term.**

See above table.

**Evaluate at the effect size of perpetration behavioral outcomes for only studies with random assignment, both in the short- and in the long-term. **

```{r perp_rcts_short_long_term}

dat |> filter(behavior_type == 'Perpetration',
               study_design == 'Randomized Controlled Trial') |>
  group_by(yes_delay) |> study_count()

# what is the *one* RCT with short-term perpetration measurement
dat |> filter(behavior_type == 'Perpetration',
               study_design == 'Randomized Controlled Trial', 
               yes_delay == F) |> 
  select(author, year, intervention_name, scale_name, delay, d, se_d)

# average effect size for the remaining studies
dat |> filter(behavior_type == 'Perpetration',
               study_design == 'Randomized Controlled Trial',
               yes_delay == T) |>
  map_robust()
```

**Compare the effect size of behavioral outcomes and attitudinal outcomes, both in the short- and long-term, among the quasi-experimental and randomly-assigned studies. (The covariation of ideas and behaviors will be assessed in the full sample of studies.)**

> Within randomized and quasi-experimental designs,  104 studies measure ideas-based outcomes immediately after treatment, with a pooled effect size of $\Delta$ = 0.311 (se = 0.042), p $<$ 0.0001. By contrast,  114 randomized and quasi-experimental evaluations measure ideas with a delay, and find a pooled effect size of  $\Delta$ = 0.288 (se = 0.032), p $<$ 0.0001.

```{r ideas_behaviors_quasi_random_delay_yes_no}

 dat |> 
  filter(study_design == 'Quasi-Experimental' | 
           study_design == 'Randomized Controlled Trial',
         scale_type == 'ideas') |>
  split(~yes_delay) |>
  imap(~bind_cols(study_count(.x), map_robust(.x)) |> kable('markdown'))
```

# A note on clustered study designs

> In this study's pre-analysis plan, we detailed a search for studies that were either ``purely observational,\ldots quasi-experimental,\ldots or experimental,'' defined as featuring random assignment. We did not anticipate, however, the quantity of cluster-assigned studies in our sample (174 cluster assignemnts, and 54 of 100 featuring random assignment.) 

```{r cluster_study_redo}

dat |> filter(!is.na(cluster_type)) |> group_by(ra) |> study_count()

dat |> group_by(ra) |> study_count()

```

> In practice, there were 8 studies with fewer than 10 clusters, drawn from 6 papers and comprising 22 effect size estimates in total. If we reclassify all of these studies as RCTs, our overall meta-analytic estimate for RCTs moves from $\Delta$ = 0.2364 to $\Delta$ = 0.2312, and for quasi-experimental studies, from $\Delta$ = 0.2157 to $\Delta$ = 0.2173.

```{r reclassify_low_cluster_rcts}
#' Ns
robust_quasi <- dat |> filter(robust_quasi_check)
length(unique(robust_quasi$unique_paper_id))
robust_quasi |> study_count()

#' effect sizes
dat |> split(~study_design) |> map(map_robust)

dat |>
  mutate(study_design = if_else(robust_quasi_check,
                                'Randomized Controlled Trial', 
                                as.character(study_design))) |>
  split(~study_design) |>
  map(map_robust)
```

### Synthesizing dichotomous and continuous dependent variables

> Using the conventional conversion formula from odds ratio to Cohen's d, ATEs that correspond in reductions of incidence from...

```{r dip_vs_odds_ratio}
odds_ratio_to_d(.6, .3)
difference_in_proportion_calc(.6, .3)
odds_ratio_to_d(.6, .3) == odds_ratio_to_d(.06, .03)
difference_in_proportion_calc(.6, .3) - difference_in_proportion_calc(.06, .03)

# A visualization of the intuition behind this estimator: 
# distribution of SDs drawn from Bernoulli distribution

proportions <- seq(from = 0, to = 1, by = 0.01)
SDs <- as.numeric(lapply(X = proportions, FUN = function(p){sqrt(p * (1 - p))}))

ggplot() + geom_point(aes(proportions, SDs)) + theme_minimal()

# test: what's a DIP that corresponds to approximately d = 0.28,
# our overall average
difference_in_proportion_calc(0.05, 0.015)
```

