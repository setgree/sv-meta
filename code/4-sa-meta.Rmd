---
title: "3-SA-meta-analyis.Rmd"
author: "Seth Green"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
editor_options: 
  chunk_output_type: console
---

### Overview

This document will work through our results, and its aim is to both reflect the pre-analyis plan as it was written in 2018 and the most up-to-date version of the text. Everything in our pre-analysis plan will be included in either the main results section or an appendix, but not necessarily in the order it was drafted up.

This document will precede chunks with quotes prepended either a "*paper*" or a "*pap*" to denote whether the text is from the paper or the pre-analysis plan.

## Setup

### load libraries

```{r setup, include=F, echo=F}
rm(list = ls())
library(dplyr, warn.conflicts = F)
library(knitr)
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(ggrepel, warn.conflicts = F)
library(lubridate)
library(purrr)
library(stringr)
library(tidyr, warn.conflicts = F)
library(tibble)

source('./functions/sum_lm.R')
source('./functions/map_robust.R')
source('./functions/study_count.R')
# NOTE ON SUM_LM: 
# `sum_lm(dat, d, se_d, coef_only = T)` = `summary(lm(formula = d ~ se_d, 
#                                          data = dat))$coefficients`

options(scipen = 99)
```

### load data
```{r load_data, include=F, echo=F}
dat <- readRDS(file = '../data/sa_meta_data_final.rds') |>
  select(unique_study_id, scale_type, everything())
```

## Results 

### Overall results and publication bias 

*paper*: "Our random effects meta-analysis of all primary prevention strategies from 1985 to 2018 reveals an overall effect size, across all interventions on all kinds of outcomes, of $\Delta$ = 0.281 (se = 0.024). This effect is statistically significant at p $<$ 0.0001."
```{r overall_meta}
dat |> map_robust()

# more verbose version of
robust(x = rma(yi = d, vi = var_d, data = dat),
       cluster = dat$unique_study_id)
```


# Figure 1
```{r figure_1}

dat |> ggplot(mapping = aes(y = d, 
                             x = se_d)) + 
  geom_point(aes(color = behavior_type,
             shape = study_design),
             size = 3) + 
  stat_smooth(method = 'lm', se = F) + 
  ggtitle("Relationship between effect sizes and standard errors") +
  labs(x = "Standard errors",
       y = "Effect size",
       color = "Outcome type",
       shape = "Study Design") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))

```

"First, we do not observe a significant relationship between effect sizes and standard errors: $\beta$ = 0.145 (se = 0.176),  p = 0.411"

```{r publication_bias_overall}
sum_lm(dat, d, se_d, coefs_only = T)
```

"Second, the relationship between SE and $\Delta$ remains insignificant within randomized (N = XXX) and quasi-experimental (N = XXX) designs, but highly significant within observational designs (N = XXX)"

```{r publication_bias_by_study_design}
# Ns
dat |>
  group_by(study_design) |>
  study_count()
# effect sizes
dat |> 
  split(~study_design) |> 
  map(.f = ~sum_lm(y = d, x = se_d, dat = .)) # can just do `map(.f = ~sum_lm())
```

"Our third check for publication bias is to separate the literature into studies published in peer-reviewed journals (N = 186) and those not (N = 118)."

```{r published_vs_unpubished}

dat |> 
  group_by(published_unpublished) |> 
    study_count()

# sanity check
length(unique(dat$unique_study_id)) == 186 + 118
```

 In our sample, being unpublished is associated with a small reduction in effect size ($\beta$  = -0.0467), but not at a statistically significant level (p = 0.28).
 
```{r publish_relationship_or_not}
dat |> sum_lm(d, published_unpublished)

# Difference in relationship by published and unpublished
dat |> split(~published_unpublished) |> map(sum_lm)

dat |> split(~published_unpublished) |> map(map_robust)
```


Neither do we observe a statistically significant relationship between days of delay and effect size: $\beta$ = 0.00023, p = 0.121. (We discuss this relationship, and the difficulties of interpreting it, in an appendix).

However,  when pooling all effects, we observe that results vary both significantly and substantively by study design. 

```{r differences_by_study_design}
# count Ns (again, just for convenience of writing)
dat |> group_by(study_design) |> 
  study_count()

# check overall effect sizes
dat |>
  split(~study_design) |>
  map(.f = map_robust)

#TODO: can we reshape these two objects to be one table and then make them a latex table? 
# and then eventually a function which just takes dat as its input?
```

\subsection{Do bystander interventions Change Behavior?}

We tested this on the 96 bystander-related interventions in our database, the majority (76/96) of which measure ideas about sexual violence like beliefs, attitudes, and knowledge, and are successful at changing them, $\Delta$ = .398 (SE = 0.069), p $<$ 0.001.  43 of the 96 bystander studies measure behaviors. Overall, 23 studies measured whether bystander interventions increase bystander behaviors, which they do to a modest extent $\Delta$ = 0.15 (SE = 0.056), p = 0.01.  In our database, 20 studies measure perpetration or victimization, the ultimate goal of these interventions. We did not find that bystander interventions meaningfully change rates of perpetration  ($\Delta$ = 0.019 (SE = 0.019), p = 0.329 or victimization ($\Delta$ =-0.009 (SE = 0.041),  p = 0.835. This suggests, unfortunately, that nearly 1 in 3 studies in our database is pursuing a theory of behavioral change that is not grounded in reality.

```{r bystander_dv_count}

# filter based on mentioning bystander in either title, description, 
# intervention name, or program name, or have a stated purpose of "4" (bystander)
bystander_dat <- dat |>
  filter(
    str_detect(paper_title, "bystander") |
      str_detect(brief_description_of_the_intervention, "bystander") |
      str_detect(intervention_name, "bystander") |
      str_detect(program_name, "bystander") |
      str_detect(stated_purpose, "4")) |>
    select(author, year, paper_title, unique_study_id, study_design,
           d, var_d, se_d, 
           behavior_type, has_both, country) |>
  mutate(perp_vict_grouped = case_when(
    behavior_type == 'Victimization' | behavior_type == 'Perpetration' ~ 'perp_vict',
    behavior_type == 'Involvement' ~ 'involvement',
    behavior_type == 'Attitude' ~ 'Attitude',
    behavior_type == 'Bystander' ~ 'Bystander'))

bystander_dat |>  study_count()

bystander_dat |> group_by(behavior_type) |> study_count()

bystander_dat |> group_by(has_both) |> study_count()

bystander_dat |> filter(behavior_type != 'Involvement') |> 
  split(~perp_vict_grouped) |> 
  map(.f = map_robust)

table(bystander_dat$behavior_type)    

# count studies that measure #just bystander behaviors
bystander_dat |>
  filter(has_both != 'attitudes') |> group_by(perp_vict_grouped) |> study_count()

# double-check -- define this object (it got lost)
# bystander_dat_not_just_atts |> 
#   group_by(unique_study_id) |>
#   filter('Bystander' %in% behavior_type & !'perp_vict' %in% perp_vict_grouped) |> ungroup() |> study_count()


# what kind of effect sizes are we talking about? 
bystander_dat |> filter(behavior_type != 'Involvement') |> split(~behavior_type) |> map(map_robust)

# group perp and vict outcomes together

bystander_dat |> filter(behavior_type != 'Involvement') |> split(~perp_vict_grouped) |> map(map_robust)

# bystander interventions are a product of their time and place
bystander_dat |> group_by(unique_study_id) |> slice(1) |> ungroup() |>
  ggplot(aes(x = year)) +
  geom_line(stat = 'count', color = 'light blue') + 
  labs(x = "Year", 
       y = "Count", 
       title = "Bystander interventions over time") +
  # scale_x_date(date_breaks = "2 years") +
  scale_x_continuous(n.breaks = 20) +
  scale_y_continuous(n.breaks = 15) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

## BTW an alternate way to do this
dat |>
  group_by(unique_study_id) |>
  summarise(
    bystander_count = ifelse(
      any(
        str_detect(c(
          brief_description_of_the_intervention, 
          intervention_name, program_name, paper_title), "bystander")) |
        any(str_detect(stated_purpose, "4")),
      TRUE, FALSE)
  ) |> 
  summarise(
    bystander = sum(ifelse(is.na(bystander_count), FALSE, bystander_count)),
    no_bystander = sum(ifelse(is.na(bystander_count), TRUE, !bystander_count))
  )

# where do bystander interventions occur
bystander_dat |> group_by(country) |> study_count()
```



# Attitudinal Change exceeds behavioral change in every category of study

In our sample, attitudinal outcomes outnumber behavioral outcomes by 355 to 145. 207 studies measure attitudes only, 36 measure only behaviors, while 61 measure both. 

```{r att_beh_breakdown}
table(dat$scale_type)

dat |> 
  group_by(has_both) |>
  study_count()

# sanity check
length(unique(dat$unique_study_id)) == 207 + 36 + 61 # true
```

Behavioral measurement has become more common over time, particularly from 2010 onward. 

```{r behavioral_outcomes_by_decade}
range(dat$year)

# how many studies in each decade
dat |> group_by(decade) |> study_count()

tab <- merge(x = dat |> group_by(decade) |> study_count(),
             y = dat |> group_by(decade, scale_type) |> summarise(n = n())) 
# print tab
tab

#reshape and make latex table
tab |>
  pivot_wider(names_from = scale_type, values_from = n) |>
  mutate(ratio = attitudes/behavior) |>
  knitr::kable(format = 'latex')

# what was the earliest study to measure behavior as we conceive it?
dat |> filter(scale_type == 'behavior') |> 
  arrange(year) |> 
  select(author, year, paper_title) |> head()



## what if we just do this for behaviors?
dat |>
  ggplot(aes(x = year, color = behavior_type)) +
  geom_line(stat = 'count') + 
  labs(x = "Year", 
       y = "Count", 
       color = "Behavior type",
       title = "Categories of behavioral outcomes over time") +
  # scale_x_date(date_breaks = "2 years") +
  scale_x_continuous(n.breaks = 10) +
  scale_y_continuous(n.breaks = 10) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# can also filter out attitudes if we want (filter(scale_type == 'attitudes'))

```

Overall, behaviors are much more resistant to change than attitudes.  The overall effect size for attitudes is $\Delta$ =  0.367 (se = .03), p < 0.0001; while the overall effect size for behaviors is $\Delta$ = 0.072 (se = 0.022) p = 0.0013. While this is still a statistically significant result, it does not meet the conventional threshold for a 'small' effect size.

```{r behaviors_attitudes_overall}
dat |> 
  split(~scale_type) |>
  map(.f = map_robust)
```

This discrepancy is particularly pronounced in observational research, where attitudinal outcomes are about 18 times larger than behavioral responses; but the discrepancy is substantial across every category of study design. However, it is notable that the largest behavioral effects are found within randomized studies, i.e. the most rigorous designs.

```{r behavior_attitudes_discrepancy_within_study_designs}

# first, Ns
dat |> 
  group_by(scale_type, study_design) |> 
  study_count() |>
  kable('latex')

# then effect sizes
dat |>
  split(~interaction(scale_type, study_design)) |> 
  map(.f = map_robust)

```

In our dataset, behavioral outcomes can be classified as victimization, perpetration, bystander (intervening in a situation where perpetration is perceived to be a risk), and involvement (for example, signing up to be part of a rape awareness group on campus).

Table XXX shows the number of studies which measure each behavioral category and their pooled effect sizes, as well as attitudes for reference.

```{r categories_of_behavior_and_attitudes}
#Ns
dat |> 
  group_by(behavior_type) |> 
  summarize('N (studies)' = n_distinct(unique_study_id)) |>
  kable('latex')

dat |> 
  split(~behavior_type) |> 
  map(.f = map_robust)
```

Here we see that primary prevention interventions do not, on average, meaningfully reduce victimization or perpetration. This, we think, is the key finding of this meta-analysis. % if I were reading this right now I'd want more zoom in on how different _categories_ of intervention reduce victimization & perpetration. 

### RCT perpetration 

```{r rct_perpetration}
dat |> 
  filter(study_design == 'Randomized Control Trial' & behavior_type == 'Perpetration') |> 
  map_robust()
```
# correlation between attitudes and behavior

Another way to assess the discrepancy between attitudes and behaviors is to focus on the 61 studies that measure both and look at the association between the two categories.

```{r dat_has_both_count}
dat_has_both <- dat |> filter(has_both == 'both')
dat_has_both |> study_count()
```

In this subset of studies, the pooled attitudinal effect is $\Delta$ = 0.278 (se = .036), p < .0001, while the pooled behavioral outcome is $\Delta$ = 0.081 (se = .03), p = .009. In other words, the attitudinal effects are slightly lower than the overall attitude effect of 0.367, while the behavioral effects are slightly larger than the overall behavioral effect of 0.072.
```{r has_both_behav_att_meta_results}
dat_has_both |> split(~scale_type) |> 
  map(map_robust)
```

A key question for this literature is: when attitudes change, do behaviors change with them? Unfortunately, in the studies that measure change across both categories, we find a small, statistically insignificant correlation of $\rho$ (59) = 0.141, p = 0.279.

```{r attitude_behavior_correlation_overall}
## TODO John-Henry: is there a way to get the behavior_type that corresponds 
# to the behavior (rather than 'attitude') into this reshaped dataset, 
# such that we could:
# A) subsequently filter it down just to look at each kind behavior
# B) color code the scatterplot 

has_both_means <- dat_has_both |> 
  group_by(unique_study_id, scale_type) |>
  mutate(mean_d = mean(d), 
         mean_var_d = mean(var_d), 
         mean_se_d = mean(se_d)) |>
  select(author, year, paper_title, unique_study_id, intervention_name,
         study_design, mean_d, mean_var_d, mean_se_d,
         ra, labels, scale_type, decade, behavior_type) |>  
  slice(1) 

has_both_reshaped <- has_both_means |> 
  pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels, decade),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))

cor.test(x = has_both_reshaped$mean_d_attitudes, 
         y = has_both_reshaped$mean_d_behavior)
```

We find broadly similar results when looking at just at RCTs; ($\rho$ (25) = 0.16, p = 0.468); just at studies that measure both attitudes and either victimization or perpetration outcomes ($\rho$ (42) = 0.153, p = 0.322); and, finally, within RCTs that measure attitudes as well as either victimization or perpetration outcomes ($\rho$ (59) = 0.141, p = 0.279).

```{r attitude_behavior_correlation_subsets}
# RCTs only?
rcts_both <- has_both_reshaped |> filter(study_design == 'Randomized Control Trial')
cor.test(x = rcts_both$mean_d_attitudes, 
         y = rcts_both$mean_d_behavior)
## TODO: Not enough 
#perp and vict only?
perp_and_vict_both <- has_both_means |>
  group_by(unique_study_id) |>
  filter(if_else(any(behavior_type %in% c('involvement', 'bystander')), F, T)) |>
   pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels, decade),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))

cor.test(perp_and_vict_both$mean_d_attitudes,
         perp_and_vict_both$mean_d_behavior)


# finally, perp and vict only in RCTs only

perp_and_vict_rcts <- has_both_means |>
  group_by(unique_study_id) |>
  filter(if_else(any(behavior_type %in% c('Involvement', 'Bystander')), F, T)) |>
  filter(study_design == 'Randomized Control Trial') |>
   pivot_wider(
    id_cols = c(author, year, paper_title, unique_study_id,
                intervention_name,
                unique_study_id, study_design, ra, labels, decade),
    names_from = c(scale_type), 
    values_from = c(mean_d, mean_var_d, mean_se_d))

cor.test(perp_and_vict_rcts$mean_d_attitudes,
         perp_and_vict_rcts$mean_d_behavior)
```

Figure XXX highlights this lack of correlation. Each point in the scatterplot is a study that measures both attitudes and behaviors, with the attitudinal effect size on the X axis and the behavioral effect size on the Y axis. The dotted black line shows what a perfect correlation would look like, and the dotted gray line shows the correlation that we actually find. Studies are color-coded by design.

```{r correlation_scatterplot}
dat_has_both <- dat |> filter(has_both == 'both')

dat |>
  filter(has_both == 'both') |>
  group_by(author, year, study_design, unique_study_id, scale_type) |>
  filter(delay == min(delay)) |>
  summarise(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) |>
  pivot_wider(id_cols = c(author, year, unique_study_id, study_design),
    names_from = c(scale_type),
    values_from = c(mean_d, mean_var_d, mean_se_d)) |>
  ggplot(aes(x = mean_d_attitudes,
             y = mean_d_behavior)) +
  geom_point(aes(color = study_design),
             size = 3) +
  geom_abline(slope = 1,
              lty = 'dashed') +
  geom_smooth(lty = "dashed",
              method = "lm",
              se = FALSE,
              color = 'grey') +
  labs(#title = "Correlation between attitudinal and behavioral change",
       x = "Effect size (ideas)",
       y = "Effect size (behaviors)",
       color = "Study Design") +
  guides(color = guide_legend(title = "Study Design")) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```

In sum, while attitudes towards rape myths appear to malleable, the problem of rape itself is much more resistant to change. 
