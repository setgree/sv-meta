---
title: "3-SA-meta-analyis.Rmd"
author: "Seth Green"
date: "8/19/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=F, echo=F}
rm(list = ls())
library(broom)
library(dplyr)
library(metafor, quietly = T)
library(rmeta)
library(ggplot2)
library(ggrepel)
library(stringr)
library(tidyr)

source('./functions/dplot.R')

# read in data and also add in two useful variables: 
#  labels (for beahvior) and attitudes_behaviors

dat <- readRDS(file = './data/sa_meta_data_for_analysis.rds') %>% 
  select(unique_study_id, scale_type, everything()) %>% 
  mutate(labels = paste(tools::toTitleCase(word(author)), year)) %>%
  group_by(unique_study_id) %>%
  mutate(attitudes_behaviors = case_when(
    all(c('attitudes', 'behavior') %in% scale_type) ~ 0,
    scale_type == 'attitudes' ~ 1,
    scale_type == 'behavior' ~ 2)) %>% 
  ungroup()


```

### First, an overall meta-analysis. 

```{r initial_exploration}

#first, is there a relationship between effect size and se?
summary(lm(formula = se_d ~ d, data =  dat))
summary(lm(formula = d ~ se_d, data =  dat))

# This is less of a relationship than I (Seth) expected;
# Roni is not very surprised because public health is in general less p-hacked than psych
# JH agrees and there's an interesting point about not going for flashy journals

# how about the overall relationship between lab/field and d? 
summary(lm(formula = d ~ lab_field, data =  dat)) 

# essentially no difference at all

# study type?
summary(lm(formula = d ~ study_design, data =  dat)) 
# yep, meaningful relationship

# scale type?
summary(lm(formula = d ~ scale_type, data =  dat)) 
# that is a substantial decline

# new variable for whether a study has attitudes, behaviors, or both (used later)
dat <- dat %>% 

```


### By study type:
"Analysis of subgroups or subsets: We will compare subsets of the data according to classifications of evaluation methodology (i.e., observational vs. quasi-experimental vs. experimental methods)..."
```{r study_type}
# first crack at meta-analysis
# this can all, obviously, be more elegant -- consult with JH
initial_meta <- robust(x = rma(yi = d, vi = var_d, data = dat), cluster = dat$unique_study_id); initial_meta

# what does grouping by study and then slicing do to the results?
dat_grouped <- dat %>%
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) %>%
  slice(1)
robust(x = rma(yi = d, vi = var_d, data = dat_grouped), cluster = dat_grouped$unique_study_id)
# once we've already grouped by study_id, whatdo we need to cluster?
grouped_meta <- rma(yi = d, vi = var_d, data = dat_grouped); grouped_meta # basically the same
# these are very similar. How about not grouping by scale_type?

dat_grouped_two <- dat %>%
  group_by(unique_study_id) %>%
  mutate(mean_d = mean(d),
         mean_var_d = mean(var_d),
         mean_se_d = mean(se_d)) %>%
  slice(1)
robust(x = rma(yi = d, vi = var_d, data = dat_grouped_two), cluster = dat_grouped_two$unique_study_id)
# again, very similar.Consult with group


# by study type?
rcts <- dat %>% filter(study_design == 'rct')
robust(x = rma(yi = d, vi = var_d, data = rcts), cluster = rcts$unique_study_id)

quasis <- dat %>% filter(study_design == 'quasi-experimental')
robust(x = rma(yi = d, vi = var_d, data = quasis), cluster = quasis$unique_study_id)

rcts_and_quasi <- dat %>% filter(study_design == 'rct' | study_design == 'quasi-experimental')
robust(x = rma(yi = d, vi = var_d, data = rcts_and_quasi), cluster = rcts_and_quasi$unique_study_id)

observational <- dat %>% filter(study_design == 'pre-post' | study_design == 'cross-sectional')
robust(x = rma(yi = d, vi = var_d, data = observational), cluster = observational$unique_study_id)

```

### By outcome type:

"... and of outcome measurement (i.e., general behavior indices vs. perpetration behavior, and attitudinal measurement vs. behavioral measurement)."

```{r outcome_type}
# let's talk about this -- what's the right way to filter by   behavior type? 
# unique(dat$scale_name)
behavioral_data <- dat %>% filter(scale_type == 'behavior')

attitudes_data <- dat %>% filter(scale_type == 'attitudes')

robust(x = rma(yi = d, vi = var_d, data = behavioral_data),
       cluster = behavioral_data$unique_study_id)
robust(x = rma(yi = d, vi = var_d, data = attitudes_data),
       cluster = attitudes_data$unique_study_id)

### CATEGORIZE BEHAVIOR:
# perpetration, victimization, intervention are the big three intervention measures. Also, there's involvement, I now see
# unique(behavioral_data$scale_name)
behavioral_data <- behavioral_data %>% mutate(
  type_of_behavior = as.factor(case_when(
    str_detect(scale_name, 
               'perp|perpetration|agression|comitted') ~ 'perpetration',
    str_detect(scale_name, 'vict|victimization|completed rape|underwent|ses-past year|survivor') ~ 'victimization',
    str_detect(scale_name, 'bystander|Observing') ~ 'bystander',
    str_detect(scale_name, 'volunteer|support|involvement') ~ 'involvement',
    TRUE ~ 'other')
    )
)
# TODO: this was crude, because the scale_names weren't written with this analysis in mind. I think we should rewrite each of the scale names to clearly state which of the four categories it falls into, and I will discuss this with JH & Roni when we next speak. Anyway:

summary(lm(formula = d ~ type_of_behavior, data = behavioral_data))
# on average, interventions seem to be more effective at reducing victimization than perpetration, but the differences aren't huge. The effects are pretty small across the board.
robust(rma(yi = d, vi = var_d, mods = type_of_behavior, data = behavioral_data), cluster = behavioral_data$unique_study_id)
# This isn't quite right, and ideally we want coefficients associated with each 
# that correspond to different effect sizes within each category
# what about just RCTs? 
rct_behavioral <- behavioral_data %>% filter(study_design == 'rct')
summary(lm(formula = d ~ type_of_behavior, data = rct_behavioral))
# effects are small here overall
robust(rma(yi = d, vi = var_d, data = rct_behavioral), cluster = rct_behavioral$unique_study_id)

```

* Evaluate the overall effect size of behavioral outcomes for all of the studies in the dataset, both in the short- and in the long-term. 
```{r behavioral_over_time}
summary(lm(formula = d ~ scale_type, data =  dat)) 
robust(x = rma(yi = d, vi = var_d, data = behavioral_data), 
       cluster = behavioral_data$unique_study_id)

no_delay_dat <- dat %>% 
  filter(delay == 0)
delay_dat <- dat %>%
  filter(delay > 0)
summary(lm(formula = d ~ scale_type, data =  delay_dat))
summary(lm(formula = d ~ scale_type, data =  no_delay_dat))
# in general, delayed results are smaller. this is crude and needs revisiting
summary(lm(formula = d ~ delay, data = dat))
# don't knwo what to make of this
robust(x = rma(yi = d, vi = var_d, data = no_delay_dat), 
       cluster = no_delay_dat$unique_study_id)

robust(x = rma(yi = d, vi = var_d, data = delay_dat), 
       cluster = delay_dat$unique_study_id)

```

* Compare the effect size of behavioral outcomes for experimental and quasi-experimental studies vs. non-experimental observational studies, both in the short- and in the long-term. 
* Compare the effect size of behavioral outcomes for studies with random assignment vs. quasi-experimental studies, both in the short- and in the long-term. 

```{r}
# so we need a dummy variable for experimental or quasi_experimental
# comparing study designs overall
summary(lm(formula = d ~ study_design, data =  behavioral_data))
# just within studies with delayed outcomes?
delay_behavior <- behavioral_data %>% filter(delay > 0)
no_delay_behavior <- behavioral_data %>% filter(delay == 0)
summary(lm(formula = d ~ study_design, data =  no_delay_behavior))
summary(lm(formula = d ~ study_design, data =  delay_behavior))
# what does this subset like overall
robust(rma(yi = d, vi = var_d, data = delay_behavior), cluster = delay_behavior$unique_study_id)
robust(rma(yi = d, vi = var_d, data =  no_delay_behavior), cluster = no_delay_behavior$unique_study_id)

```

* Evaluate at the effect size of perpetration behavioral outcomes for only studies with random assignment, both in the short- and in the long-term. 
```{r perpetration outcomes}
rct_perp <- rct_behavioral %>%
  filter(type_of_behavior == 'perpetration')
robust(rma(yi = d, vi = var_d, data =  rct_perp), cluster = rct_perp$unique_study_id)
# still about 0.1
# TODO: these _all_ have delays. maybe something like delay > 1 month?
summary(lm(formula = d ~ delay, data = rct_perp))
#...kind of not really any clear relationship?
rct_perp %>% ggplot(mapping = aes(x = delay, y = d)) + 
  geom_point() +
  geom_smooth(method = 'lm')
# the story here is the range of Ds -- what's this -0.5 increase in perpetration?
# TODO: double-check that we coded this in the right direction
range(rct_perp$d); mean(rct_perp$d)
# TODO: all this delay stuff could be addresesd with a delay yes/no variable.
# TBD: sensible thing to call that?
```
* Compare the effect size of behavioral outcomes and attitudinal outcomes, both in the short- and long-term, among the quasi-experimental and randomly-assigned studies. (The covariation of attitudes and behaviors will be assessed in the full sample of studies.)
```{r}
summary(lm(formula = d ~ scale_type, data = rcts_and_quasi))
rcts_and_quasi_delay <- rcts_and_quasi %>% 
  filter(delay > 0)

rcts_and_quasi_no_delay <- rcts_and_quasi %>% 
  filter(delay == 0)

summary(lm(formula = d ~ scale_type, data = rcts_and_quasi_delay))
summary(lm(formula = d ~ scale_type, data = rcts_and_quasi_no_delay))
```

What is the association between attitudes and behaviors (temporal match) – this answers our question of ‘should we care about attitudes?’

```{r}

has_both <- dat %>% 
  filter(attitudes_behaviors == 0) %>%
  group_by(unique_study_id, scale_type) %>%
  mutate(mean_d = mean(d), mean_var_d = mean(var_d), mean_se_d = mean(se_d)) %>%
  select(author, year, paper_title, unique_study_id, 
         mean_d, mean_var_d, mean_se_d, everything()) %>%  
  slice(1) 

cor.test(x = has_both$mean_d[has_both$scale_type == 'attitudes'], 
         y = has_both$mean_d[has_both$scale_type == 'behavior'])

# try pivot wider
# JH -- can I get a hand with this?
has_both_two <- has_both %>% 
  pivot_wider(id_cols = c(author, year, paper_title, unique_study_id, study_design, labels),
              names_from = scale_type, 
              values_from = c(mean_d, mean_var_d, mean_se_d))


cor.test(x = has_both_two$mean_d_attitudes, 
         y = has_both_two$mean_d_behavior)

# RCTs?
has_both_rcts <- has_both_two %>% filter(study_design == 'rct')
cor.test(x = has_both_rcts$mean_d_attitudes, 
         y = has_both_rcts$mean_d_behavior)

# double-check that
has_both_rcts_alternative <- has_both %>% filter(study_design == 'rct')

cor.test(x = has_both_rcts_alternative$mean_d[has_both$scale_type == 'attitudes'], 
         y = has_both_rcts_alternative$mean_d[has_both$scale_type == 'behavior'])

# what's the slope of these lines
summary(lm(mean_d_behavior ~ mean_d_attitudes, data = has_both_two))
summary(lm(mean_d_behavior ~ mean_d_attitudes, data = has_both_rcts))

```


### Plotting
```{r plotting}

# overall linear plot
ggplot(data = dat, mapping = aes(x = se_d, y = d)) +
  geom_point() +
  geom_smooth(method = 'lm') + 
  theme_minimal()


#### Visualizing this: plot D and SE, with color corresponding to
#### study type and shape corresponding to lab or field
dplot(condense = T)
# I had really thought there was more of a relationship
dplot(condense = F)
# if we look just at rtcs and quasi experiments?
dplot(sa_data = rcts_and_quasi)
# What's going on here...are the outlier studies getting a lot of weight
dat %>% filter(d <= 2 & d >= -2) %>% dplot(condense = T)


## Relationship between attitudes and behaviors 

has_both_two %>%
  select(author, year, mean_d_attitudes, mean_d_behavior) %>%
  ggplot(mapping = aes(x = mean_d_attitudes, y = mean_d_behavior)) +
  geom_point(size = 3) + 
  geom_smooth(method = 'lm', se = F, lty = 'dashed') + 
  theme_minimal()

# now same with RCTs 
has_both_rcts %>% 
  select(author, year, mean_d_attitudes, mean_d_behavior, labels) %>%
  ggplot(mapping = aes(x = mean_d_attitudes, y = mean_d_behavior)) +
  geom_point(size = 3) + 
  geom_smooth(aes(fill = NULL), lty = "dashed", fullrange = TRUE, 
              method = "lm",
              show.legend = FALSE, alpha = .1, se = F) + 
  geom_label_repel(mapping = aes(label = labels)) +
  theme_minimal()

# forest plot
rcts_grouped <- rcts %>%
  group_by(unique_study_id) %>%
  mutate(mean_d = mean(d), mean_var_d = mean(var_d), mean_se_d = mean(se_d)) %>%
  select(author, year, paper_title, unique_study_id, 
         mean_d, mean_var_d, mean_se_d, everything()) %>%  
  slice(1) 

meta_rcts_grouped <- rma(yi = mean_d, vi = mean_var_d, data = rcts_grouped, slab = labels)
#forest plot attempt one
pdf('./results/forest-plot.pdf',width = 20, height = 28)
sort.by.error <- rcts_grouped[order(rcts_grouped$mean_se_d), ]
yi <- sort.by.error$mean_d
sei <- sort.by.error$mean_se_d
var_names <- sort.by.error$labels

metaplot(mn = yi, se = sei, 
         labels = var_names,
         xlab = "Effect Size", ylab = "Study", 
         colors = meta.colors(box="blue",
                              lines=44,
                              zero = "white",
                              summary="orange",
                              text="black"),
         title(main = "SA  Forest Plot"))
abline(v= meta_rcts_grouped$beta, col = "blue", lty = "dashed")
dev.off()
dev.off()
```
